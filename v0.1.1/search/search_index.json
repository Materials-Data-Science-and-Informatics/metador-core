{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"metador-core","text":"<p>The core library of the Metador framework. It provides:</p> <ul> <li>an interface for managing structured and validated metadata (<code>MetadorContainer</code>)</li> <li>an API to manage immutable (but still \"patchable\") HDF5 files (<code>IH5Record</code>)</li> <li>an extensible entry-points based plugin system for defining plugin groups and plugins</li> <li>core plugin group types and interfaces (schemas, packers, widgets, ...)</li> <li>general semantically aligned schemas that can be used and extended</li> <li>visualization widgets for common data types based on Bokeh and Panel</li> <li>generic dashboard presenting (meta)data for which suitable widgets are installed</li> </ul>"},{"location":"#usage","title":"Usage","text":"<p>To get started, please check out the quickstart guide.</p>"},{"location":"#how-to-cite","title":"How to Cite","text":"<p>If you want to cite this project in your scientific work, please use the citation file in the repository.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We kindly thank all authors and contributors.</p> <p></p> <p>This project was developed at the Institute for Materials Data Science and Informatics (IAS-9) of the J\u00fclich Research Center and funded by the Helmholtz Metadata Collaboration (HMC), an incubator-platform of the Helmholtz Association within the framework of the Information and Data Science strategic initiative.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>Here we provide notes that summarize the most important changes in each released version.</p> <p>Please consult the changelog to inform yourself about breaking changes and security issues.</p>"},{"location":"changelog/#0.1.1","title":"v0.1.1 (2023-??-??)","text":"<ul> <li>added <code>metador</code> CLI tools for self-diagnosis (other features can be added in the future)</li> <li>added CSV/TSV widget</li> <li>added example schemas for tutorial in material science context</li> <li>renamed <code>embed_file</code> to <code>pack_file</code> and simplified its arguments</li> <li>removed <code>creator</code> from <code>BibMeta</code> (author and creator are equivalent in schema.org)</li> <li>minor tweaks and fixes</li> </ul>"},{"location":"changelog/#0.1.0","title":"v0.1.0 (2023-08-08)","text":"<ul> <li>updated dependencies to unpin versions</li> <li>removed unused dependencies</li> <li>minor tweaks and fixes</li> </ul>"},{"location":"changelog/#0.0.2","title":"v0.0.2 (2022-12-12)","text":"<ul> <li>minor tweaks and fixes</li> </ul>"},{"location":"changelog/#0.0.1","title":"v0.0.1 (2022-10-10)","text":"<ul> <li>first PyPI release</li> </ul>"},{"location":"code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the project maintainers by e-mail. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contributing/","title":"How To Contribute","text":"<p>All kinds of contributions are very welcome! You can contribute in various ways, e.g. by</p> <ul> <li>providing feedback</li> <li>asking questions</li> <li>suggesting ideas</li> <li>implementing features</li> <li>fixing problems</li> <li>improving documentation</li> </ul> <p>To make contributing to open source projects a good experience to everyone involved, please make sure that you follow our code of conduct when communicating with others.</p>"},{"location":"contributing/#ideas-questions-and-problems","title":"Ideas, Questions and Problems","text":"<p>If you have questions or difficulties using this software, please use the issue tracker.</p> <p>If your topic is not already covered by an existing issue, please create a new issue using one of the provided issue templates.</p> <p>If your issue is caused by incomplete, unclear or outdated documentation, we are also happy to get suggestions on how to improve it. Outdated or incorrect documentation is a bug, while missing documentation is a feature request.</p> <p>NOTE: If you want to report a critical security problem, do not open an issue! Instead, please create a private security advisory, or contact the current package maintainers directly by e-mail.</p>"},{"location":"contributing/#development","title":"Development","text":"<p>This project uses Poetry for dependency management.</p> <p>You can run the following lines to check out the project and prepare it for development:</p> <pre><code>git clone git@github.com:Materials-Data-Science-and-Informatics/metador-core.git\ncd metador-core\npoetry install --with docs\npoetry run poe init-dev\n</code></pre> <p>Common tasks are accessible via poe:</p> <ul> <li> <p>Use <code>poetry run poe lint</code> to run linters manually, add <code>--all-files</code> to check everything.</p> </li> <li> <p>Use <code>poetry run poe test</code> to run tests, add <code>--cov</code> to also show test coverage.</p> </li> <li> <p>Use <code>poetry run poe docs</code> to generate local documentation</p> </li> </ul> <p>In order to contribute code, please open a pull request.</p> <p>Before opening the PR, please make sure that your changes</p> <ul> <li>are sufficiently covered by meaningful tests,</li> <li>are reflected in suitable documentation (API docs, guides, etc.), and</li> <li>successfully pass all pre-commit hooks.</li> </ul>"},{"location":"coverage/","title":"Coverage Report","text":""},{"location":"credits/","title":"Authors and Contributors","text":"<p>Main authors are persons whose contributions significantly shaped the state of the software at some point in time.</p> <p>Additional contributors are persons who are not main authors, but contributed non-trivially to this project, e.g. by providing smaller fixes and enhancements to the code and/or documentation.</p> <p>Of course, this is just a rough overview and categorization. For a more complete overview of all contributors and contributions, please inspect the git history of this repository.</p>"},{"location":"credits/#main-authors","title":"Main Authors","text":"<ul> <li>Anton Pirogov (     E-Mail,     ORCID   ): original author</li> </ul>"},{"location":"credits/#additional-contributors","title":"Additional Contributors","text":"<p>... maybe you?</p>"},{"location":"dev_guide/","title":"Developer Guide","text":"<p>This guide is targeting mainly developers, maintainers and other technical contributors and provides more information on how to work with this repository.</p>"},{"location":"dev_guide/#overview","title":"Overview","text":""},{"location":"dev_guide/#repository-structure","title":"Repository Structure","text":"<p>Here is a non-exhaustive list of the most important files and directories in the repository.</p> <p>General:</p> <ul> <li><code>AUTHORS.md</code>: acknowledges and lists all contributors</li> <li><code>CHANGELOG.md</code>: summarizes the changes for each version of the software for users</li> <li><code>CODE_OF_CONDUCT.md</code>: defines the social standards that must be followed by contributors</li> <li><code>CONTRIBUTING.md</code>: explains  how others can contribute to the project</li> <li><code>README.md</code>: provides an overview and points to other resources</li> </ul> <p>Metadata:</p> <ul> <li><code>CITATION.cff</code>: metadata stating how to cite the project</li> <li><code>codemeta.json</code>: metadata for harvesting by other tools and services</li> <li><code>LICENSE</code>: the (main) license of the project</li> <li><code>LICENSES</code>: copies of all licenses that apply to files in the project</li> <li><code>.reuse/dep5</code>: granular license and copyright information for all files and directories</li> </ul> <p>Development:</p> <ul> <li><code>pyproject.toml</code>: project metadata, dependencies, development tool configurations</li> <li><code>poetry.lock</code>: needed for reproducible installation of the project</li> <li><code>src</code>: actual code provided by the project</li> <li><code>tests</code>: all tests for the code in the project</li> <li><code>mkdocs.yml</code>: configuration of the project website</li> <li><code>docs</code>: most contents used for the project website</li> </ul> <p>Automation and Quality Control:</p> <ul> <li><code>.pre-commit-config.yaml</code>: quality assurance tools used in the project</li> <li><code>.github/workflows</code>: CI scripts for GitHub (QA, documentation and package deployment)</li> <li><code>.github/ISSUE_TEMPLATE</code>: templates for the GitHub issue tracker</li> <li><code>.gitlab-ci.yml</code>: mostly equivalent CI scripts, but for GitLab</li> <li><code>.gitlab/issue_templates</code>: The same issues templates, but for GitLab</li> </ul>"},{"location":"dev_guide/#used-tools","title":"Used Tools","text":"<p>Here is a non-exhaustive list of the most important tools used in the project.</p> <p>Best practices for modern Python development are implemented by using:</p> <ul> <li><code>poetry</code> for dependency management and packaging</li> <li><code>pytest</code> for unit testing</li> <li><code>hypothesis</code> for property-based testing</li> <li><code>pre-commit</code> for orchestrating linters, formatters and other utilities</li> <li><code>black</code> for source-code formatting</li> <li><code>autoflake</code> for automatically removing unused imports</li> <li><code>flake8</code> for general linting (using various linter plugins)</li> <li><code>pydocstyle</code> for checking docstring conventions</li> <li><code>mypy</code> for editor-independent type-checking</li> <li><code>mkdocs</code> for generating the project documentation website</li> <li><code>bandit</code> for checking security issues in the code</li> <li><code>safety</code> for checking security issues in the current dependencies</li> </ul> <p>Metadata best practices for FAIR software are implemented using:</p> <ul> <li><code>somesy</code> to synchronize project metadata to <code>CITATION.cff</code> and <code>codemeta.json</code></li> <li><code>reuse</code> to check REUSE-compliance (granular copyright and license metadata)</li> <li><code>licensecheck</code> to scan for possible license incompatibilities in the dependencies</li> </ul>"},{"location":"dev_guide/#basics","title":"Basics","text":"<p>The project</p> <ul> <li>heavily uses <code>pyproject.toml</code>, which is a recommended standard</li> <li>adopts the <code>src</code> layout, to avoid common problems</li> <li>keeps the actual code (<code>src</code>) and test code (<code>tests</code>) separated</li> </ul> <p>The <code>pyproject.toml</code> is the main configuration file for the project. It contains both general information about the software as well as configuration for various tools.</p> <p>In older software, most of this information is often scattered over many little tool-specific configuration files and a <code>setup.py</code>, <code>setup.cfg</code> and/or <code>requirements.txt</code> file.</p> <p>In this project, <code>pyproject.toml</code> is the first place that should be checked when looking for the configuration of some development tool.</p>"},{"location":"dev_guide/#configuration","title":"Configuration","text":"<p>The main tool needed to manage and configure the project is Poetry.</p> <p>Please follow its setup documentation to install it correctly. Poetry should not be installed with <code>pip</code> like other Python tools.</p> <p>Poetry performs many important tasks:</p> <ul> <li>it manages the virtual environment(s) used for the project</li> <li>it manages all the dependencies needed for the code to work</li> <li>it takes care of packaging the code into a <code>pip</code>-installable package</li> </ul> <p>You can find a cheatsheet with the most important commands here and consult its official documentation for detailed information.</p> <p>Note that <code>poetry</code> is only needed for development of the repository. The end-users who just want to install and use this project do not need to set up or know anything about poetry.</p> <p>Note that if you use <code>poetry shell</code> and the project is installed with <code>poetry install</code>, in the following you do not have to prepend <code>poetry run</code> to commands you will see below.</p>"},{"location":"dev_guide/#task-runner","title":"Task Runner","text":"<p>It is a good practice to have a common way for launching different project-related tasks. It removes the need of remembering flags for various tools, and avoids duplication of the same commands in the CI pipelines. If something in a workflow needs to change, it can be changed in just one place, thus reducing the risk of making a mistake.</p> <p>Often projects use a shell script or <code>Makefile</code> for this purpose. This project uses poethepoet, as it integrates nicely with <code>poetry</code>. The tasks are defined in <code>pyproject.toml</code> and can be launched using:</p> <pre><code>poetry run poe TASK_NAME\n</code></pre>"},{"location":"dev_guide/#ci-workflows","title":"CI Workflows","text":"<p>The project contains CI workflows for both GitHub and GitLab.</p> <p>The main CI pipeline runs on each new pushed commit and will</p> <ol> <li>Run all configured code analysis tools,</li> <li>Run code tests with multiple versions of Python,</li> <li>build and deploy the online project documentation website, and</li> <li>if a new version tag was pushed, launch the release workflow</li> </ol>"},{"location":"dev_guide/#quality-control","title":"Quality Control","text":""},{"location":"dev_guide/#static-analysis","title":"Static Analysis","text":"<p>Except for code testing, most tools for quality control are added to the project as <code>pre-commit</code> hooks. The <code>pre-commit</code> tool takes care of installing, updating and running the tools according to the configuration in the <code>.pre-commit-config.yaml</code> file.</p> <p>For every new copy of the repository (e.g. after <code>git clone</code>), <code>pre-commit</code> first must be activated. This is usually done using <code>pre-commit install</code>, which also requires that <code>pre-commit</code> is already available. For more convenience, we simplified the procedure.</p> <p>In this project, you can run:</p> <pre><code>poetry run poe init-dev\n</code></pre> <p>This will make sure that <code>pre-commit</code> is enabled in your repository copy.</p> <p>Once enabled, every time you try to <code>git commit</code> some changed files various tools will run on those (and only those) files.</p> <p>This means that (with some exceptions) <code>pre-commit</code> by default will run only on the changed files that were added to the next commit (i.e., files in the git staging area). These files are usually colored in green when running <code>git status</code>.</p> <ul> <li>Some tools only report the problems they detected</li> <li>Some tools actively modify files (e.g., fix formatting)</li> </ul> <p>In any case, the <code>git commit</code> will fail if a file was modified by a tool, or some problems were reported. In order to complete the commit, you need to</p> <ul> <li>resolve all problems (by fixing them or marking them as false alarm), and</li> <li><code>git add</code> all changed files again (to update the files in the staging area).</li> </ul> <p>After doing that, you can retry to <code>git commit</code> your changes.</p> <p>To avoid having to deal with many issues at once, it is a good habit to run <code>pre-commit</code> by hand from time to time. In this project, this can be done with:</p> <pre><code>poetry run poe lint --all-files\n</code></pre>"},{"location":"dev_guide/#testing","title":"Testing","text":"<p>pytest is used as the main framework for testing.</p> <p>The project uses the <code>pytest-cov</code> plugin to integrate <code>pytest</code> with <code>coverage</code>, which collects and reports test coverage information.</p> <p>In addition to writing regular unit tests with <code>pytest</code>, consider using hypothesis, which integrates nicely with <code>pytest</code> and implements property-based testing - which involves automatic generation of randomized inputs for test cases. This can help to find bugs often found for various edge cases that are easy to overlook in ad-hoc manual tests. Such randomized tests can be a good addition to hand-crafted tests and inputs.</p> <p>To run all tests, either invoke <code>pytest</code> directly, or use the provided task:</p> <pre><code>poetry run poe test\n</code></pre>"},{"location":"dev_guide/#documentation","title":"Documentation","text":"<p>The project uses <code>mkdocs</code> with the popular and excellent <code>mkdocs-material</code> theme to generate the project documentation website, which provides both user and developer documentation.</p> <p><code>mkdocs</code> is configured in the <code>mkdocs.yml</code> file, which we prepared in a way that there is</p> <ul> <li>no need to duplicate sections from files in other places (such as <code>README.md</code>)</li> <li>fully automatic API documentation pages based on Python docstrings in the code</li> <li>a detailed test coverage report is included in the website</li> </ul> <p>The first point is important, because avoiding duplication means avoiding errors whenever text or examples are updated. The second point is convenient, as modules and functions do not need to be added by hand, which is easy to forget. The third point removes the need to use an external service such as CodeCov to store and present code coverage information.</p> <p>As software changes over time and users cannot always keep up with the latest developments, each new version of the software should provide version-specific documentation. To make this both possible as well as convenient, this project uses <code>mike</code> to generate and manage the <code>mkdocs</code> documentation for different versions of the software.</p>"},{"location":"dev_guide/#online-documentation","title":"Online Documentation","text":"<p>To avoid dependence on additional services such as readthedocs, the project website is deployed using GitHub Pages.</p> <p>The provided CI pipeline automatically generates the documentation for the latest development version (i.e., current state of the <code>main</code> branch) as well as every released version (i.e., marked by a version tag <code>vX.Y.Z</code>).</p>"},{"location":"dev_guide/#offline-documentation","title":"Offline Documentation","text":"<p>You can manually generate a local and fully offline copy of the documentation, which can be useful for e.g. previewing the results during active work on the documentation:</p> <pre><code>poetry install --with docs\npoetry run poe docs\n</code></pre> <p>Once the documentation site is built, run <code>mkdocs serve</code> and open <code>https://localhost:8000</code> in your browser to see the local copy of the website.</p>"},{"location":"dev_guide/#releases","title":"Releases","text":"<p>From time to time the project is ready for a new release for users.</p>"},{"location":"dev_guide/#creating-a-new-release","title":"Creating a New Release","text":"<p>Before releasing a new version, push the commit the new release should be based on to the upstream repository, and make sure that:</p> <ul> <li>the CI pipeline completes successfully</li> <li>the version number in <code>pyproject.toml</code> is updated, in particular:</li> <li>it must be larger than the previous released version</li> <li>it should adequately reflect the severity of changes</li> <li>the provided user and developer documentation is up-to-date, including:</li> <li>a new section in the <code>CHANGELOG.md</code> file summarizing changes in the new version</li> <li>possibly revised information about contributors and/or maintainers</li> </ul> <p>If this is the case, proceed with the release by:</p> <ul> <li>creating a new tag that matches the version in the <code>pyproject.toml</code>: <code>git tag vX.Y.Z</code></li> <li>pushing the new tag to the upstream repository: <code>git push origin vX.Y.Z</code></li> </ul> <p>The pushed version tag will trigger a pipeline that will:</p> <ul> <li>build and deploy the documentation website for the specific version</li> <li>publish the package to enabled targets (see below)</li> </ul>"},{"location":"dev_guide/#release-targets","title":"Release Targets","text":"<p>Targets for releases can be enabled or disabled in <code>.github/workflows/ci.yml</code> and configured by adapting the corresponding actions in <code>.github/workflows/releases.yml</code>.</p>"},{"location":"dev_guide/#github-release","title":"Github Release","text":"<p>By default, the release workflow will create a basic Github Release that provides a snapshot of the repository as a download. This requires no additional configuration.</p> <p>See here for information on how the Github release can be customized.</p> <p>Note that this release target is mostly for demonstration purposes. For most Python projects, using PyPI is the recommended primary distribution method.</p>"},{"location":"dev_guide/#pypi-and-compatible-package-indices","title":"PyPI (and compatible package indices)","text":"<p>For releases to PyPI and Test PyPI the project uses the new Trusted Publishers workflow that is both more secure and convenient to use than other authorization methods.</p> <p>Before the project can be released to PyPI or Test PyPI the first time, first a pending publisher must be added in the PyPI account of the main project maintainer, using <code>release.yml</code> as the requested workflow name.</p> <p>Once this is done, set the corresponding option (<code>to_pypi</code> / <code>to_test_pypi</code>) to <code>true</code> in the <code>publish</code> job in <code>ci.yml</code> to enable the corresponding publication target.</p> <p>If the old and less secure token-based authentication method is needed or the package should be published to a different PyPI-compatible package index, please adapt <code>release.yml</code> accordingly).</p>"},{"location":"license/","title":"License","text":"<p>Unless stated otherwise, all code provided by this project (excluding external dependencies) is distributed under the following license:</p> <pre><code>MIT License\n\nCopyright (c) 2022 Forschungszentrum J\u00fclich GmbH - Institute Materials Data Science and Informatics (IAS9) - Stefan Sandfeld (s.sandfeld@fz-juelich.de)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre> <p>This project is REUSE compliant. The following detailed license and copyright information in DEP5 format can also be found in the <code>.reuse/dep5</code> file in the project source directory:</p> <pre><code>Format: https://www.debian.org/doc/packaging-manuals/copyright-format/1.0/\nUpstream-Name: metador-core\nUpstream-Contact: Anton Pirogov &lt;a.pirogov@fz-juelich.de&gt;\nSource: https://github.com/Materials-Data-Science-and-Informatics/metador-core\n\nFiles: .gitignore pyproject.toml somesy.toml poetry.lock .pre-commit-config.yaml codemeta.json CITATION.cff README.md RELEASE_NOTES.md CHANGELOG.md CODE_OF_CONDUCT.md AUTHORS.md CONTRIBUTING.md .gitlab-ci.yml .gitlab/* .github/* mkdocs.yml docs/* tutorial/*\nCopyright: 2021 Forschungszentrum J\u00fclich GmbH - Institute for Materials Data Science and Informatics (IAS9) - Stefan Sandfeld &lt;s.sandfeld@fz-juelich.de&gt;\nLicense: CC0-1.0\n\nFiles: src/metador_core/* tests/*\nCopyright: 2021 Forschungszentrum J\u00fclich GmbH - Institute for Materials Data Science and Informatics (IAS9) - Stefan Sandfeld &lt;s.sandfeld@fz-juelich.de&gt;\nLicense: MIT\n</code></pre>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#installation","title":"Installation","text":"<p>You can install the current stable version of Metador from PyPI:</p> <pre><code>pip install metador-core\n</code></pre>"},{"location":"quickstart/#getting-started","title":"Getting Started","text":"<p>If you successfully installed the package, check out the tutorial notebooks.</p> <p>These are intended to showcase what Metador has to offer and get you started with usage and development of your own schemas, widgets or other plugins.</p> <p>To explore the notebooks interactively, clone this repo, install it, and then run:</p> <pre><code>pip install notebook\njupyter notebook ./docs/notebooks\n</code></pre> <p>You can use the metador-extension-cookiecutter template to generate a well-structured Python package repository that you can use to quickly get started with Metador plugin development.</p>"},{"location":"quickstart/#compatibility-and-known-issues","title":"Compatibility and Known Issues","text":"<p>Currently this package supports Python <code>&gt;=3.8</code>.</p> <p>We will try to support all still officially updated versions of Python, unless forced to drop it for technical reasons.</p>"},{"location":"notebooks/01_MetadorContainer/","title":"Creating a MetadorContainer","text":"In\u00a0[1]: Copied! <pre># some imports we will need later\nfrom urllib.request import urlopen\nimport hashlib\nimport IPython.display as jupyter\nimport pydantic\nimport h5py\n# --------------------------------\n</pre> # some imports we will need later from urllib.request import urlopen import hashlib import IPython.display as jupyter import pydantic import h5py # -------------------------------- In\u00a0[2]: Copied! <pre>from metador_core.container import MetadorContainer\n\n# store the name of our container (we'll keep working with it)\nCONTAINER_NAME: str = \"my_first_container.h5\"\n\n# access a container by immediately wrapping the freshly created h5py.File\nwith MetadorContainer(CONTAINER_NAME, \"w\") as mc:\n    # store something\n    mc[\"group/list\"] = [9, 8, 7, 42]\n    # list contents of container\n    print(\"Contents:\")\n    mc.visit(print)\n    print(\"---\")\n\n    # access the new dataset\n    my_list = mc[\"group/list\"]\n    print(my_list)\n\n    # index into it\n    print(\"The answer is:\", my_list[3])\n</pre> from metador_core.container import MetadorContainer  # store the name of our container (we'll keep working with it) CONTAINER_NAME: str = \"my_first_container.h5\"  # access a container by immediately wrapping the freshly created h5py.File with MetadorContainer(CONTAINER_NAME, \"w\") as mc:     # store something     mc[\"group/list\"] = [9, 8, 7, 42]     # list contents of container     print(\"Contents:\")     mc.visit(print)     print(\"---\")      # access the new dataset     my_list = mc[\"group/list\"]     print(my_list)      # index into it     print(\"The answer is:\", my_list[3]) <pre>Contents:\ngroup\ngroup/list\n---\n&lt;HDF5 dataset \"list\": shape (4,), type \"&lt;i8\"&gt;\nThe answer is: 42\n</pre> <p>The Metador container we created is an HDF5 file - technically, you can open it just as any other regular HDF5 file, or even inspect it with <code>hdfview</code>. But you might be surprised to find many objects you did not (knowingly) create - these are the \"technical details\" which <code>MetadorContainer</code> hides from you for your own good. Doing changes outside of the provided <code>MetadorContainer</code> interface will disturb these additional structures, so just do not do it.</p> IMPORTANT: Never open the container as h5py.File directly!      Always use your Metador containers with MetadorContainer, or you risk ending up with broken containers!  <p>Also, to give some more general advice to save you from trouble down the line - just as with any kind of resource, you should use the special <code>with</code> notation to ensure that the file is properly closed when you are done with it or some exception happens. If you do not use <code>with</code>, you are responsible for calling <code>close()</code> yourself, and most likely will run into annoying issues that waste your valuable time.</p> IMPORTANT: Whenever possible, use with when accessing containers and avoid using the same container object in multiple Jupyter cells!   In\u00a0[3]: Copied! <pre>from metador_core.plugins import plugingroups\nprint(plugingroups)\n</pre> from metador_core.plugins import plugingroups print(plugingroups) <pre>Available 'plugingroup' plugins:\n\t'harvester' (0.1.0)\n\t'packer' (0.1.0)\n\t'schema' (0.1.0)\n\t'widget' (0.1.0)\n\t'plugingroup' (0.1.0)\n</pre> <p>All the individual plugin groups can be imported individually, e.g. to get quick access to schemas, you can simply import and list them like this:</p> In\u00a0[4]: Copied! <pre>from metador_core.plugins import schemas\nprint(schemas)\n</pre> from metador_core.plugins import schemas print(schemas) <pre>Available 'schema' plugins:\n\t'core.bib' (0.1.0)\n\t'core.dashboard' (0.1.0)\n\t'core.dir' (0.1.0)\n\t'core.file' (0.1.0)\n\t'core.imagefile' (0.1.0)\n\t'core.org' (0.1.0)\n\t'core.packerinfo' (0.1.0)\n\t'core.person' (0.1.0)\n\t'core.table' (0.1.0)\n\t'example.matsci.info' (0.1.0)\n\t'example.matsci.instrument' (0.1.0)\n\t'example.matsci.material' (0.1.0)\n\t'example.matsci.method' (0.1.0)\n\t'example.matsci.specimen' (0.1.0)\n</pre> In\u00a0[5]: Copied! <pre># following line roughly corresponds to the pseudo-Python statement:\n#     from schemas import 'core.bib' as BibMeta\nBibMeta = schemas[\"core.bib\"]\n\n# we can show some plugin info, including the version:\nprint(BibMeta.Plugin)\n</pre> # following line roughly corresponds to the pseudo-Python statement: #     from schemas import 'core.bib' as BibMeta BibMeta = schemas[\"core.bib\"]  # we can show some plugin info, including the version: print(BibMeta.Plugin) <pre>{\n  \"group\": \"schema\",\n  \"name\": \"core.bib\",\n  \"version\": \"0.1.0\"\n}\n</pre> <p>If you know what version you need and expect, you should access it using <code>get</code> and pass an expected version. Then Metador will also check whether the installed version is compatible:</p> In\u00a0[6]: Copied! <pre>BibMeta = schemas.get(\"core.bib\", (0, 0, 1)) # ok\n\ntry: # this will fail\n    SpecificBibMetaVersion = schemas.get(\"core.bib\", (2,3,4))\nexcept KeyError as e:\n    print(e)\n</pre> BibMeta = schemas.get(\"core.bib\", (0, 0, 1)) # ok  try: # this will fail     SpecificBibMetaVersion = schemas.get(\"core.bib\", (2,3,4)) except KeyError as e:     print(e) In\u00a0[7]: Copied! <pre>try:\n    # create bibliographic metadata object\n    my_bibmeta = BibMeta(name=\"Title for my container\")  # &lt;- this will fail\n    \nexcept pydantic.ValidationError as e:\n    print(e)  # see what went wrong\n</pre> try:     # create bibliographic metadata object     my_bibmeta = BibMeta(name=\"Title for my container\")  # &lt;- this will fail      except pydantic.ValidationError as e:     print(e)  # see what went wrong <pre>3 validation errors for BibMeta\nabstract\n  field required (type=value_error.missing)\nauthor\n  field required (type=value_error.missing)\ndateCreated\n  field required (type=value_error.missing)\n</pre> <p>You can see that the schema complains, because we did not provide all the required information. Let us try again:</p> In\u00a0[8]: Copied! <pre>from datetime import datetime\ntoday = datetime.today().isoformat()\n\ncontainer_author = {\n    \"@id\": \"https://orcid.org/0000-0002-1825-0097\",\n    \"givenName\": \"Josiah\", \"familyName\": \"Carberry\"\n}\n\nmy_bibmeta = BibMeta(\n    name=\"Title for my container\",\n    abstract=\"This is my first Metador-compliant container\",\n    author=[container_author],\n    creator=container_author,\n    dateCreated=today\n)\nprint(my_bibmeta)  # look at what we have created\n</pre> from datetime import datetime today = datetime.today().isoformat()  container_author = {     \"@id\": \"https://orcid.org/0000-0002-1825-0097\",     \"givenName\": \"Josiah\", \"familyName\": \"Carberry\" }  my_bibmeta = BibMeta(     name=\"Title for my container\",     abstract=\"This is my first Metador-compliant container\",     author=[container_author],     creator=container_author,     dateCreated=today ) print(my_bibmeta)  # look at what we have created <pre>{\n  \"name\": \"Title for my container\",\n  \"@context\": \"https://w3id.org/ro/crate/1.1/context\",\n  \"@type\": \"Dataset\",\n  \"abstract\": \"This is my first Metador-compliant container\",\n  \"author\": [\n    {\n      \"@id\": \"https://orcid.org/0000-0002-1825-0097\",\n      \"name\": \"Josiah Carberry\",\n      \"@context\": \"https://w3id.org/ro/crate/1.1/context\",\n      \"@type\": \"Person\",\n      \"givenName\": \"Josiah\",\n      \"familyName\": \"Carberry\"\n    }\n  ],\n  \"dateCreated\": \"2023-09-06T12:47:43.119041\",\n  \"hasPart\": [],\n  \"creator\": {\n    \"@id\": \"https://orcid.org/0000-0002-1825-0097\",\n    \"givenName\": \"Josiah\",\n    \"familyName\": \"Carberry\"\n  }\n}\n</pre> <p>Metador strives to be compliant with existing metadata standards and provide semantic metadata that is backed by existing vocabularies and ontologies, so the generic schemas are aligned with RO-Crate. The schema class will  try to understand (i.e., parse) your metadata and bring it into standard-compliant form. This is the reason why now you can see more fields that you actually provided - the output is, in fact, valid JSON-LD!</p> <p>We provided the <code>author</code> information in a plain old Python <code>dict</code>. This is okay - schemas will try to understand your input, if provided in basic JSON-compatible types (i.e. <code>int</code>, <code>float</code>, <code>bool</code>, <code>list</code> and <code>dict</code>). Nevertheless, it is recommended that you construct more complex metadata objects using the corresponding schema class to profit from validation early on. Otherwise, in the worst case, you will only see problems with your metadata when you try attaching it to a node.</p> NOTE: Prefer using schema classes over plain dicts when constructing metadata.  In\u00a0[9]: Copied! <pre>BibMeta.Fields\n</pre> BibMeta.Fields Out[9]: <pre>author\n\ttype: List[Person]\n\torigin: metador_core.schema.common.BibMeta (plugin: core.bib 0.1.0)\n\tschemas: Person\n\tdescription:\n\t\tList of authors (creators of the actual data).\n\t\nname\n\ttype: &lt;class 'metador_core.schema.types.NonEmptyStr'&gt;\n\torigin: metador_core.schema.common.BibMeta (plugin: core.bib 0.1.0)\n\tdescription:\n\t\tName, title or caption of the entity.\n\t\nabstract\n\ttype: &lt;class 'metador_core.schema.types.NonEmptyStr'&gt;\n\torigin: metador_core.schema.common.BibMeta (plugin: core.bib 0.1.0)\n\tdescription:\n\t\tA short description that summarizes the creative work.\n\t\ndateCreated\n\ttype: typing.Union[datetime.date, datetime.datetime]\n\torigin: metador_core.schema.common.BibMeta (plugin: core.bib 0.1.0)\n\t\nid_\n\ttype: Annotated[Optional[NonEmptyStr], Field(alias='@id')]\n\torigin: metador_core.schema.ld.LDSchema\n\t\nidentifier\n\ttype: Optional[Union[URL, Text]]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tArbitrary identifier of the entity.\n\t\t\n\t\tPrefer @id if the identifier is web-resolvable, or use more\n\t\tspecific fields if available.\n\t\nurl\n\ttype: Optional[URL]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tURL of the entity.\n\t\ndescription\n\ttype: Optional[Text]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tDescription of the entity.\n\t\nalternateName\n\ttype: Optional[List[Text]]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tKnown aliases of the entity.\n\t\nsameAs\n\ttype: Optional[List[URL]]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\t\nversion\n\ttype: Optional[Union[NonNegativeInt, Text]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tdescription:\n\t\tVersion of this work.\n\t\t\n\t\tEither an integer, or a version string, e.g. \"1.0.5\".\n\t\t\n\t\tWhen using version strings, follow https://semver.org\n\t\twhenever applicable.\n\t\t\n\t\ncitation\n\ttype: Optional[Set[Union[LDOrRef[CreativeWork], Text]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, CreativeWork\n\tdescription:\n\t\tCitation or reference to another creative work, e.g.\n\t\tanother publication, scholarly article, etc.\n\t\nkeywords\n\ttype: Optional[Set[Text]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tdescription:\n\t\tKeywords or tags to describe this creative work.\n\t\ncontributor\n\ttype: Optional[List[LDOrRef[OrgOrPerson]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, Organization, Person\n\tdescription:\n\t\tAdditional people who contributed to the work, e.g.\n\t\tin research, the people who would be in the acknowledgements\n\t\tsection of the relevant paper.\n\t\nmaintainer\n\ttype: Optional[List[LDOrRef[OrgOrPerson]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, Organization, Person\n\t\nproducer\n\ttype: Optional[List[LDOrRef[OrgOrPerson]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, Organization, Person\n\t\nprovider\n\ttype: Optional[List[LDOrRef[OrgOrPerson]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, Organization, Person\n\t\npublisher\n\ttype: Optional[List[LDOrRef[OrgOrPerson]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, Organization, Person\n\t\nsponsor\n\ttype: Optional[List[LDOrRef[OrgOrPerson]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, Organization, Person\n\t\neditor\n\ttype: Optional[List[LDOrRef[Person]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, Person\n\t\ndateModified\n\ttype: Optional[DateOrDatetime]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\t\ndatePublished\n\ttype: Optional[DateOrDatetime]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\t\ncopyrightHolder\n\ttype: Optional[LDOrRef[OrgOrPerson]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, Organization, Person\n\t\ncopyrightYear\n\ttype: Optional[Int]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\t\ncopyrightNotice\n\ttype: Optional[Text]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\t\nlicense\n\ttype: Optional[Union[URL, LDOrRef[CreativeWork]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, CreativeWork\n\t\nabout\n\ttype: Optional[Set[LDOrRef[Thing]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, Thing\n\t\nsubjectOf\n\ttype: Optional[Set[LDOrRef[CreativeWork]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, CreativeWork\n\t\nhasPart\n\ttype: Set[LDIdRef]\n\torigin: metador_core.schema.common.rocrate.DirMeta (plugin: core.dir 0.1.0)\n\tschemas: LDIdRef\n\tdescription:\n\t\tReferences to (a subset of) contained files and subdirectories.\n\t\nisPartOf\n\ttype: Optional[Set[Union[URL, LDOrRef[CreativeWork]]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, CreativeWork\n\t\nisBasedOn\n\ttype: Optional[Set[Union[URL, LDOrRef[CreativeWork]]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: LDIdRef, CreativeWork\n\t\ndistribution\n\ttype: Optional[URL]\n\torigin: metador_core.schema.common.schemaorg.Dataset\n\tdescription:\n\t\tDownloadable form of this dataset, at a specific location, in a specific format.\n\t\nvariableMeasured\n\ttype: Optional[List[Union[Text, PropertyValue]]]\n\torigin: metador_core.schema.common.schemaorg.Dataset\n\tschemas: PropertyValue\n\tdescription:\n\t\tVariables that are measured in the dataset.\n\t</pre> <p>As you can see, the <code>Fields</code> interface provides useful information about every field in a schema:</p> <ul> <li>The type, which tells you what kind of objects are suitable</li> <li>The origin, telling you in which schema the field was defined</li> <li>If provided, a description of the field (contents of the docstring)</li> <li>The schemas that you can use to construct an object of a suitable type</li> </ul> <p>The last point is especially important - not all schemas are registered as plugins, for various reasons: they can be too general, too specific, not meaningful on their own, etc., but be a part of some schema you want or need to use. Through <code>Fields</code> you can access all schemas that are relevant, even if they are not a plugin.</p> IMPORTANT: Never import schema classes directly, always go through metador_core.plugins.schemas for installed plugin schemas, and use the Fields interface to access nested schemas that are needed for fields. That way, if schemas are relocated for whatever reason, your code will still work!  <p>Now let us improve the schema instance construction. Let us first figure out what we can put into the <code>author</code> field:</p> In\u00a0[10]: Copied! <pre>print(BibMeta.Fields.author)\n</pre> print(BibMeta.Fields.author) <pre>author\n\ttype: List[Person]\n\torigin: metador_core.schema.common.BibMeta (plugin: core.bib 0.1.0)\n\tschemas: Person\n\tdescription:\n\t\tList of authors (creators of the actual data).\n\t\n</pre> <p>Now we know that for both fields we need something called <code>Person</code>. Let's now get our hands on that <code>Person</code> class and see what it wants:</p> In\u00a0[11]: Copied! <pre>Person = BibMeta.Fields.author.schemas.Person\n\nprint(Person.Fields)\n</pre> Person = BibMeta.Fields.author.schemas.Person  print(Person.Fields) <pre>id_\n\ttype: Annotated[Optional[NonEmptyStr], Field(alias='@id')]\n\torigin: metador_core.schema.ld.LDSchema\n\t\nname\n\ttype: Optional[Text]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tName, title or caption of the entity.\n\t\nidentifier\n\ttype: Optional[Union[URL, Text]]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tArbitrary identifier of the entity.\n\t\t\n\t\tPrefer @id if the identifier is web-resolvable, or use more\n\t\tspecific fields if available.\n\t\nurl\n\ttype: Optional[URL]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tURL of the entity.\n\t\ndescription\n\ttype: Optional[Text]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tDescription of the entity.\n\t\nalternateName\n\ttype: Optional[List[Text]]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tKnown aliases of the entity.\n\t\nsameAs\n\ttype: Optional[List[URL]]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\t\ngivenName\n\ttype: Optional[Text]\n\torigin: metador_core.schema.common.schemaorg.Person\n\tdescription:\n\t\tGiven name, typically the first name of a Person.\n\t\nfamilyName\n\ttype: Optional[Text]\n\torigin: metador_core.schema.common.schemaorg.Person\n\tdescription:\n\t\tFamily name of a Person.\n\t\nadditionalName\n\ttype: Optional[Text]\n\torigin: metador_core.schema.common.schemaorg.Person\n\tdescription:\n\t\tAdditional name for a Person, e.g. for a middle name.\n\t\nemail\n\ttype: Optional[Text]\n\torigin: metador_core.schema.common.schemaorg.Person\n\tdescription:\n\t\tE-mail address.\n\t\naffiliation\n\ttype: Optional[LDOrRef[Organization]]\n\torigin: metador_core.schema.common.schemaorg.Person\n\tschemas: LDIdRef, Organization\n\tdescription:\n\t\tAn organization this person is affiliated with.\n\t\n</pre> <p>Now we have all information to recreate our bibliographic metadata using the correct <code>Person</code> schema:</p> In\u00a0[12]: Copied! <pre>container_author = Person(\n    id_ = \"https://orcid.org/0000-0002-1825-0097\",\n    givenName= \"Josiah\", familyName=\"Carberry\"\n)\n\nmy_bibmeta = BibMeta(\n    name=\"Title for my container\",\n    abstract=\"This is my first Metador-compliant container\",\n    author=[container_author],\n    dateCreated=today,\n)\nprint(my_bibmeta.json(indent=2))  # look at what we have created\n</pre> container_author = Person(     id_ = \"https://orcid.org/0000-0002-1825-0097\",     givenName= \"Josiah\", familyName=\"Carberry\" )  my_bibmeta = BibMeta(     name=\"Title for my container\",     abstract=\"This is my first Metador-compliant container\",     author=[container_author],     dateCreated=today, ) print(my_bibmeta.json(indent=2))  # look at what we have created <pre>{\n  \"name\": \"Title for my container\",\n  \"@context\": \"https://w3id.org/ro/crate/1.1/context\",\n  \"@type\": \"Dataset\",\n  \"abstract\": \"This is my first Metador-compliant container\",\n  \"author\": [\n    {\n      \"@id\": \"https://orcid.org/0000-0002-1825-0097\",\n      \"name\": \"Josiah Carberry\",\n      \"@context\": \"https://w3id.org/ro/crate/1.1/context\",\n      \"@type\": \"Person\",\n      \"givenName\": \"Josiah\",\n      \"familyName\": \"Carberry\"\n    }\n  ],\n  \"dateCreated\": \"2023-09-06T12:47:43.119041\",\n  \"hasPart\": []\n}\n</pre> <p>After this brief detour about schema access and introspection, let us (finally!) add the bibliographic metadata object we created to the container:</p> In\u00a0[13]: Copied! <pre>with MetadorContainer(CONTAINER_NAME, \"a\") as mc:\n    print(\"Attached metadata before:\", mc[\"/\"].meta.keys())  # &lt;- empty set\n\n    mc[\"/\"].meta[\"core.bib\"] = my_bibmeta  # set the metadata\n\n    print(\"Attached metadata after:\", mc[\"/\"].meta.keys())  # &lt;- includes \"core.bib\" now\n    \n    # retrieve the metadata again and sanity-check:\n    retrieved = mc[\"/\"].meta[\"core.bib\"]\n    print(retrieved.dict())\n    print(\"Got back what we put in:\", my_bibmeta == retrieved)\n    print(\"Got back a BibMeta object:\", isinstance(retrieved, BibMeta))\n</pre> with MetadorContainer(CONTAINER_NAME, \"a\") as mc:     print(\"Attached metadata before:\", mc[\"/\"].meta.keys())  # &lt;- empty set      mc[\"/\"].meta[\"core.bib\"] = my_bibmeta  # set the metadata      print(\"Attached metadata after:\", mc[\"/\"].meta.keys())  # &lt;- includes \"core.bib\" now          # retrieve the metadata again and sanity-check:     retrieved = mc[\"/\"].meta[\"core.bib\"]     print(retrieved.dict())     print(\"Got back what we put in:\", my_bibmeta == retrieved)     print(\"Got back a BibMeta object:\", isinstance(retrieved, BibMeta)) <pre>Attached metadata before: dict_keys([])\nAttached metadata after: dict_keys(['core.bib'])\n{'name': 'Title for my container', '@context': 'https://w3id.org/ro/crate/1.1/context', '@type': 'Dataset', 'abstract': 'This is my first Metador-compliant container', 'author': [{'@id': AnyHttpUrl('https://orcid.org/0000-0002-1825-0097', ), 'name': 'Josiah Carberry', '@context': 'https://w3id.org/ro/crate/1.1/context', '@type': 'Person', 'givenName': 'Josiah', 'familyName': 'Carberry'}], 'dateCreated': datetime.datetime(2023, 9, 6, 12, 47, 43, 119041), 'hasPart': set()}\nGot back what we put in: True\nGot back a BibMeta object: True\n</pre> NOTE: All general metadata about the whole container and its contents should be attached directly to the container root group (i.e., path \"/\").  In\u00a0[14]: Copied! <pre># download an image from the internet:\nIMG_NAME = \"python-logo.png\"\nimg_file: bytes = urlopen(f\"https://www.python.org/static/img/{IMG_NAME}\").read()\n\n# we will need these values to construct the metadata:\nimg_w, img_h = 290, 82  # &lt;- size in pixels\nimg_mime = \"image/png\"  # &lt;- MIME type\nimg_sha256 = hashlib.sha256(img_file).hexdigest()  # &lt;- sha256 hashsum\n\n# look at our image:\nprint(IMG_NAME, \"sha256:\", img_sha256)\njupyter.display(jupyter.Image(data=img_file))\n</pre> # download an image from the internet: IMG_NAME = \"python-logo.png\" img_file: bytes = urlopen(f\"https://www.python.org/static/img/{IMG_NAME}\").read()  # we will need these values to construct the metadata: img_w, img_h = 290, 82  # &lt;- size in pixels img_mime = \"image/png\"  # &lt;- MIME type img_sha256 = hashlib.sha256(img_file).hexdigest()  # &lt;- sha256 hashsum  # look at our image: print(IMG_NAME, \"sha256:\", img_sha256) jupyter.display(jupyter.Image(data=img_file)) <pre>python-logo.png sha256: a92254c92269252313148acd7c3a8d595fa6cc8921ec28b2ee0f5c75f84297c3\n</pre> In\u00a0[15]: Copied! <pre># get the default basic metadata schema for image files\nImageFileMeta = schemas[\"core.imagefile\"]\n\n# prepare the metadata\nimg_meta = ImageFileMeta(\n    filename=IMG_NAME,  # preserve the original filename (we'll rename it in the container)\n    \n    # following is required for all files:\n    contentSize=len(img_file),\n    encodingFormat=img_mime,\n    sha256=img_sha256,\n\n    # following is required for all images:\n    width=img_w,\n    height=img_h,\n    )\n\nprint(img_meta.yaml())  # look at what the schema made out of it\n</pre> # get the default basic metadata schema for image files ImageFileMeta = schemas[\"core.imagefile\"]  # prepare the metadata img_meta = ImageFileMeta(     filename=IMG_NAME,  # preserve the original filename (we'll rename it in the container)          # following is required for all files:     contentSize=len(img_file),     encodingFormat=img_mime,     sha256=img_sha256,      # following is required for all images:     width=img_w,     height=img_h,     )  print(img_meta.yaml())  # look at what the schema made out of it <pre>'@context': https://w3id.org/ro/crate/1.1/context\n'@type': File\ncontentSize: 10102\nencodingFormat: image/png\nfilename: python-logo.png\nheight:\n  '@context': https://schema.org\n  '@type': QuantitativeValue\n  unitText: px\n  value: 82\nsha256: a92254c92269252313148acd7c3a8d595fa6cc8921ec28b2ee0f5c75f84297c3\nwidth:\n  '@context': https://schema.org\n  '@type': QuantitativeValue\n  unitText: px\n  value: 290\n\n</pre> In\u00a0[16]: Copied! <pre>try:\n    img_meta.width = \"42 meters\"\nexcept pydantic.ValidationError as e:\n    print(e)\n</pre> try:     img_meta.width = \"42 meters\" except pydantic.ValidationError as e:     print(e) <pre>2 validation errors for ImageFileMeta\nwidth -&gt; __root__ -&gt; 0\n  value is not a valid integer (type=type_error.integer)\nwidth -&gt; __root__ -&gt; 0\n  value is not a valid float (type=type_error.float)\n</pre> <p>This demonstrates how Metador tries to help the user avoiding mistakes whenever it can detect them, to ensure that containers have metadata which is as correct and as complete as possible. After this little interlude, let us proceed and finally put the image file and its prepared metadata into our container:</p> In\u00a0[17]: Copied! <pre>import numpy as np\n\n# put image into the container\nwith MetadorContainer(CONTAINER_NAME, \"a\") as mc:\n    # embed image in the container\n    #\n    # NOTE: raw bytes embedded into HDF5 must be wrapped in np.void, see:\n    # https://docs.h5py.org/en/latest/strings.html#how-to-store-raw-binary-data\n    mc[\"/images/some_image\"] = np.void(img_file) \n    \n    # attach image file metadata\n    mc[\"/images/some_image\"].meta[\"core.imagefile\"] = img_meta\n</pre> import numpy as np  # put image into the container with MetadorContainer(CONTAINER_NAME, \"a\") as mc:     # embed image in the container     #     # NOTE: raw bytes embedded into HDF5 must be wrapped in np.void, see:     # https://docs.h5py.org/en/latest/strings.html#how-to-store-raw-binary-data     mc[\"/images/some_image\"] = np.void(img_file)           # attach image file metadata     mc[\"/images/some_image\"].meta[\"core.imagefile\"] = img_meta NOTE: You might wonder now: is creating metadata always so cumbersome? No, you will usually not construct metadata for each file like this, we only did this for didactic purposes in order to demonstrate the validation and normalization capabilities built into schemas. You usually will use a harvester pipeline to assemble metadata, and might want to write harvesters for new file types yourself. Harvesters will be introduced in a later tutorial.  In\u00a0[18]: Copied! <pre>FileMeta = schemas.get(\"core.file\", (0,1,0))\n\nwith MetadorContainer(CONTAINER_NAME, \"r\") as mc:\n    img = mc[\"/images/some_image\"]  # get node of the embedded image\n    \n    # list attached metadata\n    print(list(img.meta.keys()))  # only lists the schemas as-added!\n    \n    print(\"Has core.imagefile metadata:\", \"core.imagefile\" in img.meta)  # as expected\n    image_meta = img.meta[\"core.imagefile\"]\n        \n    print(\"Has core.file metadata:\", \"core.file\" in img.meta)  # surprise!\n    file_meta = img.meta[\"core.file\"]  # any idea what happens now?\n    \n    # Let's figure it out:\n    print(\"Type of image_meta:\", repr(type(image_meta)))\n    print(\"Type of file_meta:\", repr(type(file_meta)))\n    print(\"file_meta is an instance of ImageFileMeta:\", isinstance(file_meta, ImageFileMeta))\n    print(\"image_meta is an instance of FileMeta:\", isinstance(image_meta, FileMeta))\n</pre> FileMeta = schemas.get(\"core.file\", (0,1,0))  with MetadorContainer(CONTAINER_NAME, \"r\") as mc:     img = mc[\"/images/some_image\"]  # get node of the embedded image          # list attached metadata     print(list(img.meta.keys()))  # only lists the schemas as-added!          print(\"Has core.imagefile metadata:\", \"core.imagefile\" in img.meta)  # as expected     image_meta = img.meta[\"core.imagefile\"]              print(\"Has core.file metadata:\", \"core.file\" in img.meta)  # surprise!     file_meta = img.meta[\"core.file\"]  # any idea what happens now?          # Let's figure it out:     print(\"Type of image_meta:\", repr(type(image_meta)))     print(\"Type of file_meta:\", repr(type(file_meta)))     print(\"file_meta is an instance of ImageFileMeta:\", isinstance(file_meta, ImageFileMeta))     print(\"image_meta is an instance of FileMeta:\", isinstance(image_meta, FileMeta)) <pre>['core.imagefile']\nHas core.imagefile metadata: True\nHas core.file metadata: True\nType of image_meta: &lt;class 'metador_core.schema.common.ImageFileMeta'&gt; (core.imagefile 0.1.0)\nType of file_meta: &lt;class 'metador_core.schema.common.rocrate.FileMeta'&gt; (core.file 0.1.0)\nfile_meta is an instance of ImageFileMeta: False\nimage_meta is an instance of FileMeta: True\n</pre> <p>If you are familiar with class inheritance in object-oriented programming, the outputs are not surprising, as this is exactly what is happening here. We will not go into technical details about Metador schema inheritance, but give a brief example why this is a very useful feature to have in the context of schemas.</p> <p>Imagine that you work with a very niche image file format <code>.niche</code> and want to embed such a file in a container. As a progressively thinking scientist who cares about FAIRness of their data, you want to re-use existing schemas and standards. Therefore, you design the schema <code>niche-imagefile</code> for your niche format as a specialization of the <code>core.imagefile</code> (the one we used above), with some extra properties about the niche encoding of your image data that you want to store alongside the image.</p> <p>Unfortunately, other researchers who might get a copy of your container will not have your <code>niche-imagefile</code> schema available. Luckily, they do have the schemas <code>core.file</code> and <code>core.imagefile</code>. As both schemas are \"ancestor\" schemas of your schema, the researchers still can access general information about your image, such as <code>width</code> and <code>height</code>, by requesting <code>core.imagefile</code> metadata. If they care about specifics of your image, they might later consider installing your schema plugin to access all of the information.</p> <p>But how do they even know that your <code>.niche</code> file, in fact, is a well-described image file at all?</p> In\u00a0[19]: Copied! <pre>with MetadorContainer(CONTAINER_NAME, \"r\") as mc:\n    \n    print(\"Schemas available in the container and Python package+version providing it:\")\n    \n    for schema in mc.metador.schemas:\n        # The provider package is the one that was used when creating the container,\n        # so in general it might be different from the one you have installed!\n        pkg = mc.metador.schemas.provider(schema)\n        print(schema.name, schema.version,  \":\", pkg.name, pkg.version)\n</pre> with MetadorContainer(CONTAINER_NAME, \"r\") as mc:          print(\"Schemas available in the container and Python package+version providing it:\")          for schema in mc.metador.schemas:         # The provider package is the one that was used when creating the container,         # so in general it might be different from the one you have installed!         pkg = mc.metador.schemas.provider(schema)         print(schema.name, schema.version,  \":\", pkg.name, pkg.version) <pre>Schemas available in the container and Python package+version providing it:\ncore.bib (0, 1, 0) : metador-core (0, 1, 1)\ncore.imagefile (0, 1, 0) : metador-core (0, 1, 1)\n</pre> <p>Our container has actual metadata objects of types <code>\"core.bib\"</code> and <code>\"core.imagefile\"</code>. By now, you can probably guess that the other two listed schemas are respective parent schemas. So even if we did not know anything about this container before, now we know that we e.g. can ask for files:</p> In\u00a0[20]: Copied! <pre>with MetadorContainer(CONTAINER_NAME, \"r\") as mc:\n    \n    # query all nodes in the container for \"core.file\"-compatible metadata\n    for node in mc.metador.query(\"core.file\"):\n        metadata = node.meta.get(\"core.file\")\n        # print the path of the embedded file and its hashsum\n        # (core.file has hashsum as mandatory attribute - all files must have it!)\n        print(node.name, \":\", metadata.sha256)\n</pre> with MetadorContainer(CONTAINER_NAME, \"r\") as mc:          # query all nodes in the container for \"core.file\"-compatible metadata     for node in mc.metador.query(\"core.file\"):         metadata = node.meta.get(\"core.file\")         # print the path of the embedded file and its hashsum         # (core.file has hashsum as mandatory attribute - all files must have it!)         print(node.name, \":\", metadata.sha256)  <pre>/images/some_image : a92254c92269252313148acd7c3a8d595fa6cc8921ec28b2ee0f5c75f84297c3\n</pre> <p>So we have successfully created a container, added basic bibilographic metadata to it, embedded an image file and also attached some metadata about it to help other people finding it. This concludes our first lesson giving you a first taste of Metador. We hope that you enjoyed it and we will see you again in the next lesson!</p>"},{"location":"notebooks/01_MetadorContainer/#using-metadorcontainer-to-manage-metadata-in-hdf5-files","title":"Using MetadorContainer to manage Metadata in HDF5 files\u00b6","text":"<p>This tutorial is intended for people who already have a basic idea of what Metador is and the value it can provide. Probably now you simply want to jump right in and get started as quickly as possible. These lessons will guide you through the main features and interfaces of the Metador framework step by step and will explain how to perform common tasks which are needed to implement a new use-case or connect a scientific domain to the Metador ecosystem.</p> <p>Learning Goals:</p> <ul> <li>Understand the main features provided by the <code>MetadorContainer</code> interface</li> <li>Learn to use basic metadata schemas in order to attach metadata objects to data in a container</li> <li>Learn to discover and access metadata stored in a possibly unknown container</li> </ul>"},{"location":"notebooks/01_MetadorContainer/#introduction","title":"Introduction\u00b6","text":"<p>Making very domain-specific and heterogeneaus research datasets more FAIR is hard. The goal of the Metador platform is to support standardization efforts of domain-specific research data and metadata and make reaping the benefits of these efforts as easy as possible.</p> <p>As in most domains high-volume scientific data is numerical, Metador containers are built on top of the well-established and mature HDF5 format, which is optimized for this kind of data. Thus, Metador is essentially built around HDF5 archives of research data where metadata is organized in a certain way, so that various components of the Metador framework are able to make sense of your data and provide features based on that information. Luckily, you don't need to know most of the technical details to create valid Metador containers with rich metadata - you just have to think about your metadata needs, Metador will do the bookkeeping behind the scenes.</p> <p>In fact, if you have used the <code>h5py</code> library in Python to archive your data, then you already know almost everything you need to know to create Metador containers. If you have never used <code>h5py</code>, please familiarize yourself with h5py and come back later, when you have a working understanding of <code>h5py.File</code>, <code>h5py.Group</code>, <code>h5py.Dataset</code> and how to use them to store and access data in HDF5 files.</p> <p>Now, assuming that you are familiar with <code>h5py</code>, you can think of a Metador container as a special <code>h5py.File</code>. Only instead of storing simple key-value pairs as \"metadata\" using <code>.attrs</code>, you are enabled to add rich, complex, structured metadata through a new property of <code>Dataset</code>s and <code>Group</code>s, which is called <code>.meta</code>. Note that you still can use <code>.attrs</code>, if you need to, but the Metador framework will completely ignore them. When you work with Metador containers, it is best to consider <code>.attrs</code> as a legacy feature that is to be avoided.</p> NOTE: As metadata can be attached to both Groups and Datasets, for the sake of brevity we refer to both of these simply as Nodes of the container."},{"location":"notebooks/01_MetadorContainer/#creating-or-opening-a-container","title":"Creating or Opening a Container\u00b6","text":"<p>To work with Metador containers, you simply can use <code>MetadorContainer</code> instead of <code>h5py.File</code> whenever you create or open an HDF file.</p> <p>On the first sight, you will notice that a <code>MetadorContainer</code> seems to behave like a normal <code>h5py.File</code>:</p>"},{"location":"notebooks/01_MetadorContainer/#babys-first-metadata-plugins-and-schemas","title":"Baby's First Metadata: Plugins and Schemas\u00b6","text":"<p>In contrast to other systems that allow you to attach arbitrary key-value pairs as \"metadata\" and call that \"being generic\", Metador provides a generic metadata system that still prioritizes standards, structure and correctness. Therefore, all metadata must we valid according to some known metadata schema. These schemas are used for automatic validation, i.e. checking that the metadata has the right shape, data types and seems plausible. You simply cannot attach invalid metadata to container nodes. The requirement to have schemas for all metadata is undeniably additional work, but it is work that pays off - we promise!</p> <p>As with any kind of standard, the value of a schema is proportional to the number of its users. In order to minimize the required extra work and at the same time maximize its value, Metador is designed as an extensible and open framework that encourages the sharing and reuse of various elements, including schemas.  For very general kinds of data, Metador provides metadata schemas that can and should be used or extended. For your domain-specific needs, you will first need to understand your metadata, and then formalize this understanding by using, extending or creating a metadata schema plugin (this will be the topic of a different lesson). In this lesson, we will only use the default schemas that are always available to you.</p>"},{"location":"notebooks/01_MetadorContainer/#accessing-metador-plugin-groups-and-plugins","title":"Accessing Metador Plugin Groups and Plugins\u00b6","text":"<p>All installed plugins, including all schemas that you can use in your current Python environment, can be accessed through the Metador plugin system living in <code>metador_core.plugins</code>. You can list all different kinds of plugins that are available like this:</p>"},{"location":"notebooks/01_MetadorContainer/#accessing-available-metadata-schemas","title":"Accessing Available Metadata Schemas\u00b6","text":"<p>A proper scientific dataset should, at the very least, provide some bibliographic metadata - after all, you want to be credited for it, and should credit the shoulders you are standing on. Often, such metadata is requested during the submission into a public repository and is stored \"outside\" of the published data. Metador is a dataset-centric system that encourages the creation of archives which are self-describing and self-contained. Metadata explaining the contents should be stored alongside the data and be readily available to anyone who gets a hold of a copy of the container, even if the repository is not available anymore 50 years from now.</p> <p>First we need to load the default schema plugin for bibliographic metadata. If you simply want to get any version, you can access the group like a dict:</p>"},{"location":"notebooks/01_MetadorContainer/#creating-a-metadata-object-by-instantiating-a-schema","title":"Creating a metadata object by instantiating a schema\u00b6","text":"<p>Now let's use the schema we loaded to add some bibliographic information to our container!</p>"},{"location":"notebooks/01_MetadorContainer/#exploring-and-inspecting-a-schema","title":"Exploring and Inspecting a Schema\u00b6","text":"<p>But how can you know what kind of data a schema wants you to provide? What if the schema is complicated and nested? How do you know what schemas or classes you need? To answer all these questions, each schema can be inspected through the special <code>Fields</code> attribute:</p>"},{"location":"notebooks/01_MetadorContainer/#we-need-more-data-embedding-files-in-a-metadorcontainer","title":"We Need More Data: Embedding Files in a MetadorContainer\u00b6","text":"<p>Even though Metador is based on HDF5, you might still want to (and in fact should!) store all files that are relevant for your dataset inside the container, even if the file contents are not a multidimensional numerical table. A <code>MetadorContainer</code> thus can also be seen as a more powerful substitute for simply archiving your project directory into a ZIP file and should contain everything that is helpful to other researchers to understand and work with your data.</p> High-level Guideline for Container Creation: <ul><li>     Embed metadata in a container using the .meta interface and Metador schemas whenever possible     </li><li>     Store numerical data as numpy.ndarrays (i.e. as usual with h5py)     and add suitable metadata, e.g. using the core.table schema     </li><li>     Embed any other data (or metadata where no Metador schemas exist yet) as raw bytes     and add metadata compatible with the core.file schema     </li></ul> <p>We will now demonstrate how to embed files for a very common use-case - images. Images can be scientific data themselves, be a visualization of scientific data or simply provide a thumbnail, so it is very likely that you will want to include some in your container.</p>"},{"location":"notebooks/01_MetadorContainer/#getting-an-image-file","title":"Getting an Image File\u00b6","text":"<p>In practice, you will probably have some images stored on your computer that you would like to embed. For demonstration purposes, we will simply grab an image file from the internet and prepare all the information that we will need. As an exercise, feel free to adapt the following code for one of your images.</p>"},{"location":"notebooks/01_MetadorContainer/#constructing-the-metadata","title":"Constructing the Metadata\u00b6","text":"<p>Now we are ready to construct the metadata object for the image gathering all the pieces of information:</p>"},{"location":"notebooks/01_MetadorContainer/#interlude-some-notes-on-metadata-serialization-and-parsing","title":"Interlude: Some Notes on Metadata Serialization and Parsing\u00b6","text":"<p>By now, you have seen all three ways of serializing a metadata object:</p> <ul> <li><code>.dict()</code>, returns a plain Python <code>dict</code> containing the metadata</li> <li><code>.json()</code>, returns a string with a valid JSON encoding of the metadata</li> <li><code>.yaml()</code>, returns a string with a valid YAML encoding of the metadata</li> </ul> <p>When you <code>print</code> a metadata object, you will see a pretty-printed <code>json</code> serialization.</p> <p>You never have to use the serialization functions explicitly when interfacing with a <code>MetadorContainer</code>, but they are helpful for inspecting or extracting metadata stored in a container, i.e. to print it (as we do here) or save a copy of the metadata in a separate file. These are not Metador-specific features, but are provided by the pydantic library which is the foundation for all Metador schemas. Once you need to write your own schemas, you might want to familiarize yourself with this library.</p> <p>Furthermore, observe what happened to the simple <code>width</code> and <code>height</code> values that we provided when constructing the metadata object:</p> <ul> <li>the values were parsed into objects representing schema.org-compliant <code>QuantitativeValue</code></li> <li>the values were enriched with the information that they denote pixels (<code>px</code>)</li> </ul> <p>Such inference is possible in situations when there is a unique correct unit expected by the used schema for some attribute, as we have in this case. This demonstrates how Metador tries to simplify creation of rich metadata whenever it can infer some piece of knowledge encoded in the used schema.</p> <p>Now see what happens if we try to assign a value with a wrong unit:</p>"},{"location":"notebooks/01_MetadorContainer/#finding-metadata-schema-inheritance-and-the-table-of-contents","title":"Finding Metadata: Schema Inheritance and the Table of Contents\u00b6","text":"<p>By now, you have seen that you can attach metadata to groups and datasets very much like with the HDF5-native <code>.attrs</code>, only that the <code>.meta</code> interface is based on schema plugins that provide additional valuable features for complex scientific metadata needs, such as validation and semantic enrichment. In this section, we will look at the remaining features that <code>MetadorContainer</code> provides.</p>"},{"location":"notebooks/01_MetadorContainer/#schema-inheritance","title":"Schema Inheritance\u00b6","text":"<p>As Metador encourages reuse, it supports schema inheritance, i.e. subtyping. A schema plugin can be a specialization of a different, more general schema that we call the parent schema. Conversely, each object stored for a more specialized schema can be interpreted as an object of the parent schema, ignoring \"extra stuff\" that make the child schema for specific.</p> <p>In the previous section, we recommended to attach to each embedded file an object compatible with the <code>core.file</code> schema, while in the example we embedded an image attaching metadata based on the <code>core.imagefile</code> schema. This works because the <code>core.imagefile</code> schema is a specialization, i.e. a child schema based on <code>core.file</code>. Let us now understand what exactly this entails:</p>"},{"location":"notebooks/01_MetadorContainer/#table-of-contents","title":"Table of Contents\u00b6","text":"<p>The container table of contents to the rescue! Its main feature is the ability to query available metadata in the container. When exploring an unknown container, we might not even know what we want to look at yet. The TOC can list all schemas that are available in the container, to give you an idea what you can query for:</p>"},{"location":"notebooks/01_MetadorContainer/#tldr-summary","title":"TL;DR: Summary\u00b6","text":""},{"location":"notebooks/01_MetadorContainer/#opening-and-closing-containers","title":"Opening and Closing Containers\u00b6","text":"<ul> <li>Metador containers are essentially \"special\" HDF5 files</li> <li>Use the Python <code>with</code> syntax to avoid technical problems</li> <li>Wrap your <code>h5py.File</code> objects with <code>MetadorContainer</code> and use only the wrapped object</li> </ul>"},{"location":"notebooks/01_MetadorContainer/#attaching-and-accessing-metadata","title":"Attaching and Accessing Metadata\u00b6","text":"<ul> <li>Do not use <code>.attrs</code> for storing metadata, unless you have specific reasons (e.g. compliance to another standard)</li> <li>Do use <code>.meta</code> (provided by <code>MetadorContainer</code>) to attach metadata to container nodes</li> <li>Metador metadata is always validated against a metadata schema</li> <li>Usable metadata schemas (special Python classes) are provided by the Metador plugin system</li> <li>Metadata is stored and retrieved through objects that are instances of these schemas</li> <li>You can retrieve a more metadata object as any of its parent schemas (schema inheritance)</li> </ul>"},{"location":"notebooks/01_MetadorContainer/#inspecting-metadata-schemas","title":"Inspecting Metadata Schemas\u00b6","text":"<ul> <li>Use <code>.Fields</code> on a schema that you use to get information about expected types and nested schemas</li> <li>Through <code>SomeSchema.Fields.fieldX.schemas</code> you can access nested schemas that might not be exposed as separate plugins</li> <li>Always use this mechanism instead of trying to <code>import</code> these inner schemas</li> </ul>"},{"location":"notebooks/01_MetadorContainer/#using-metadata-standards","title":"Using Metadata Standards\u00b6","text":"<ul> <li>The default metadata schemas are designed for compliance with schema.org and ROCrate</li> <li>All additional plugin schemas should also strive to reuse existing schemas and semantic standards</li> <li>For general metadata, try to follow RO-Crate recommentations (e.g. that researchers should be referenced by their ORCID)</li> <li>General metadata about the whole container should be attached to the container root group</li> </ul>"},{"location":"notebooks/01_MetadorContainer/#searching-for-metadata","title":"Searching for Metadata\u00b6","text":"<ul> <li>The container table of contents is accessible with <code>.metador</code> and can be used to find interesting (meta)data</li> <li>Use <code>.metador.schemas</code> to see all available metadata types in the container</li> <li>Use <code>.metador.query(...)</code> to iterate through all nodes that have a certain kind of metadata</li> <li>The <code>.metador.query(...)</code> operation also supports schema inheritance</li> </ul>"},{"location":"notebooks/02_YourFirstMetadorPlugin/","title":"Writing Metador Plugins","text":"In\u00a0[1]: Copied! <pre>class MyNewPlugin:\n    \n    class Plugin:\n        name = \"my.newplugin\"\n        version = (0, 1, 0)\n        # (... possibly other plugin group specific declarations ...)\n        \n    # (... all methods required by the plugin group ...)\n    \n    # (... implementation details of YOUR plugin ...)\n</pre> class MyNewPlugin:          class Plugin:         name = \"my.newplugin\"         version = (0, 1, 0)         # (... possibly other plugin group specific declarations ...)              # (... all methods required by the plugin group ...)          # (... implementation details of YOUR plugin ...) In\u00a0[2]: Copied! <pre>\"\"\"Metador schema plugins provided by this package.\"\"\"\nfrom metador_core.schema import MetadataSchema\nfrom metador_core.schema.types import Int, Str\n\nclass MyFirstSchema(MetadataSchema):\n\n    class Plugin:\n        name = \"dummy.my-first-schema\"\n        version = (0, 1, 0)\n\n    magic_number: Int\n    some_text: Str = \"(no text)\"\n</pre> \"\"\"Metador schema plugins provided by this package.\"\"\" from metador_core.schema import MetadataSchema from metador_core.schema.types import Int, Str  class MyFirstSchema(MetadataSchema):      class Plugin:         name = \"dummy.my-first-schema\"         version = (0, 1, 0)      magic_number: Int     some_text: Str = \"(no text)\" <p>You can see that the minimal schema plugin is very close to the general \"plugin skeleton\" we discussed above. What makes this a schema plugin is that our plugin class is a subclass of <code>MetadataSchema</code> and the inner <code>Plugin</code> class is a subclass of <code>SchemaPlugin</code> (which itself is a subclass of <code>PluginBase</code> that you have seen above). These are the minimal requirements imposed by the schema plugin group.</p> <p>Schemas are defined mostly by using Python type hints - if you are familiar with dataclasses, then think of schemas as very fancy dataclasses. In another tutorial we will take a deep dive into schema development, but for now it is enough to know that our schema expects a metadata object that requires a field called <code>magic_number</code>, which must be an integer value, and also supports an optional field <code>some_text</code>, which if provided will override the default value <code>\"(no text)\"</code>.</p> In\u00a0[3]: Copied! <pre>try:\n    \n    # run these two lines in your python interpreter:\n    from metador_core.plugins import schemas           # A\n    MyFirstSchema = schemas[\"dummy.my-first-schema\"]   # B\n    # ----\n    \n    print(\"Congratulations, your new plugin was registered correctly! :)\")\nexcept KeyError:\n    print(\"Your plugin was not found :(\")\n</pre> try:          # run these two lines in your python interpreter:     from metador_core.plugins import schemas           # A     MyFirstSchema = schemas[\"dummy.my-first-schema\"]   # B     # ----          print(\"Congratulations, your new plugin was registered correctly! :)\") except KeyError:     print(\"Your plugin was not found :(\") <pre>Your plugin was not found :(\n</pre> <p>Assuming that you test in the regular <code>python3</code> shell:</p> <ul> <li>If line A fails, then you are probably not in the correct virtual environment, because the interpreter cannot find <code>metador_core</code>.</li> <li>If line B fails, than you either did a mistake with the entry point in the <code>pyproject.toml</code> file, or forgot to run <code>poetry install</code>.</li> </ul> <p>You might wonder why we always access a schema through the plugin system, instead of simply importing it. One reason is that we wanted to verify that the plugin is registered correctly. Another reason is that plugin code can be moved by developers within a package, or even into completely different package. You should not need to know where a plugin comes from in order to use it, this is one of the main purposes of the Metador plugin system.</p>  When working with plugins that are not provided by yourself (and even then), access them through the plugin system instead of importing them directly, in order to avoid breakage of your code only because something was \"moved\" to a different location!  <p>If you want, now you can try instantiating your schema in various ways to get a feeling for it:</p> In\u00a0[4]: Copied! <pre>print(MyFirstSchema(magic_number=42))\nprint(MyFirstSchema(magic_number=23, nonsense=True))\nprint(MyFirstSchema(magic_number=1, some_text=\"hello\"))\n</pre> print(MyFirstSchema(magic_number=42)) print(MyFirstSchema(magic_number=23, nonsense=True)) print(MyFirstSchema(magic_number=1, some_text=\"hello\")) <pre>{\n  \"magic_number\": 42,\n  \"some_text\": \"(no text)\"\n}\n{\n  \"magic_number\": 23,\n  \"some_text\": \"(no text)\",\n  \"nonsense\": true\n}\n{\n  \"magic_number\": 1,\n  \"some_text\": \"hello\"\n}\n</pre> <p>As an exercise, you can create a <code>MetadorContainer</code> and attach an instance of your metadata schema to a node - your schema is on equal footing with the default schemas you have already seen, and it can be used in exactly the same ways. In a later tutorial, you will learn how to make use of schema inheritance and make your schema aligned with semantic standards.</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#your-first-metador-plugin","title":"Your First Metador Plugin\u00b6","text":"<p>In this tutorial you will learn how Metador plugins are defined by creating a new Python package that provides a simple custom metadata schema. We will keep it simple and focus on the general aspects which apply to any kind of Metador plugin. In later tutorials, we will discuss all the specifics for different kinds of plugins in much more depth.</p> <p>Prerequisites:</p> <ul> <li>Working knowledge of the Linux shell (basic navigation and using CLI tools)</li> <li>Working knowledge of version control with <code>git</code> (creating and managing a repository)</li> </ul> <p>Learning Goals:</p> <ul> <li>Learn how to create a new Python package that provides new metador plugins</li> <li>Learn to define and register a new plugin providing a simple metadata schema</li> </ul>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#creating-a-new-python-package","title":"Creating a New Python Package\u00b6","text":"<p>Metador plugins use the standard <code>entrypoint</code> system which is widely supported and used in various Python projects. In order to use this system and correctly register plugins, you cannot just write some Python files, but must organize them as a proper Python package.</p>      If you already know how to create a Python package     (i.e. something you can upload to PyPI and pip install)     and know what an entrypoint is, feel free to create a fresh Python package in     the way which is familiar or comfortable to you. In that case, adapt all general     steps to your setup. If you do this, we cannot provide any assistance if you encounter problems.     Therefore, if you are not sure, stick with the tutorial for reproducible results.  <p>We recommend to create Python packages using a tool called <code>poetry</code>. It is also the tool used for the development of <code>metador-core</code>. If you have no experience with entrypoints and Python packages, don't worry - we will guide you through the process of setting up a new package.</p>      This is not a Linux shell, git or poetry tutorial!     If you have general questions about these things, consult the corresponding documentation."},{"location":"notebooks/02_YourFirstMetadorPlugin/#install-poetry","title":"Install Poetry\u00b6","text":"<p>First, make sure that you have <code>poetry</code> available. To install it, follow the steps in the documentation (this is important - it is not supposed to be installed using <code>pip install</code>).  Check that poetry is installed by running <code>poetry --version</code>, which should reply with something like <code>Poetry version 1.1.14</code>.</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#create-the-package","title":"Create the Package\u00b6","text":"Metador requires Python 3.8 or newer, consider using pyenv to install a recent version if you do not have one available!  <p>First, navigate on the command line to the directory where you want to create your new metador plugin package, e.g.:</p> <pre>cd ~/projects                 # &lt;- replace with path where your project should live\npoetry new my-metador-plugin\ncd my-metador-plugin          # &lt;- this directory was created by poetry for you\n</pre> <p>You should find yourself in your fresh project directory which already contains:</p> <ul> <li>a directory <code>my_metador_plugin</code> where your code will live</li> <li>a directory <code>tests</code> where you can place all the tests for your code (hopefully many!)</li> <li>an empty file <code>README.rst</code> (as a reminder that you should write a README)</li> <li>a <code>pyproject.toml</code> file that tracks all dependencies to other packages (!!!)</li> </ul>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#put-the-directory-under-version-control","title":"Put the directory under version control\u00b6","text":"<p>We will not discuss this further, but at this point you probably should run <code>git init</code> in the project directory, add a <code>.gitignore</code> which is suitable for Python projects (e.g. this one) and do the first commit. In the future, also make sure to include changes to a file called <code>poetry.lock</code> in your commits (if there is one). We assume that you are able to take care of proper version control for your project and will not mention it anymore.</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#add-general-information-to-pyprojecttoml","title":"Add general information to <code>pyproject.toml</code>\u00b6","text":"<p>Make sure that the automatic <code>authors</code> entry is correct and has an e-mail address that can be used to contact you.</p> <p>Write a brief <code>description</code> of what this package will provide.</p> <p>If you already have connected your local directory to a remote public git repository hosting service, such as GitLab or GitHub, also add the URL as <code>repository_url</code> just under the other fields under <code>[tool.poetry]</code> - this information is used by Metador to help users to get your plugin in case they need it.</p> <p>If you have some time, feel free to also add other package metadata.</p>      Please remember to keep this package metadata information up-to-date!"},{"location":"notebooks/02_YourFirstMetadorPlugin/#enter-the-virtual-environment","title":"Enter the virtual environment\u00b6","text":"<p>Run <code>poetry shell</code> to activate a project-specific virtual environment. Poetry will create one for you, if it does not exist yet.</p> <p>You see that you are in a virtual environment, because your command prompt in the terminal will begin with something like <code>(my-metador-plugin-KQKVg0oX-py3.8)</code>. The name of all virtual environments which poetry creates is always starting with your project name and will contain some automatic identifier (like <code>KQKVg0oX</code>).</p> <p>In case you are used to running <code>activate</code> and <code>deactivate</code> for your virtual environments - these commands are not used together with poetry. Instead:</p> <ul> <li>Use <code>poetry shell</code> inside the project directory to activate (whenever you want to work on your package)</li> <li>Use <code>exit</code> anywhere to deactivate (e.g. when you want to switch to another project or some custom environment)</li> </ul>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#add-metador-core-as-a-dependency","title":"Add <code>metador-core</code> as a dependency\u00b6","text":"<p>Run the following command to add <code>metador-core</code> as a dependency to your project:</p> <pre>poetry add git+ssh://git@github.com:Materials-Data-Science-and-Informatics/metador-core.git\n</pre>      When metador-core is published on PyPI, you can simply add metador-core to your dependencies.          Until then, as an early adopter you install the current version from the main branch.          Make sure that your public ssh key is properly registered in Github to access the private repository.  <p>If everything went smoothly (it can take a couple of minutes), then:</p> <ul> <li>the output of the poetry command contains a line like: <code> * Installing metador-core (...)</code></li> <li>the <code>pyproject.toml</code> has a new entry for <code>metador-core</code> under <code>[tool.poetry.dependencies]</code></li> </ul> <p>Now we are ready to get started with development!</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#the-metador-plugin-system","title":"The Metador Plugin System\u00b6","text":"<p>Before we go on and define our schema plugin, let us first take a closer look at the Metador plugin system.</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#plugin-groups-bags-of-similar-plugins","title":"Plugin Groups: Bags of Similar Plugins\u00b6","text":"<p>Every Metador plugin belongs to a plugin group, even plugin groups themselves are just plugins of a plugin group called <code>plugingroup</code> and can define new kinds of objects that can be provided as plugins (but this is an advanced topic that you most likely never will need to worry about). Probably the most important plugin group is the <code>schema</code> plugin group, as everything in Metador depends on schemas.</p> <p>In general, if you want to define a new well-behaved plugin, but do not know the specifics of the relevant plugin group yet and no other guidance is provided, a good place to start is the documentation of the class where the plugin group itself is defined, if no other documentation is provided. It should contain all important information for writing a suitable plugin, that is, one that provides the expected interface and behavior.</p> <p>The plugin system will try to check all plugins and validate them, to catch simple but common implementation mistakes. This includes things such as forgetting to implement a method or to set a required attribute. Nevertheless, you cannot rely on the automatic superficial checks. You are responsible for the correctness of your plugins, because most higher-level requirements for plugins cannot be checked automatically.</p>      Strictly follow the requirements for the interface and behavior of plugins expected by a plugin group, always write tests for your plugins."},{"location":"notebooks/02_YourFirstMetadorPlugin/#anatomy-of-a-plugin","title":"Anatomy of a Plugin\u00b6","text":"<p>Each plugin group defines their own requirements for what each plugin must provide, but there is a minimum of requirements shared by all plugins - they must provide a special inner class that defines at least the plugin name and version. So the common skeleton of a shared plugin looks like this:</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#plugin-naming","title":"Plugin naming\u00b6","text":"<p>The name of a plugin, aside of being meaningul, must satisfy two properties:</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#the-plugin-entrypoint-must-be-equal-to-the-name__xyz-ie-name-and-version-of-the-plugin","title":"The plugin entrypoint must be equal to the <code>name__x.y.z</code>, i.e. name and version of the plugin\u00b6","text":"<p>Luckily, this is one property that the plugin system will check for all plugins and warn you when you try to load them. Soon you will see how to declare the entrypoint for your plugin.</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#the-plugin-name-must-be-unique-within-its-plugin-group","title":"The plugin name must be unique within its plugin group\u00b6","text":"<p>This property cannot be checked automatically. To avoid problems, you should not use too general or too short names for your plugins. Otherwise, there is a risk that someone else will pick the same name for their plugin, which can lead to serious problems. To avoid or at least constrain this problem, you must add a \"namespace prefix\" to the names of all plugins that you define.</p>      Pick a suitable namespace prefix that you consistently use for all plugins that you develop!          This means: All your plugins should have names of the form MYPREFIX.PLUGINNAME,     where MYPREFIX is your chosen \"namespace prefix\".  <p>The prefix that you pick should be something that most likely other people will not use, but something short enough that people are not too annoyed by typing out the name of your plugin by hand (remember that metadata in containers is inspected by accessing the metadata based on the schema plugin name).</p> <p>Suitable choices for a prefix are:</p> <ul> <li>your last name</li> <li>your username on Github</li> <li>the short name of your employer organization or institute</li> </ul> <p>If you work for a larger organization, feel free to refine this namespacing approach to a suitable level that will prevent plugin name collisions, e.g. use plugin names such as <code>my-org.my-dept.my-plugin</code>.</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#plugin-versioning","title":"Plugin versioning\u00b6","text":"<p>All plugins must respect semantic versioning, so a version is a triple <code>MAJOR.MINOR.PATCH</code>. For different plugin groups this translates to slightly different requirements, which are usually explicitly spelled out for the specific context of a plugin group. In general, semantic versioning means:</p> <ul> <li>You increase <code>MAJOR</code> by 1 whenever other things could break if they update to the new version</li> <li>You increase <code>MINOR</code> by 1 whenever you added new features without breaking anything</li> <li>You increase <code>PATCH</code> by 1 whenever you fixed problems in your plugin without changing its intended behaviour</li> </ul> <p>The initial version you pick for your plugin is not important, but for consistency we recommend setting the first version to <code>0.1.0</code> (written as <code>(0, 1, 0)</code> in the example above), which is common practice in most software projects.</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#defining-the-schema-plugin","title":"Defining the Schema Plugin\u00b6","text":"<p>Now after gaining insight into the general workings of the Metador Plugin system, we are finally ready and can get to work.</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#write-the-schema","title":"Write the Schema\u00b6","text":"<p>In your project, create a new file <code>schemas.py</code> inside the <code>my_metador_plugin</code> directory (which currently only contains an <code>__init__.py</code> file) and add the following contents:</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#declare-the-entrypoint","title":"Declare the Entrypoint\u00b6","text":"<p>Now open your <code>pyproject.toml</code> file and define the entry point by adding these two lines (e.g. just before the <code>[build-system]</code> section):</p> <pre>[tool.poetry.plugins.metador_schema]\n'dummy.my-first-schema__0.1.0' = \"my_metador_plugin.schemas:MyFirstSchema\"\n</pre> <p>The first line says that we want to declare a <code>schema</code> plugin (for plugin group <code>X</code>, the section would be <code>[tool.poetry.plugins.metador_X]</code>).</p> <p>The second line declares the entry point, with our plugin name on the left and the location of our plugin on the right.</p> <p>The location string on the right corresponds to how the class is imported: <code>from my_metador_plugin.schemas import MyFirstSchema</code>.</p> <p>Finally, run <code>poetry install</code>, which will make poetry re-register your package and thus make the entrypoint known to the environment.</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#we-are-done","title":"We are done!\u00b6","text":"<p>In order to see if everything worked, make sure that you are still inside <code>poetry shell</code> (remember that we said that you should work inside of it!) and try to import your schema in the <code>python3</code> interpreter (lines A and B below).</p> <p>If you run this notebook in the same virtual environment which is used for the plugin package, you can just restart the notebook and evaluate the following cell:</p>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#summary","title":"Summary\u00b6","text":""},{"location":"notebooks/02_YourFirstMetadorPlugin/#python-packages","title":"Python Packages\u00b6","text":"<ul> <li>Metador plugins must be provided by Python packages using the entrypoint system</li> <li>The easiest and most modern way to create a package is using the <code>poetry</code> tool</li> <li>Entrypoints are declared within the <code>pyproject.toml</code> file that contains all package metadata</li> </ul>"},{"location":"notebooks/02_YourFirstMetadorPlugin/#metador-plugins","title":"Metador Plugins\u00b6","text":"<ul> <li>Each plugin belongs to a plugin group, which corresponds to an entry point group</li> <li>All plugins must have a unique name and possess a version tag that follows semantic versioning</li> <li>Plugins must satisfy additional requirements that depend of the respective plugin group</li> </ul>"},{"location":"notebooks/03_Schemas/","title":"Designing Metadata Schemas","text":"In\u00a0[1]: Copied! <pre>from metador_core.schema import MetadataSchema\nfrom metador_core.schema.types import Bool\n\nclass SimpleSchema(MetadataSchema):\n\"\"\"My new schema, totally unrelated to any other schemas.\"\"\"\n        \n    fun: Bool\n\"\"\"Flag whether the person reading this tutorial is having fun.\"\"\"\n    \nSimpleSchema(fun=True)  # instantiate schema / create metadata object\n</pre> from metador_core.schema import MetadataSchema from metador_core.schema.types import Bool  class SimpleSchema(MetadataSchema):     \"\"\"My new schema, totally unrelated to any other schemas.\"\"\"              fun: Bool     \"\"\"Flag whether the person reading this tutorial is having fun.\"\"\"      SimpleSchema(fun=True)  # instantiate schema / create metadata object Out[1]: <pre>SimpleSchema(fun=True)</pre> <p>As you have seen in the last tutorial, a schema can be exposed as a plugin (just as any other kind of Metador plugin) by:</p> <ul> <li>adding a <code>Plugin</code> inner class that (at least) defines a <code>name</code> and a <code>version</code></li> <li>declaring an entrypoint with the same <code>name</code> in the correct plugin group (for schemas: <code>\"metador_schema\"</code>)</li> </ul> <p>But there are still a number of questions before you can become productive with schema development, especially:</p> <ul> <li>how can you extend an existing schema correctly to get all the advantages provided by Metador?</li> <li>how can you express the requirements for the values that can go into all the different fields?</li> </ul> <p>By the end of this tutorial, you will have an answer to both of these and many other questions.</p> In\u00a0[2]: Copied! <pre>from metador_core.plugins import schemas\n\nImageFile = schemas[\"core.imagefile\"]  # &lt;- we stated no version\n\ntry:\n    class NicheImage(ImageFile):\n\"\"\"Schema for the .niche image format.\"\"\"\n    \nexcept TypeError as e:\n    print(e)\n</pre> from metador_core.plugins import schemas  ImageFile = schemas[\"core.imagefile\"]  # &lt;- we stated no version  try:     class NicheImage(ImageFile):         \"\"\"Schema for the .niche image format.\"\"\"      except TypeError as e:     print(e) <pre>NicheImage: Cannot inherit from plugin 'core.imagefile' of unspecified version!\n</pre> <p>To request a plugin with an expected version you want to use, access it using <code>get</code> and pass a version triple, like in the following example:</p> In\u00a0[3]: Copied! <pre>ImageFile = schemas.get(\"core.imagefile\", (0,1,0))  # &lt;- now this is better!\n\nclass NicheImage(ImageFile):\n\"\"\"Schema for the .niche image format.\"\"\"\n    \n    class Plugin:\n        name = \"dummy.imagefile.niche\"\n        version = (0, 1, 0)\n</pre> ImageFile = schemas.get(\"core.imagefile\", (0,1,0))  # &lt;- now this is better!  class NicheImage(ImageFile):     \"\"\"Schema for the .niche image format.\"\"\"          class Plugin:         name = \"dummy.imagefile.niche\"         version = (0, 1, 0) <p>In practice, you would also register your schema plugin as an entrypoint in your own Python package, as described in the previous tutorial. For practical purposes, we will bypass this step in this and following tutorials, and instead will manually \"load\" the schema into the plugin system using the register_in_group function. That way you can conveniently stay in this notebook and follow along, without needing to copy-paste everything into a project.</p> register_in_group is only used in the notebook tutorials, simply ignore it and do not try to use it in your actual project!  In\u00a0[4]: Copied! <pre>from metador_core.plugin.util import register_in_group\n\nregister_in_group(schemas, NicheImage)  # &lt;- only used in tutorials, must not and cannot be used in practice!\n</pre> from metador_core.plugin.util import register_in_group  register_in_group(schemas, NicheImage)  # &lt;- only used in tutorials, must not and cannot be used in practice! <pre>Notebook: Plugin 'dummy.imagefile.niche' registered in 'schema' group!\n</pre> <p>On the surface nothing exciting happened, internally though, your schema was processed the same way it would be when loaded from an entrypoint - many checks were performed, and now it is accessible through the plugin interface.</p> <p>Now the schema is registered, so let's try to access it through the plugin system:</p> In\u00a0[5]: Copied! <pre>MySchema = schemas[\"dummy.imagefile.niche\"]\nprint(MySchema)\n</pre> MySchema = schemas[\"dummy.imagefile.niche\"] print(MySchema) <pre>Schema &lt;class '__main__.NicheImage'&gt; (dummy.imagefile.niche 0.1.0) (version unspecified)\n========================================================================================\n\nDescription:\n------------\n\tSchema for the .niche image format.\nFields:\n-------\n\nid_\n\ttype: Annotated[Optional[NonEmptyStr], Field(alias='@id')]\n\torigin: metador_core.schema.ld.LDSchema\n\t\nname\n\ttype: Optional[Text]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tName, title or caption of the entity.\n\t\nalternateName\n\ttype: Optional[List[Text]]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tAlternative names of the entity.\n\t\nidentifier\n\ttype: Optional[Union[URL, Text]]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tArbitrary identifier of the entity.\n\t\t\n\t\tPrefer @id if the identifier is web-resolvable, or use more\n\t\tspecific fields if available.\n\t\nurl\n\ttype: Optional[URL]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tURL of the entity.\n\t\ndescription\n\ttype: Optional[Text]\n\torigin: metador_core.schema.common.schemaorg.Thing\n\tdescription:\n\t\tDescription of the entity.\n\t\nversion\n\ttype: Optional[Union[NonNegativeInt, Text]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tdescription:\n\t\tVersion of this work.\n\t\t\n\t\tEither an integer, or a version string, e.g. \"1.0.5\".\n\t\t\n\t\tWhen using version strings, follow https://semver.org\n\t\twhenever applicable.\n\t\t\n\t\ncitation\n\ttype: Optional[Set[Union[LDOrRef[CreativeWork], Text]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: CreativeWork, LDIdRef\n\tdescription:\n\t\tCitation or reference to another creative work, e.g.\n\t\tanother publication, scholarly article, etc.\n\t\nabstract\n\ttype: Optional[Text]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tdescription:\n\t\tA short description that summarizes the creative work.\n\t\nkeywords\n\ttype: Optional[Set[Text]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tdescription:\n\t\tKeywords or tags to describe this creative work.\n\t\nauthor\n\ttype: Optional[List[LDOrRef[OrgOrPerson]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: Person, Organization, LDIdRef\n\tdescription:\n\t\tPeople responsible for the work, e.g. in research,\n\t\tthe people who would be authors on the relevant paper.\n\t\ncontributor\n\ttype: Optional[List[LDOrRef[OrgOrPerson]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: Person, Organization, LDIdRef\n\tdescription:\n\t\tAdditional people who contributed to the work, e.g.\n\t\tin research, the people who would be in the acknowledgements\n\t\tsection of the relevant paper.\n\t\nmaintainer\n\ttype: Optional[List[LDOrRef[OrgOrPerson]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: Person, Organization, LDIdRef\n\t\nproducer\n\ttype: Optional[List[LDOrRef[OrgOrPerson]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: Person, Organization, LDIdRef\n\t\nprovider\n\ttype: Optional[List[LDOrRef[OrgOrPerson]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: Person, Organization, LDIdRef\n\t\npublisher\n\ttype: Optional[List[LDOrRef[OrgOrPerson]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: Person, Organization, LDIdRef\n\t\nsponsor\n\ttype: Optional[List[LDOrRef[OrgOrPerson]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: Person, Organization, LDIdRef\n\t\neditor\n\ttype: Optional[List[LDOrRef[Person]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: Person, LDIdRef\n\t\ndateCreated\n\ttype: Optional[DateOrDatetime]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\t\ndateModified\n\ttype: Optional[DateOrDatetime]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\t\ndatePublished\n\ttype: Optional[DateOrDatetime]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\t\ncopyrightHolder\n\ttype: Optional[LDOrRef[OrgOrPerson]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: Person, Organization, LDIdRef\n\t\ncopyrightYear\n\ttype: Optional[Int]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\t\ncopyrightNotice\n\ttype: Optional[Text]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\t\nlicense\n\ttype: Optional[Union[URL, LDOrRef[CreativeWork]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: CreativeWork, LDIdRef\n\t\nabout\n\ttype: Optional[Set[LDOrRef[Thing]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: Thing, LDIdRef\n\t\nsubjectOf\n\ttype: Optional[Set[LDOrRef[CreativeWork]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: CreativeWork, LDIdRef\n\t\nhasPart\n\ttype: Optional[Set[LDOrRef[CreativeWork]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: CreativeWork, LDIdRef\n\t\nisPartOf\n\ttype: Optional[Set[Union[URL, LDOrRef[CreativeWork]]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: CreativeWork, LDIdRef\n\t\nisBasedOn\n\ttype: Optional[Set[Union[URL, LDOrRef[CreativeWork]]]]\n\torigin: metador_core.schema.common.schemaorg.CreativeWork\n\tschemas: CreativeWork, LDIdRef\n\t\ncontentSize\n\ttype: &lt;class 'pydantic.types.StrictInt'&gt;\n\torigin: metador_core.schema.common.rocrate.FileMeta (plugin: core.file 0.1.0)\n\tdescription:\n\t\tSize of the object in bytes.\n\t\nsha256\n\ttype: &lt;class 'metador_core.schema.types.NonEmptyStr'&gt;\n\torigin: metador_core.schema.common.rocrate.FileMeta (plugin: core.file 0.1.0)\n\tdescription:\n\t\tSha256 hashsum string of the object.\n\t\nencodingFormat\n\ttype: MimeTypeStr\n\torigin: metador_core.schema.common.rocrate.FileMeta (plugin: core.file 0.1.0)\n\tdescription:\n\t\tMIME type of the file.\n\t\nwidth\n\ttype: Pixels\n\torigin: metador_core.schema.common.ImageFileMeta (plugin: core.imagefile 0.1.0)\n\tschemas: Pixels\n\tdescription:\n\t\tWidth of the image in pixels.\n\t\nheight\n\ttype: Pixels\n\torigin: metador_core.schema.common.ImageFileMeta (plugin: core.imagefile 0.1.0)\n\tschemas: Pixels\n\tdescription:\n\t\tHeight of the image in pixels.\n\t\nbitrate\n\ttype: Optional[Text]\n\torigin: metador_core.schema.common.schemaorg.MediaObject\n\tdescription:\n\t\tBitrate of the entity (e.g. for audio or video).\n\t\nduration\n\ttype: Optional[Duration]\n\torigin: metador_core.schema.common.schemaorg.MediaObject\n\tdescription:\n\t\tDuration of the entity (e.g. for audio or video).\n\t\nstartTime\n\ttype: Optional[TimeOrDatetime]\n\torigin: metador_core.schema.common.schemaorg.MediaObject\n\tdescription:\n\t\tPhysical starting time, e.g. of a recording or measurement.\n\t\nendTime\n\ttype: Optional[TimeOrDatetime]\n\torigin: metador_core.schema.common.schemaorg.MediaObject\n\tdescription:\n\t\tPhysical ending time, e.g. of a recording or measurement.\n\t\nfilename\n\ttype: NonEmptyStr\n\torigin: metador_core.schema.common.rocrate.FileMeta (plugin: core.file 0.1.0)\n\tdescription:\n\t\tOriginal name of the file in source directory.\n\t\n</pre> <p>So we have successfully registered a schema, but it will act exactly the same as the parent schema - we did not change or define any fields! Before we actually add some fields for our new image type, we first need to clarify the rules of schema inheritance and understand better how useful fields can be defined. We will revisit our niche image format again and complete it at the end of this tutorial.</p> In\u00a0[6]: Copied! <pre>schemas.parent_path(\"dummy.imagefile.niche\", (0,1,0))\n</pre> schemas.parent_path(\"dummy.imagefile.niche\", (0,1,0)) Out[6]: <pre>[PGSchema.PluginRef(group='schema', name='core.file', version=(0, 1, 0)),\n PGSchema.PluginRef(group='schema', name='core.imagefile', version=(0, 1, 0)),\n PGSchema.PluginRef(group='schema', name='dummy.imagefile.niche', version=(0, 1, 0))]</pre> <p>This tells us that <code>core.file</code> is the parent schema of <code>core.imagefile</code>, which in turn is the parent schema of our new schema.</p> <p>We can also check the \"descendant\" schemas of a schema, like this:</p> In\u00a0[7]: Copied! <pre>schemas.children(\"core.file\", (0,1,0))\n</pre> schemas.children(\"core.file\", (0,1,0)) Out[7]: <pre>{PGSchema.PluginRef(group='schema', name='core.imagefile', version=(0, 1, 0)),\n PGSchema.PluginRef(group='schema', name='dummy.imagefile.niche', version=(0, 1, 0))}</pre> <p>We get back a set of all schemas that, directly or indirectly, are based on <code>core.file</code> and can be used in every place where <code>core.file</code> is expected - so one can say that this set is the set of installed schemas which are \"compatible\" with <code>core.file</code> as-is, without needing to do anything to the metadata.</p> Parent Compatibility:     Every metadata object that is valid according to a schema must also be valid according to its parent schema.  <p>This is the most important requirement for writing schemas, as without this, building a hierarchy of schemas would have no value. A schema is like a \"micro-standard\", it can be designed in better or worse ways, but as long as people agree to use it, it has value. Extending a schema in a compatible way is like building on an existing standard, so it is a virtuous thing to do, if you try to make your metadata FAIR.</p> <p>In our example, your responsibility is to ensure this for the declared parent plugin, <code>core.imagefile</code>. The authors of <code>core.imagefile</code> are responsible for making sure that each valid <code>core.imagefile</code> is a valid <code>core.file</code>, so you do not have to worry or think about that. If everyone is doing their part, the chain will work and your new schema will also be a valid <code>core.file</code> by transitivity, that is, \"for free\" from your perspective.</p> In\u00a0[8]: Copied! <pre>from metador_core.schema import MetadataSchema\nfrom metador_core.schema.types import Int, Float, Str\nfrom typing import Union, Optional\n\n@register_in_group(schemas)\nclass Parent(MetadataSchema):\n    \n    class Plugin:\n        name = \"dummy.parent\"\n        version = (0, 1, 0)\n        \n    foo: Union[Int, Str]\n    bar: Optional[Float]\n    qux: Str\n</pre> from metador_core.schema import MetadataSchema from metador_core.schema.types import Int, Float, Str from typing import Union, Optional  @register_in_group(schemas) class Parent(MetadataSchema):          class Plugin:         name = \"dummy.parent\"         version = (0, 1, 0)              foo: Union[Int, Str]     bar: Optional[Float]     qux: Str <pre>Notebook: Plugin 'dummy.parent' registered in 'schema' group!\n</pre> <p>Now we will register a child class that satisfies the rules:</p> In\u00a0[9]: Copied! <pre>from metador_core.schema.decorators import make_mandatory\n\n@register_in_group(schemas)\n@make_mandatory(\"bar\")\nclass Child1(Parent):\n    \n    class Plugin:\n        name = \"dummy.child1\"\n        version = (0, 1, 0)\n        \n    foo: Int\n    new_field: Bool\n</pre> from metador_core.schema.decorators import make_mandatory  @register_in_group(schemas) @make_mandatory(\"bar\") class Child1(Parent):          class Plugin:         name = \"dummy.child1\"         version = (0, 1, 0)              foo: Int     new_field: Bool <pre>Notebook: Plugin 'dummy.child1' registered in 'schema' group!\n</pre> <p>You can see that we used the <code>make_mandatory</code> decorator - what it does is taking an inherited field from the parent, and make sure that is not <code>Optional</code> anymore, i.e. is turned into a required field. There are two advantages this decorator provides for this possibly simplest case of a field restriction:</p> <ul> <li>you do not have to \"duplicate\" the inherited type just to get rid of the <code>Optional</code></li> <li>it clearly communicates in what way the field is changed compared to the parent schema</li> </ul> Use the make_mandatory decorator to make an optional inherited field mandatory.     It will ensure that the field actually exists in the parent and will define the correct non-optional type for you automatically.  <p>Now let us see what happens if we violate the parent consistency rules:</p> In\u00a0[10]: Copied! <pre>try:  # we know it will go wrong\n\n    @register_in_group(schemas)\n    class Child2(Parent):\n\n        class Plugin:\n            name = \"dummy.child2\"\n            version = (0, 1, 0)\n\n        foo: Float\n            \nexcept TypeError as e:\n    print(e)  # show the message\n</pre> try:  # we know it will go wrong      @register_in_group(schemas)     class Child2(Parent):          class Plugin:             name = \"dummy.child2\"             version = (0, 1, 0)          foo: Float              except TypeError as e:     print(e)  # show the message <pre>The type assigned to field 'foo'\nin schema &lt;class '__main__.Child2'&gt; (dummy.child2 0.1.0):\n\n  &lt;class 'pydantic.types.StrictFloat'&gt;\n\ndoes not look like a valid subtype of the inherited type:\n\n  typing.Union[pydantic.types.StrictInt, pydantic.types.StrictStr]\n\nfrom schema __main__.Parent (plugin: dummy.parent 0.1.0).\n\nIf you are ABSOLUTELY sure that this is a false alarm,\nuse the @overrides decorator to silence this error\nand live forever with the burden of responsibility.\n\n</pre> <p>In <code>Child1</code>, we re-declared <code>foo</code> to accept <code>int</code> values, which is fine - the parent is declared as <code>Union[int, str]</code>, which means that it can accept either an <code>int</code> or a <code>str</code>. Now in <code>Child2</code> we tried to declare a field <code>foo</code> in our schema as a float, but the parent schema does not allow floats. The system can infer that something is wrong and will refuse to register this faulty schema.</p> <p>Always remember the fields the parent schema inherited itself - the following attempt will also fail:</p> In\u00a0[11]: Copied! <pre>try:  # we know it will go wrong\n\n    @register_in_group(schemas)\n    class Child3(Child1):\n\n        class Plugin:\n            name = \"dummy.child3\"\n            version = (0, 1, 0)\n\n        qux: Float\n            \nexcept TypeError as e:\n    print(e)  # show the message\n</pre> try:  # we know it will go wrong      @register_in_group(schemas)     class Child3(Child1):          class Plugin:             name = \"dummy.child3\"             version = (0, 1, 0)          qux: Float              except TypeError as e:     print(e)  # show the message <pre>The type assigned to field 'qux'\nin schema &lt;class '__main__.Child3'&gt; (dummy.child3 0.1.0):\n\n  &lt;class 'pydantic.types.StrictFloat'&gt;\n\ndoes not look like a valid subtype of the inherited type:\n\n  &lt;class 'pydantic.types.StrictStr'&gt;\n\nfrom schema __main__.Parent (plugin: dummy.parent 0.1.0).\n\nIf you are ABSOLUTELY sure that this is a false alarm,\nuse the @overrides decorator to silence this error\nand live forever with the burden of responsibility.\n\n</pre> <p><code>Child3</code> inherits from <code>Child1</code>, which does not define <code>qux</code>, but it does inherit <code>qux</code> from <code>Parent</code> unchanged. Remember that you can use <code>Fields</code> to inspect all the fields declared in a schema. This will also tell you where a field is actually \"coming from\":</p> In\u00a0[12]: Copied! <pre>print(Child1.Fields)\n</pre> print(Child1.Fields) <pre>foo\n\ttype: &lt;class 'pydantic.types.StrictInt'&gt;\n\torigin: __main__.Child1 (plugin: dummy.child1 0.1.0)\n\t\nnew_field\n\ttype: &lt;class 'pydantic.types.StrictBool'&gt;\n\torigin: __main__.Child1 (plugin: dummy.child1 0.1.0)\n\tdescription:\n\t\t\n\t\t        StrictBool to allow for bools which are not type-coerced.\n\t\t        \n\t\nbar\n\ttype: &lt;class 'pydantic.types.StrictFloat'&gt;\n\torigin: __main__.Child1 (plugin: dummy.child1 0.1.0)\n\t\nqux\n\ttype: &lt;class 'pydantic.types.StrictStr'&gt;\n\torigin: __main__.Parent (plugin: dummy.parent 0.1.0)\n\t\n</pre> In\u00a0[13]: Copied! <pre>class SomeSchema(MetadataSchema):\n    some_field: Float\n    nested_obj: Child1\n        \nprint(SomeSchema(some_field=1.23, nested_obj=Child1(foo=1, bar=3.14, qux=\"hi\", new_field=True)).yaml())\n</pre> class SomeSchema(MetadataSchema):     some_field: Float     nested_obj: Child1          print(SomeSchema(some_field=1.23, nested_obj=Child1(foo=1, bar=3.14, qux=\"hi\", new_field=True)).yaml()) <pre>nested_obj:\n  bar: 3.14\n  foo: 1\n  new_field: true\n  qux: hi\nsome_field: 1.23\n\n</pre> In\u00a0[14]: Copied! <pre>from typing import Literal\nfrom phantom.interval import Inclusive\nfrom metador_core.schema.ld import LDSchema, ld_decorator\n\nclass FluffinessScore(float, Inclusive, low=0, high=10):\n\"\"\"Fluffiness score for the animal, carefully estimated by petting it.\n    The value is in the closed interval [0, 10], where the score is\n    estimated on a linear scale assuming that\n    * 0 means \"it has no hair\"\n    * 10 means \"absolute fluffball\"\n    \"\"\"\n\nmy_semantics = ld_decorator(context=\"https://www.example.com/animal-ontology\")\n\n@my_semantics(type=\"Animal\")\nclass AnimalMeta(LDSchema):\n\"\"\"Metadata for representing animals in the jungle pet discovery project.\"\"\"\n    \n    voice: Literal[\"dog-like\", \"cat-like\", \"bird-like\", \"other\"]\n\"\"\"Voice category of the animal.\n    We classify animals into:\n    * dog-like (if they bark) \n    * cat-like (if they meow)\n    * bird-like (if they chirp)\n    * other\n    \"\"\"\n    \n    fluffiness: FluffinessScore\n    # if no docstring is given and the typehint is a class,\n    # then its docstring will be used.\n\n\n# let's describe an animal!\nprint(AnimalMeta(id_=\"https://petid.org/874\", voice=\"dog-like\", fluffiness=5).yaml())\n</pre> from typing import Literal from phantom.interval import Inclusive from metador_core.schema.ld import LDSchema, ld_decorator  class FluffinessScore(float, Inclusive, low=0, high=10):     \"\"\"Fluffiness score for the animal, carefully estimated by petting it.          The value is in the closed interval [0, 10], where the score is     estimated on a linear scale assuming that     * 0 means \"it has no hair\"     * 10 means \"absolute fluffball\"     \"\"\"  my_semantics = ld_decorator(context=\"https://www.example.com/animal-ontology\")  @my_semantics(type=\"Animal\") class AnimalMeta(LDSchema):     \"\"\"Metadata for representing animals in the jungle pet discovery project.\"\"\"          voice: Literal[\"dog-like\", \"cat-like\", \"bird-like\", \"other\"]     \"\"\"Voice category of the animal.          We classify animals into:     * dog-like (if they bark)      * cat-like (if they meow)     * bird-like (if they chirp)     * other     \"\"\"          fluffiness: FluffinessScore     # if no docstring is given and the typehint is a class,     # then its docstring will be used.   # let's describe an animal! print(AnimalMeta(id_=\"https://petid.org/874\", voice=\"dog-like\", fluffiness=5).yaml()) <pre>'@context': https://www.example.com/animal-ontology\n'@id': https://petid.org/874\n'@type': Animal\nfluffiness: 5\nvoice: dog-like\n\n</pre> In\u00a0[15]: Copied! <pre>from metador_core.schema.types import MimeTypeStr\n\nclass NicheMimetype(MimeTypeStr, pattern=r\"image/niche\"):\n\"\"\"The MIME type of .niche image files.\"\"\"\n\n# we extend the semantic context of ROCrate (ImageFile is based on ROCrate), as described here: \n# https://www.researchobject.org/ro-crate/1.1/appendix/jsonld.html#extending-ro-crate\nmy_context=[\"https://w3id.org/ro/crate/1.1/context\", {\n  \"animalMeta\": \"https://www.example.com/animal-ontology/Animal\"\n}]\n\next_rocrate = ld_decorator(context=my_context)\n\n@register_in_group(schemas)\n@ext_rocrate(type=\"File\")\nclass NicheImage(ImageFile):\n\"\"\"Schema for the .niche image format.\n    The format achieves improved compression for images of animals,\n    given some information about the depicted animal.\n    \"\"\"\n    \n    class Plugin:\n        name = \"dummy.imagefile.niche\"\n        version = (0, 1, 0)\n    \n    # we constrain the allowed MIME type field from core.imagefile\n    encodingFormat: NicheMimetype\n\"\"\"The MIME type of .niche image files, must be 'image/niche'.\"\"\"\n    \n    # we add a new field with the new relevant information for our format,\n    # which we conveniently already have defined earlier\n    animalMeta: AnimalMeta\n</pre> from metador_core.schema.types import MimeTypeStr  class NicheMimetype(MimeTypeStr, pattern=r\"image/niche\"):     \"\"\"The MIME type of .niche image files.\"\"\"  # we extend the semantic context of ROCrate (ImageFile is based on ROCrate), as described here:  # https://www.researchobject.org/ro-crate/1.1/appendix/jsonld.html#extending-ro-crate my_context=[\"https://w3id.org/ro/crate/1.1/context\", {   \"animalMeta\": \"https://www.example.com/animal-ontology/Animal\" }]  ext_rocrate = ld_decorator(context=my_context)  @register_in_group(schemas) @ext_rocrate(type=\"File\") class NicheImage(ImageFile):     \"\"\"Schema for the .niche image format.          The format achieves improved compression for images of animals,     given some information about the depicted animal.     \"\"\"          class Plugin:         name = \"dummy.imagefile.niche\"         version = (0, 1, 0)          # we constrain the allowed MIME type field from core.imagefile     encodingFormat: NicheMimetype     \"\"\"The MIME type of .niche image files, must be 'image/niche'.\"\"\"          # we add a new field with the new relevant information for our format,     # which we conveniently already have defined earlier     animalMeta: AnimalMeta <pre>Notebook: Plugin 'dummy.imagefile.niche' registered in 'schema' group!\n</pre> <p>Now let's take our new schema for a ride and create a metadata object with information about a image file encoded in our <code>.niche</code> format:</p> In\u00a0[16]: Copied! <pre>img_meta = NicheImage(\n    # some dummy values (that a harvester would usually get for you):\n    filename=\"someimage.niche\",\n    sha256=\"abc\",\n    contentSize=123,\n    height=100, width=200,\n    # now our custom added fields:\n    encodingFormat=\"image/niche\",\n    animalMeta=AnimalMeta(voice=\"cat-like\", fluffiness=3)\n)\n\nyaml_meta = img_meta.yaml()\nprint(f\"Metadata as seen through {NicheImage.Plugin.name}:\")\nprint(yaml_meta)\n\nprint(f\"Metadata as seen through {ImageFile.Plugin.name}:\")\nprint(ImageFile.parse_raw(yaml_meta).yaml())\n</pre> img_meta = NicheImage(     # some dummy values (that a harvester would usually get for you):     filename=\"someimage.niche\",     sha256=\"abc\",     contentSize=123,     height=100, width=200,     # now our custom added fields:     encodingFormat=\"image/niche\",     animalMeta=AnimalMeta(voice=\"cat-like\", fluffiness=3) )  yaml_meta = img_meta.yaml() print(f\"Metadata as seen through {NicheImage.Plugin.name}:\") print(yaml_meta)  print(f\"Metadata as seen through {ImageFile.Plugin.name}:\") print(ImageFile.parse_raw(yaml_meta).yaml()) <pre>Metadata as seen through dummy.imagefile.niche:\n'@context':\n- https://w3id.org/ro/crate/1.1/context\n- animalMeta: https://www.example.com/animal-ontology/Animal\n'@type': File\nanimalMeta:\n  '@context': https://www.example.com/animal-ontology\n  '@type': Animal\n  fluffiness: 3\n  voice: cat-like\ncontentSize: 123\nencodingFormat: image/niche\nfilename: someimage.niche\nheight:\n  '@context': https://schema.org\n  '@type': QuantitativeValue\n  unitText: px\n  value: 100\nsha256: abc\nwidth:\n  '@context': https://schema.org\n  '@type': QuantitativeValue\n  unitText: px\n  value: 200\n\nMetadata as seen through core.imagefile:\n'@context': https://w3id.org/ro/crate/1.1/context\n'@type': File\nanimalMeta:\n  '@context': https://www.example.com/animal-ontology\n  '@type': Animal\n  fluffiness: 3\n  voice: cat-like\ncontentSize: 123\nencodingFormat: image/niche\nfilename: someimage.niche\nheight:\n  '@context': https://schema.org\n  '@type': QuantitativeValue\n  unitText: px\n  value: 100\nsha256: abc\nwidth:\n  '@context': https://schema.org\n  '@type': QuantitativeValue\n  unitText: px\n  value: 200\n\n</pre> <p>You can see how metadata objects created with our new schema are reusable and interoperable, both within the Metador tool ecosystem and beyond:</p> <ul> <li>As we used schema inheritance, people can still see and access all the fields that are not specific to your schema without having it available</li> <li>As we added JSON-LD annotations, a semantic system can understand the metadata without having any idea about Metador and its concepts</li> </ul>"},{"location":"notebooks/03_Schemas/#writing-schemas-types-types-everywhere","title":"Writing Schemas: Types! Types everywhere!\u00b6","text":"<p>In this tutorial you will learn everything you need to know for modelling and representing your metadata and defining your own high-quality Metador schemas.</p> <p>Prerequisites:</p> <ul> <li>basic understanding of Metador schemas and plugins (covered in previous tutorials)</li> <li>Optional, but helpful: some experience with dataclasses and/or pydantic</li> </ul> <p>Learning Goals:</p> <ul> <li>Learn how to create a new schema or one based on an existing schema</li> <li>Learn how to model your metadata using expressive type hints</li> <li>Understand best practices for schema design and some common pitfalls to avoid</li> </ul>"},{"location":"notebooks/03_Schemas/#defining-a-schema","title":"Defining a schema\u00b6","text":"Every Metador schema is either a direct subclass of MetadataSchema, or subclass of an existing schema.  <p>So the following is a perfectly valid, very simple schema:</p>"},{"location":"notebooks/03_Schemas/#extending-an-existing-schema-the-absolute-minimum","title":"Extending an existing schema: The absolute minimum\u00b6","text":"<p>In the first tutorial, we mentioned that one core feature of schemas in Metador is that they can be easily extended and encouraged you to do so. We discussed metadata for a custom image format as an example. In the following, you will learn everything you need to do this in practice and we will arrive at a reasonable schema by the end of this tutorial.</p> <p>We want to extend the <code>core.imagefile</code> schema to have some format-specific extra fields, so we take that schema as the base class for our schema. Furthermore, at the time we start writing our schema the <code>core.imagefile</code> schema is available in a certain version. To make sure that our schema works as expected in the future, we must also state which version of <code>core.imagefile</code> we intend our schema to be based on.</p>      When developing a plugin that depends on other plugins,     you must state the versions of the plugins you expect to get when accessing them.  <p>Metador will try to detect if you forget to do this, but it might not be able to for all ways the plugins can be interacting. Therefore keep this in mind when developing plugins - otherwise, when the plugins you use are updated with changes that your plugin is not prepared for, things might break.</p> <p>In the important special case when you try extending a plugin class that you retrieved without a stating a version, Metador will actively stop you:</p>"},{"location":"notebooks/03_Schemas/#the-family-life-of-schemas","title":"The family life of schemas\u00b6","text":"<p>Schemas in Metador only support single inheritance, which means that you can state only one parent schema plugin it claims to extend. This means that each Metador schema has a neat linear inheritance chain. We can inspect the inheritance chain of registered parent schemas like this:</p>"},{"location":"notebooks/03_Schemas/#all-schemas-are-equal-but-some-schemas-are-more-equal-to-plugin-or-not","title":"All schemas are equal, but some schemas are more equal: To Plugin or not\u00b6","text":"<p>So you might already have noticed that not every schema you define must be a plugin - you can define, use and even inherit from schemas that are not plugins just fine, being registered as a plugin which a nice <code>name</code> and well-maintained <code>version</code> is just an extra property, one which important schemas usually have. Every schema you register as a plugin and not declare <code>auxiliary</code> can be attached as-is to a node in a Metador container, e.g. if you want to turn a schema for 3D positions into a plugin, it is something that you should mark as <code>auxiliary</code>.</p> <p>So what kind of schemas should be a plugin?</p> <ul> <li>Every schema that you want others to put into containers must be plugin</li> <li>Every schema that you want other people to actually use and possibly extend must be a plugin</li> <li>Every schema that is too general in scope or does not make sense on its own can be a plugin, but you must mark it as <code>auxiliary</code></li> <li>Apart from these guidelines, it is up to you</li> </ul> <p>Remember that nested schemas can be accessed through the <code>Fields</code> interface, so there is no need to declare all small nested schemas your schema is built from - except you think that they could be useful outside the context of your schema.</p>"},{"location":"notebooks/03_Schemas/#on-the-shoulders-of-giants-extending-a-schema-correctly","title":"On the shoulders of giants: Extending a schema correctly\u00b6","text":"<p>You have already seen in the last tutorial that schemas are defined using Python type hints, the same way dataclasses are defined. We will now look at how you can extend your schema, and how you can make sure that the parent compatibility is not violated.</p>      The following rules apply to the relationship between a parent schema and a child schema. Note that additional rules must be considered when you update a schema - this will be discussed later in the context of schema versioning.  <p>When you define a new schema based on an existing parent schema, there are only two rules you have to follow:</p> Parent Compatibility (1): Adding a new field that is not defined in the parent schema is safe.  <p>When loading a metadata object based on your schema, the parent schema will simply ignore your new fields, so adding a new field will cause no problems up the \"ancestry chain\".</p> <p>The case needing some more care and consideration is the following:</p> Parent Compatibility (2): You may only change a field which exists in the parent schema to a more restricted type.  <p>This means that you must not replace an existing field in a way that your schema would accept a value the parent schema would not accept. Notice that we say \"exists\" and not \"is defined\" - the parent itself can have inherited fields on its own which you also have to keep in mind.</p> Implication: Removing a field existing in the parent is impossible.  <p>If you could remove a field, you could also re-define it to something else entirely in another child schema, which would violate rule Parent Compatibility (2), so this cannot be possible.</p> <p>If a parent field is mandatory, unfortunately you are out of luck and will have to provide a value. If it is optional, then you and can simply ignore the field.</p> <p>If you cannot accept these restrictions and design your schema respecting them, then the chosen parent class is not suitable for your purpose - you will either need to discuss changes you would like to see with the author of the parent schema (if they would be generally helpful), look for another better suited parent schema, or simply not inherit from any existing schema.</p> <p>Now let us look at a toy example. Consider the following parent class:</p>"},{"location":"notebooks/03_Schemas/#python-typology","title":"Python Typology\u00b6","text":"<p>After seeing how schema and field inheritance work and learning how to declare a child schema correctly without violating parent compatibility, now you might wonder what kind of types you can use in your schemas in order to express as precisely as possible what values are supposed to go into which fields. The possibilities, in fact, are limitless and there are many ways to model the same requirement. To get you started and give you an idea, we will give a quick overview of the most common and most useful types, give some guidance on how and when to use them, and equally important - what to avoid.</p>"},{"location":"notebooks/03_Schemas/#primitives-times-dates-and-pydantic-built-in-types","title":"Primitives, times, dates and pydantic built-in types\u00b6","text":"<p>The <code>pydantic</code> library - the beating heart of Metador schemas - also provides many useful type hints which you can use for various kinds of information, including entities such as IP addresses, URLs, colors and more (for some things we advise against using the types pydantic provides, which again is discussed further below).</p> <p>Furthermore, in <code>metador_core.schema.types</code> we provide a number of generally useful types for different purposes which are meant to be imported directly and used in your schemas. Take a look at them and prefer reusing or extenting them before you start defining your own types.</p> <p>For strings, you should use <code>NonEmptyStr</code> from <code>metador_core.schema.types</code>. It will make sure that only strings that are non-empty and have non-whitespace characters will be accepted - this is what you usually want when requesting a string.</p> <p>For values corresponding to the built-in Python types <code>bool</code>, <code>int</code>, <code>float</code>, you should use the types <code>Bool</code>, <code>Int</code>, and <code>Float</code> in <code>metador_core.schema.types</code> (these are essentially aliases for the Strict types provided by pydantic). However, for numeric fields you should rarely need them and in many situations will probably use the more precise constrained types which are discussed further below.</p> <p>You can also use <code>date</code>, <code>datetime</code> and <code>time</code> from the <code>datetime</code> package to represent the corresponding quantities.</p>"},{"location":"notebooks/03_Schemas/#metador-schemas","title":"Metador schemas\u00b6","text":"<p>You can always use any other existing Metador schemas when defining a field.</p>      When defining a nested complex schema, you should Metador schemas for each nested entity type.  <p>Breaking down complex and nested hierarchical structures into schemas for the different parts should be your preferred and default approach when modelling metadata. For example, we can use one of the schemas we defined above inside another schema and it will work as expected:</p>"},{"location":"notebooks/03_Schemas/#optional-values","title":"Optional values\u00b6","text":"<p>If you want to declare a field that is relevant, but might not always be available, use <code>typing.Optional</code> in order to make omitting the value possible (if omitted, the value will automatically be <code>None</code> for this field).</p>  When you define a new field in a schema, try to keep it mandatory as long as possible.  <p>Only relax a field to <code>Optional</code> when you are sure that there is no feasible way to provide the desired information consistently. This way you will break less potential child schemas that are based on your schema.</p> <p>If a child schema will rely on a field being optional, but you suddenly make it mandatory, the child schema will also have to make it mandatory (otherwise it violates the parent rule discussed above by allowing a \"missing value\", which is <code>None</code>, that is not allowed in the parent, that is your schema). Also, obtaining missing information for a schema after-the-fact is harder or even impossible (e.g. you cannot re-do the scientific experiment), so it is better to err on the side of \"strictness\" up-front.</p> <p>Examples: <code>Optional[Int]</code>, <code>Optional[SIValue]</code></p>"},{"location":"notebooks/03_Schemas/#collections","title":"Collections\u00b6","text":"<p>To describe a collection of values or objects of the same kind, you can use <code>typing.List</code> or <code>typing.Set</code>.</p> Prefer Set over List, unless you need to keep duplicate values or the order of the items matters.  <p>Many things where you might first instinctively use a <code>List</code> are actually, semantically <code>Set</code>s, so make sure that you choose one or the other consciously. This has actual practical consequences for the behavior of harvesters (that we will talk about in a different tutorial).</p> <p>Examples: <code>List[Float]</code>, <code>Set[AnyHttpUrl]</code></p>"},{"location":"notebooks/03_Schemas/#literals-and-enums","title":"Literals and Enums\u00b6","text":"<p>Sometimes you want a field to have a fixed value, or a value from a controlled list. The tools that you can use are <code>Literal</code>s (from <code>typing</code>) and <code>Enum</code>s (from <code>enum</code>). The rule of thumb is that you probably should use a simple <code>Literal</code> when there is just one or a handful of values that are permitted, but for a longer list you should define an Enum class.</p> <p>Examples: <code>Literal[\"a\", \"b\", \"c\"]</code>, <code>Literal[0, 42]</code>, <code>Literal[\"always_this_value\"]</code>, <code>SimAlgorithm</code></p> <p>with <code>SimAlgorithm</code> e.g. defined as:</p> <pre>class SimAlgorithm(str, Enum):\n    simple = \"simple-simulation\"\n    fancy = \"fancy-simulation\"\n    # ... other supported algorithms\n</pre>"},{"location":"notebooks/03_Schemas/#constrained-types","title":"Constrained types\u00b6","text":"<p>Constrained types are variants of the primitive and collection types that we discussed above with a restricted value range. They can be used to represent requirements such as:</p> <ul> <li>the number must be positive</li> <li>the value must be in $[-\\pi, \\pi)$</li> <li>the string must have a length between 100 and 1000 characters</li> <li>the list can have at most 7 elements</li> </ul> <p>For constrained types, in Metador schemas we prefer using types based on the phantom library. The advantage of using <code>phantom</code> types is that these work well with schema inheritance and the automatic checks that make sure that your types are compatible with the parent schema, whereas constrained types provided by <code>pydantic</code> do not have these nice properties.</p>      Prefer using constrained types from the phantom library."},{"location":"notebooks/03_Schemas/#what-to-avoid","title":"What to avoid\u00b6","text":""},{"location":"notebooks/03_Schemas/#alternative-choices-with-union","title":"Alternative choices with <code>Union</code>\u00b6","text":"<p>If you must accept values of multiple different types, you can use <code>typing.Union</code>.</p>      Do not use Union if you can avoid it and make sure you understand how it works in schemas!  <p>The first reason to avoid them is that each <code>Union</code> forces any tool that wants to use objects following your schema to do a case analysis (\"if the field is this type, do this, or if that type, do that\"). While this kind of \"controlled ambiguity\" can be useful, desirable or at least non-problematic in certain situations, in the context of metadata modelling it should be avoided, because it complicates the (re-)use of the metadata (including any transformations and mechanical inspection).</p> <p>The second reason to avoid <code>Union</code> is that there exist certain unintuitive subtleties that can lead to unexpected/unintended behavior. For example, due to how parsing of the metadata by <code>pydantic</code> works, when loading a metadata object, a value will always be of the first type (in the order as listed in the <code>Union</code>) for which the conversion succeeds.</p>      In a Union, always list more specific types before more general types.  <p>Completely avoiding <code>Union</code>s could be difficult, e.g. if your schema is implementing an existing standard where such alternatives (\"either a number or a string\") are already allowed. So please just be especially careful with <code>Union</code>s and when testing your schema, make sure to pay special attention to how fields with <code>Union</code> types behave. At least make sure that you understand how the parsing logic works (see the tutorial on custom parsers) before using <code>Union</code>s extensively.</p>"},{"location":"notebooks/03_Schemas/#unconstrained-numbers-and-strings","title":"Unconstrained numbers and strings\u00b6","text":"<p>Avoid \"plain\" types such as <code>Int</code>, <code>Float</code> in your schemas, except in drafting stages of your schema. We used them in the examples to keep it simple, but in actual use cases, there usually are restrictions that should apply - values you actually should exclude because they do not make sense in your context. Always try to formulate a constrained type and only fall back to these primitive types when there is no other solution.</p>      Only use unconstrained types if there are really no additional value constraints that should apply.  <p>This also applies to strings. For example, above we said that you should use <code>NonEmptyStr</code> instead of <code>Str</code>. The reason is that even an empty string <code>\"\"</code> is still a string, and so is <code>\"   \"</code> - it is not considered a missing value, which could lead to unintended consequences down the line. This is an example for a value constraint that seems \"common sense\", but is easy to overlook when defining a schema and can lead to subtle errors down the line.</p> <p>Making sure that missing values are represented in an unambiguous way is crucial to make harvesters work correctly. There already exists a unique, unambiguous value representing missing information in Python, which is <code>None</code>, and the way to state that information is missing is wrapping the type of value in <code>Optional</code> to allow <code>None</code>. This kind of discipline might feel unfamiliar to you, but you will see that it can prevent many avoidable mistakes and removes ambiguity - all values that are not <code>None</code> are considered to be meaningful values.</p> Constrain your types to accept only meaningful values!     Optionality is enabled explicitly by Optional, and the unique \"missing value\" value is None!  <p>Tuple:</p> <p>There is a type hint <code>Tuple</code> in <code>typing</code> that can be used for tuples, but there are not many cases where a tuple would be the best solution. Instead of a tuple where each component is semantically different or has a different type, such as <code>Tuple[int, str, bool]</code>, you should write a schema instead and give those components a name. In cases where you expect a sequence of elements of the same type, you usually want a <code>List</code> (possibly with constraints), except when you really have a fixed number of items.</p> <p>One defendable case for a <code>Tuple</code> would be a vector in a space with fixed dimensions, such as a 2D or 3D vector. For such use cases, assuming that you provide documentation about the meaning (e.g. using <code>Tuple[float, float]</code> and documenting that it is supposed to be a <code>(x,y)</code> position), this would be a reasonable definition. Even then, defining a helper schema with <code>x: float</code> and <code>y: float</code> could be the better choice, because the meaning is made explicit in the field name and thus can be better interpreted without relying on additional documentation.</p>      Only use Tuple after carefully considering other options and convincing yourself that a simple tuple is the best option.  <p>Dict:</p> <p>There is a type hint <code>Dict</code> in <code>typing</code>, but good use cases for it are rare and of technical nature.</p> Do not use dicts for modelling, unless you have really no information about the possible keys and the types of permitted values for those keys.  <p>If you have something which looks like a <code>dict</code>, you should define another schema encoding all the information about the structure, and use that instead. Remember that you can freely define schemas which are not registered as plugins and use them as pieces for building up your larger, possibly nested plugin schema.</p> <p>dataclasses and other dataclass-like things that are not Metador schemas:</p> <p>There are many libraries that superficially look like they work the same way or very similarly to Metador schemas, however they will not be compatible - or worse, they can look as if they work, but will break in specific situations. Specifically,</p>      Do not use plain dataclasses and do not use default pydantic BaseModel-based model classes  <p>When you consult pydantic documentation, keep in mind that instead of <code>BaseModel</code>, in Metador the top level class that is used for our models is <code>MetadataSchema</code>.</p> <p>Pydantic constrained types:</p>      Only use the built-in constrained types provided by pydantic, if you cannot see any reasonable use-case where someone would want to tighten the constraint on the field in a derivative schema.  <p>This applies to pydantic types such as <code>PositiveInt</code>, <code>NegativeFloat</code>, and types constructed with the <code>conint</code>, <code>constr</code>, etc. helper functions.</p>      When you do decide to use pydantic type constraints, you must use Annotated,     as described here."},{"location":"notebooks/03_Schemas/#documenting-schemas","title":"Documenting Schemas\u00b6","text":"<p>Use python docstrings, for the schema class as well as for the fields, and document the meaning and purpose. This is the information others will consult when trying to use your schema and helping them to decide whether your schema is useful for their purpose. You should include information that you technically encode into constraint types, if it is not absolutely obvious - you don't have to spell out that a <code>NonEmptyStr</code> is a nonempty string, but if <code>MySpecialParameter</code> is actually a constrained type representing number range, it should also be explained in the documentation of the field that uses your custom constrained type. Furthermore, documentation should include all non-technical, human-level information about the intended context for the schema and helps to to use and interpret it correctly.</p>  Document your schema class and its fields properly - this is what users will see when accessing the Fields interface!  <p>A well-documented schema might actually contain more documentation text than actual \"code\", e.g. like this:</p>"},{"location":"notebooks/03_Schemas/#at-last-a-wonderful-niche-schema","title":"At last: A wonderful niche schema\u00b6","text":"<p>New let's use what we learned to properly define our custom image file format, as promised in the beginning:</p>"},{"location":"notebooks/03_Schemas/#managing-change-responsibly-versioning-of-schemas","title":"Managing Change Responsibly: Versioning of Schemas\u00b6","text":"<p>When you write the first version of a schema, you have a lot of freedom in how you want to design it. But once others start using it, you have the responsibility to be careful with the changes you do, avoid changes that can or will break child schemas that could have been created already. Make sure that the severity of changes is reflected in the version of your schema plugin - which has to follow strict semantic versioning.</p> <p>If your schema plugin $X$ had version <code>(MAJOR, MINOR, PATCH)</code> and you did changes to it directly or indirectly resulting in an updated schema $X'$, you have to update the version of your schema as well.</p> <p>A non-exhaustive list of relevant changes includes:</p> <ul> <li>Adding, changing or removing fields of $X$</li> <li>Adding, changing or removing schema decorators that affect fields of $X$ (such as the LD annotations or <code>@make_mandatory</code>)</li> <li>Doing any of the above to a schema that $X$ depends on (e.g. nested schemas, parent schemas)</li> <li>Updating the required version of any plugin that $X$ depends on (e.g. ones you reuse from others)</li> </ul> <p>It is important to understand to effect your changes have on the ability to process metadata that was already created with the previous version. Some changes do not require any action, but others do. Changes that \"break\" things should be as rare as possible, but of course are sometimes unavoidable. Breaking changes are always annoying, but not necessarily a horrible experience - if managed well, they just require some extra work which usually is straight-forward. Good management of breaking changes in the reason why versioning discipline is important. This includes updating the semantic versioning triple (so that machines can see whether a schema and some metadata are compatible), and communicating the changes by other means (inform users and provide ways to upgrade their existing metadata to the new version of the schema).</p> <p>Some changes are backward-compatible, meaning that your schema can be put in place of the older version and nothing will break - every metadata object the previous version of your schema created is still valid for the new version.</p> <p>In rare cases, changes might even forward-compatible, meaning that older versions of your schema will work with metadata objects created by your newer version. For example, if you improve the schema definition without affecting what kind of objects it can process in any way.</p> <p>Let $v(S)$ denote the set of metadata objects that are valid according to schema $S$. Then the rules for version bumping are as follows (bumping the version here means incrementing the corresponding component and resetting the less important components back do $0$). You have to bump:</p> <ul> <li><p><code>PATCH</code> if $v(X') = v(X)$</p> </li> <li><p><code>MINOR</code> if $v(X') \\supset v(X)$</p> </li> <li><p><code>MAJOR</code> if $v(X') \\subset v(X)$</p> </li> </ul> Design your schemas to be initially as \"strict\" as possible!     You can always loosen up your requirements in future versions later without breaking existing metadata, but not the other way round.  <p>This is quite abstract, so here are a few concrete examples:</p> <ul> <li>Any new field added in $X'$ or constrained more than before makes $v(X') \\subset v(X)$</li> </ul> <p>This is also true if the new field is optional - because if the field is provided, it is validated and must have the correct type, the schema cannot just \"ignore\" values if they are wrong. Think of the case that someone extended your schema and added a field with the same name to it which has an incompatible type. Your new field then breaks \"parent compatibility\" for that schema - a breaking change.</p> <ul> <li>Adding a custom parser that can process more inputs without changing the declared type makes $v(X') \\supset v(X)$</li> </ul> <p>We will not discuss metadata normalization and custom parsers in this tutorial (there is a separate tutorial on this topic), but this is one way how the set of accepted values can actually be expanded in a backward-compatible manner.</p> <ul> <li>Fixing a bug that made $X$ reject something it was actually supposed to accept in the first place can be considered $v(X') = v(X)$</li> </ul> <p>Of course, strictly speaking, the set of accepted objects changes between the two versions - but here it is about the semantic intent. This of course only applies to mistakes that affect only specific edge cases (for example, your integer constraint was off by one, or you were using an open instead of the intended closed interval). Decide wisely what is actually a \"bug fix\", and what is a \"schema change\".</p> <ul> <li>Updating the version of a schema plugin your schema depends on requires you to bump on the same level.</li> </ul> <p>This means, if your schema has version <code>(0, 1, 0)</code> and was based on a parent schema (or a nested schema) with version <code>(1, 2, 0)</code>, but you update it to its newest version <code>(2, 0, 0)</code>, then your own schema version must change to <code>(1, 0, 0)</code>.</p> You have much more freedom and fewer things to consider when defining the first version of your schema - because it did not exist and nobody was using it. Try to get your schema \"as right as possible\" before you publish it. Once published, try to minimize releases with \"breaking changes\" and provide metadata migration strategies to your users."},{"location":"notebooks/03_Schemas/#testing-schemas","title":"Testing Schemas\u00b6","text":"<p>Writing a schema is only one half of the job, though. In order to make sure that everything works correctly, a schema must be properly tested. Does the schema accept only the values in fields that are supposed to be accepted, and reject values that do not make sense? Does the parent compatibility actually work, if we try it for concrete instances? All these questions and our expectations about how the schema behaves must be codified into a proper set of tests. Especially as a schema can be developed further over time, a test suite will help to detect actual mistakes as well as accidental \"breaking changes\" you did not consider.</p> TODO: Write when schema testing framework is in place"},{"location":"notebooks/03_Schemas/#summary","title":"Summary\u00b6","text":"<p>This was a long-winded tutorial, if you are here - congratulations! The good news is that now you have a deep understanding of schemas - the most important entity in the Metador system. You will see that most other plugin types are actually simpler! You really earned a break, and some summary notes:</p>"},{"location":"notebooks/03_Schemas/#plugins","title":"Plugins\u00b6","text":"<ul> <li>Not every schema you define must be a registered plugin, nested schemas can be accessed through the <code>Fields</code> interface</li> <li>Useful schemas that should not be attachable to container nodes can be plugins, but must be marked as <code>auxiliary</code></li> <li>Auxiliary schemas (e.g. small nested schemas) that are only useful in the scope of your schema should not be plugins</li> </ul>"},{"location":"notebooks/03_Schemas/#inheritance","title":"Inheritance\u00b6","text":"<ul> <li>Every schema is either direct subclass of <code>MetadataSchema</code> or specializes/extends an existing parent schema</li> <li>Parent compatibility: When extending a schema, you can only add new fields or restrict the values allowed for existing fields</li> </ul>"},{"location":"notebooks/03_Schemas/#type-hints","title":"Type Hints\u00b6","text":"<ul> <li>Use <code>Optional</code> when a field can be missing, but for many reasons you should prefer mandatory fields</li> <li>Use <code>Literal</code> types and <code>Enum</code> classes for discrete, fixed controlled lists of allowed values</li> <li>Use classes from the <code>datetime</code> package for times and dates</li> <li>Use classes from the <code>phantom</code> package for constrained types, such as number ranges</li> <li>Use default <code>pydantic</code> types for things like URLs</li> <li>Avoid <code>pydantic</code> constrained types, unless in no child schema would want or need to narrow down the constraints</li> <li>Avoid unconstrained default Python types as hints, unless you really have no special requirements</li> <li>Avoid <code>Tuple</code>, unless the meaning of the components is rather trivial and suitable for a tuple</li> <li>Avoid <code>Dict</code>, unless you have no idea what it can contain and are sure it is the only way</li> <li>Avoid <code>Union</code>, unless you really need it and understand how parsing in schemas / pydantic works</li> </ul>"},{"location":"notebooks/03_Schemas/#versioning","title":"Versioning\u00b6","text":"<ul> <li>Semantic versioning is to be followed for a responsible management of breaking changes</li> <li>For a schema, any change that will reject objects that would have been accepted before is a breaking change</li> <li>To minimize breakage, start with strict schemas (tight value bounds, mandatory fields) and relax requirements in revisions, if necessary</li> <li>Do not forget implicit changes, such as updating the version of a schema that your schema depends on</li> </ul>"},{"location":"notebooks/03_Schemas/#documentation-and-testing","title":"Documentation and Testing\u00b6","text":"<ul> <li>Make sure to provide documentation in the schema and field docstrings to help the users of your schema</li> <li>Write tests for your schemas</li> </ul> TODO: summarize testing when section done"},{"location":"notebooks/03_Schemas/#appendix-1-flowchart-defining-or-extending-a-schema","title":"Appendix 1: Flowchart - Defining or Extending a Schema\u00b6","text":"<p>The following flowchart could be useful to guide you in your schema and/or field design process. You can find it here to look at it in full size.</p> <p></p>"},{"location":"notebooks/03_Schemas/#appendix-2-a-little-glossary","title":"Appendix 2: A little glossary\u00b6","text":"<p>You hopefully were able to follow the explanations with an intuitive understanding of certain terms we use. Going forward, it might be helpful to understand how multiple related but different terms are connected and explicitly point out the different perspectives and contexts they come from.</p> <p>Schemas, models, classes / fields, attributes, keys:</p> <ul> <li>A Metador schema is a pydantic model that is extended with many additional features specific to Metador</li> <li>A pydantic model is similar (but technically unrelated) to python dataclasses, but with powerful validation capabilities</li> <li>All of these are just Python classes whose purpose it is mainly to carry around (meta)data in a structured way</li> <li>The attributes of an object are called fields in pydantic and Metador schemas and correspond to keys  in unstructured <code>dicts</code></li> </ul> <p>Classes, types, sets / instances, values:</p> <ul> <li>A Python class is a complex data type, just like the regular data types you know, essentially it is just a fancy <code>dict</code></li> <li>A class instance therefore is just a value of that type, so being a subclass is the same as being a subtype</li> <li>Each type has a finite or infinite number of possible values, which we will naturally call value set</li> <li>A type is a proper subtype if and only if it its value set is a subset of the value set of the other type</li> <li>A subtype is more constrained or narrow than the original type</li> <li>A value is-a (instance of) a type if and only if it is contained in the value set of that type</li> </ul> <p>Inheritance and subtypes:</p> <ul> <li>A subschema (or child schema) is just a subclass of an existing parent schema, which is its base class</li> </ul> <p>So the following terms are intimately connected:</p> <ul> <li>inheritance, thinking in OOP terms (representing an is-a relationship)</li> <li>subtyping, thinking in terms of types and values</li> <li>subsets, thinking in terms of value sets</li> </ul> <p>Composition of schemas and types:</p> <ul> <li>A nested schema is a schema which is used in a field definition of a larger schema</li> <li>A schema is a composition of its fields</li> </ul> <p>So you compose schemas / classes / types to describe how larger objects are built up from smaller ones.</p> <p>So the following terms are also just different ways of looking at the same thing:</p> <ul> <li>composition of classes (has-a relationship in OOP terms)</li> <li>the product of types</li> <li>the cartesian product of the value sets</li> </ul> <p>Don't worry about the last two interpretations, if they are not familiar to you. These were just listed to complete the picture, but we will stick to calling this relationship either composition or nesting.</p> <p>Here a few concrete examples:</p> <ul> <li>The number <code>5</code> relates to <code>int</code> like a metadata object (class instance) relates to its schema (class) (is-a)</li> <li>The type <code>int</code> relates to <code>Union[int, str]</code> like a child schema relates to its parent schema (subtype-of)</li> <li>The <code>int</code> and <code>str</code> relate to <code>Tuple[int, str]</code> like a nested schema relates to the whole schema (composition)</li> <li>The integers relate to <code>int</code> like all valid metadata objects relate to their schema (value set)</li> </ul> <p>All of this might be a bit confusing at first, but once the connections between these perspectives \"click\", you will be rewarded by a better understanding of data in general, which will also guide you to better schemas.</p>"},{"location":"notebooks/03b_SemanticsInteroperability/","title":"Semantics and Interoperability","text":"In\u00a0[1]: Copied! <pre># this is just a Python dict encoding the JSON-LD context:\nmy_context = {\n    \"name\": \"http://schema.org/name\",\n    \"image\": {\n      \"@id\": \"http://schema.org/image\",\n      \"@type\": \"@id\"\n    },\n    \"homepage\": {\n      \"@id\": \"http://schema.org/url\",\n      \"@type\": \"@id\"\n    }\n}\n</pre> # this is just a Python dict encoding the JSON-LD context: my_context = {     \"name\": \"http://schema.org/name\",     \"image\": {       \"@id\": \"http://schema.org/image\",       \"@type\": \"@id\"     },     \"homepage\": {       \"@id\": \"http://schema.org/url\",       \"@type\": \"@id\"     } } Metador itself is not a semantic system and cannot perform validation of JSON-LD annotations! You have to make sure that your context is valid JSON-LD and makes sense using other external tools, if necessary.  Step 3: Use field names in your schema which are interpretable in your defined context.  <p>Semantics Beginner: The context is like a dictionary for looking up semantic interpretations, so your schema only makes semantic sense if the names you use have a definition. If you are using an existing standard, consult its documentation for the correct property names for various object types.</p> <p>Semantics Expert: If you are using a custom context, you probably have a good understanding of this. One limitation you have to keep in mind is that you cannot use \"qualified\" names using a prefix as is often done, because you cannot easily have a colon in a field name in Python. Your context therefore must fully define all the concrete field names you use. This means that you cannot call a schema field <code>foaf:name</code>, but you are free to use any valid JSON-LD, including these kind of abbreviations, within your context definition - as long as in the end all the actual field names used in the schema are declared without any namespace prefix.</p>      In a JSON-LD @context used for a Metador schema, all needed types and terms must be defined without a qualifying namespace prefix.       The @context should explicitly define all the used field names which are not implicitly covered by the @type assigned to the schema (if any). Otherwise these fields will have no semantic interpretation and will remain opaque to semantics-based tools.  In\u00a0[2]: Copied! <pre>from pydantic.color import Color\n\nfrom metador_core.schema.ld import LDSchema, ld\n\nmy_context = \"https://www.example.com/my/context\"  # &lt;- could also be some more complex JSON-LD object\n\n@ld(context=my_context, type=\"Animal\")\nclass MySemanticSchema(LDSchema):\n    furColor: Color\n        \n# create an instance:\nmyAnimal = MySemanticSchema(id_=\"https://www.animalid.org/01234\", furColor=\"#ff8000\")\n\n# serialize the instance:\nanimalJson = myAnimal.json(indent=2)\nprint(animalJson)\n\n# deserialize it back:\nsameAnimal = MySemanticSchema.parse_raw(animalJson)\nprint(\"Loaded back same animal?\", myAnimal == sameAnimal)\n</pre> from pydantic.color import Color  from metador_core.schema.ld import LDSchema, ld  my_context = \"https://www.example.com/my/context\"  # &lt;- could also be some more complex JSON-LD object  @ld(context=my_context, type=\"Animal\") class MySemanticSchema(LDSchema):     furColor: Color          # create an instance: myAnimal = MySemanticSchema(id_=\"https://www.animalid.org/01234\", furColor=\"#ff8000\")  # serialize the instance: animalJson = myAnimal.json(indent=2) print(animalJson)  # deserialize it back: sameAnimal = MySemanticSchema.parse_raw(animalJson) print(\"Loaded back same animal?\", myAnimal == sameAnimal) <pre>{\n  \"@id\": \"https://www.animalid.org/01234\",\n  \"furColor\": \"#ff8000\",\n  \"@context\": \"https://www.example.com/my/context\",\n  \"@type\": \"Animal\"\n}\nLoaded back same animal? True\n</pre> <p>Notice that we did not specify <code>@context</code> and <code>@type</code> for the metadata object <code>myAnimal</code> - the schema knows them already and just \"tacks them on\" to each animal metadata object, and whenever it is serialized/stored (in a Metador container, JSON file, etc.), it will have the correct annotation. When loading a serialized animal with these annotations, the schema will also not complain as long as these are exactly the ones we attached to the schema (remember, Metador does not actually understand semantics!).</p> <p>You see that we used <code>id_</code> even though we did not declare it. The <code>id_</code> field is automatically available to all semantic schemas derived from <code>LDSchema</code>, in order to set the JSON-LD <code>@id</code> of a semantic object.</p>  The JSON-LD @id field is provided by the field id_ which is defined in LDSchema and inherited to all semantic schemas.  <p>This is a property specific to each instance of a schema, a concrete metadata object, whereas the <code>@type</code> and <code>@context</code> are identical for all the instances. In Python/OOP jargon - <code>@context</code> and <code>@type</code> are class variables (with the special property of being constant values) and are attached using schema decorators, whereas <code>@id</code> is an actual instance variable specific to individual objects - just as all the fields you usually define in your schema. Naturally, in Metador schemas we call fields that behave like <code>@context</code> and <code>@type</code> simply constant fields.</p> In\u00a0[3]: Copied! <pre>from metador_core.schema.ld import ld_decorator\n\nmy_semantics = ld_decorator(context=my_context)\n</pre> from metador_core.schema.ld import ld_decorator  my_semantics = ld_decorator(context=my_context) <p>The custom decorator works just like the <code>ld</code> decorator, but has two advantages:</p> <ul> <li>you will not need to state the <code>context</code> for every single schema anymore</li> <li>if your context needs to change, you only need to change it once for your decorator</li> </ul> <p>Therefore, it is advisable to define a custom decorator whenever you use the a context for multiple schemas. A usage example:</p> In\u00a0[4]: Copied! <pre>from metador_core.schema.types import NonEmptyStr\n\n# define our semantic schema:\n@my_semantics(type=\"Animal\")\nclass MySemanticSchema(LDSchema):\n    furColor: Color\n    \nprint(\"Same result as above with custom decorator:\")\nprint(MySemanticSchema(id_=\"https://www.animalid.org/01234\", furColor=\"#ff8000\").json(indent=2))\n\n@my_semantics\nclass AnotherSemanticSchema(LDSchema):\n    something: NonEmptyStr\n        \nprint(\"Instance of a schema with the @context, but no @type:\")\nprint(AnotherSemanticSchema(something=\"hello\").json(indent=2))\n</pre> from metador_core.schema.types import NonEmptyStr  # define our semantic schema: @my_semantics(type=\"Animal\") class MySemanticSchema(LDSchema):     furColor: Color      print(\"Same result as above with custom decorator:\") print(MySemanticSchema(id_=\"https://www.animalid.org/01234\", furColor=\"#ff8000\").json(indent=2))  @my_semantics class AnotherSemanticSchema(LDSchema):     something: NonEmptyStr          print(\"Instance of a schema with the @context, but no @type:\") print(AnotherSemanticSchema(something=\"hello\").json(indent=2)) <pre>Same result as above with custom decorator:\n{\n  \"@id\": \"https://www.animalid.org/01234\",\n  \"furColor\": \"#ff8000\",\n  \"@context\": \"https://www.example.com/my/context\",\n  \"@type\": \"Animal\"\n}\nInstance of a schema with the @context, but no @type:\n{\n  \"something\": \"hello\",\n  \"@context\": \"https://www.example.com/my/context\"\n}\n</pre> <p>(Semantics Beginner) Q: I still don't get it, how exactly is the schema \"better\" now by adding these fields?</p> <p>A: Never forget that machines are really, really stupid. Using structured ways to organize the metadata (using Metador schemas, JSON, etc.) instead of using free-form natural language helps a technical system to understand structure, the \"shape\" of your metadata - which is an important step forward. Technical systems can to a lot with data and metadata without any understanding, because the required understanding is provided by humans - the software developers who understand the domain and metadata and write software which uses it. The advantage of adding such semantic \"hints\" might not be obvious, if you are able to understand the field names and read the corresponding documentation. But imagine a schema designed in a language you don't know - could you make sense of a schema like this?</p> In\u00a0[5]: Copied! <pre>from metador_core.schema import MetadataSchema\nfrom metador_core.schema.types import Int\n\nclass RuffleMeta(MetadataSchema):\n\"\"\"A Ruffle is just a simple combination of the Quirzl phase and the Shpongle factor\n    measured during the Xylic-Yzgel process at a fixed time step.\"\"\"\n    \n    quirzl: NonEmptyStr\n\"\"\"Quirzl phase of the Ruffle.\"\"\"\n    \n    shpongle: Int\n\"\"\"Shpongle factor of the Ruffle.\"\"\"\n</pre> from metador_core.schema import MetadataSchema from metador_core.schema.types import Int  class RuffleMeta(MetadataSchema):     \"\"\"A Ruffle is just a simple combination of the Quirzl phase and the Shpongle factor     measured during the Xylic-Yzgel process at a fixed time step.\"\"\"          quirzl: NonEmptyStr     \"\"\"Quirzl phase of the Ruffle.\"\"\"          shpongle: Int     \"\"\"Shpongle factor of the Ruffle.\"\"\" <p>Maybe this is something you are familiar with, but with weird names, or maybe this is a field of science you have never seen before - you have no chance to know either way. Semantic methods such as JSON-LD annotations solve this problem by connecting your schemas and their fields to a formalized system for knowledge representation - objects that refer to the same entity in an ontology are supposed to mean the same kind of thing, regardless of how the field, schema or object is named. This helps a human who does not understand your language or domain, and also helps a machine which is trying to process your data without knowing all of its context as well as you do.</p> <p>(Semantics Expert) Q: Why do I have to re-create all the schemas in Metador by hand? That's double-work!</p> <p>A: There are multiple reasons why unfortunately you cannot simply import your ontology into Metador.</p> <p>On a technical level, the RDF-based semantic web / linked data technologies work by a different logic than most other ways of organizing data. For example, OWL was designed with logical reasoning in mind, assuming that the available information is already existing and making sense. It does not care about validation and not allow to easily check for the \"shape\" of the information or have any assumptions about it. This shortcoming was addressed by the semantics community by creating other RDF-based languages, such as SHACL.</p> <p>But while the validation problem might be seen as solved within the RDF-based world, the concepts do not trivially translate into structures in concrete programming languages. The best that a suitable tool, even if it would exist, could do would be trying to approximately convert from SHACL or OWL into some other way of defining entities - usually such automatic conversions are not readily usable and can at best be a starting point for manual tweaking and inspection. Even for non-semantic standards such as JSON Schema this is a non-trivial task and tooling could not automatically handle certain conceptual gaps between the logic of JSON Schema and other technologies with overlapping purpose, such as pydantic (which Metador uses). For example, neither JSON Schema nor RDF-based languages natively support OOP-like schema inheritance that way Metador allows and relies upon, and neither of them addresses the issue of parsing and normalization.</p> <p>The non-technical answer is that there are many scientists that with some experience with Python, but there are very few semantics experts in research institutions who could formalize all requirements purely relying on RDF, OWL and SHACL. These languages are also not the area of expertise of typical software developers and research software engineers and many transformations that can easily be done based on JSON can be challenging using e.g. SPARQL queries. But both researchers and software engineers can quickly learn to use <code>@context</code> and <code>@type</code> correctly and can understand a high-level documentation of a well-designed and well-documented ontology enough to make use of semantics in their schemas.</p> <p>So Metador supports and encourages providing semantics to increase interoperability between systems, but it is not and does not try to be a fully semantics-based system. Instead, it tries to make the gap between the typical JSON-based web technologies and the world of semantics as narrow as possible, by encouraging pragmatic use of JSON-LD.</p> <p>The same machinery can be used to attach arbitrary schema-specific (i.e. equal for all objects of that schema) constant fields that behave exactly like the JSON-LD annotations, i.e.:</p> <ul> <li>constant fields are not required when creating or loading a metadata object</li> <li>if those fields already exist in the object that is loaded into a schema, they are discarded</li> <li>when serializing an object, the object will have the constant fields as defined by its schema</li> </ul> <p>Constant fields are useful for enriching metadata objects with additional information that is fully determined by their schema and which they always should \"carry along with them\", e.g. to provide additional required information when the metadata objects are used outside of Metador ecosystem.</p> <p>If you need this functionality, take a look at the more general <code>add_const_fields</code> decorator:</p> In\u00a0[6]: Copied! <pre>from metador_core.schema.decorators import add_const_fields\nhelp(add_const_fields)\n</pre> from metador_core.schema.decorators import add_const_fields help(add_const_fields) <pre>Help on function add_const_fields in module metador_core.schema.decorators:\n\nadd_const_fields(consts: Dict[str, Any], *, override: bool = False)\n    Add constant fields to pydantic models.\n    \n    Must be passed a dict of field names and the constant values (only JSON-like types).\n    \n    Constant fields are optional during input.\n    If present during parsing, they are be ignored and overriden with the constant.\n    Constant fields are included in serialization, unless `exclude_defaults` is set.\n    \n    This can be used e.g. to attach JSON-LD annotations to schemas.\n    \n    Constant fields are inherited and may only be overridden by other constant fields\n    using this decorator, they cannot become normal fields again.\n\n</pre>"},{"location":"notebooks/03b_SemanticsInteroperability/#semantic-and-structural-interoperability-for-metador-schemas","title":"Semantic and Structural Interoperability for Metador Schemas\u00b6","text":"<p>Prerequisites:</p> <ul> <li>basic understanding of Metador schemas and plugins (covered in previous tutorials)</li> <li>basic understanding of the concepts and tools used for the semantic web (RDF, OWL, JSON-LD, etc.)</li> </ul> <p>Learning Goals:</p> <ul> <li>Understand the relationship and interplay between schemas in Metador and semantic standards</li> <li>Learn how to make a schema semantic using JSON-LD annotations</li> </ul> TODO: Extend with section on interop with JSON Schema for structural interop. with other systems"},{"location":"notebooks/03b_SemanticsInteroperability/#introduction","title":"Introduction\u00b6","text":"<p>Each Metador schema represents a structural encoding for objects that belong to some abstract or real-world category we have in our mind - we know what it means, because we can read the documentation and look at the code if needed. To make this information visible to a machine, service, tool that uses your metadata, but does not know anything about Metador, this \"human level metadata\" must be provided to give the metadata meaning - semantics. This does not only help machines, but also other people interested in your data and metadata.</p> <p>In the context of Metador, we say that a schema is semantic if it is aligned and compatible with existing linked-data / semantic web standards. We assume that you have at least a rough idea about the vision, purpose and existing tooling, and you now want to know how to connect your schemas to existing vocabularies and ontologies, so that your metadata is interoperable with these tools, can be unambiguously interpreted, added into a knowledge graph, queried using SPARQL and profit from all the other nice features that the semantic web ecosystem provides. In the following, we will describe all the necessary steps to do this.</p>"},{"location":"notebooks/03b_SemanticsInteroperability/#context-is-important-preparation-steps-for-semantic-schemas","title":"Context is Important: Preparation steps for semantic schemas\u00b6","text":"<p>Until now, all the schemas we defined were structured, provided validation of the metadata, but were lacking a formal semantic interpretation of the fields, unless they inherited from a schema that already was providing some semantics (thus at least covering the inherited fields). If your schema represents a type for which a semantic standard already exists, you can make your schema fully semantic rather easily.</p> <p>Semantics is provided for schemas by attaching JSON-LD annotations to them. The consequence of doing this is that every serialized object (as JSON or YAML, etc) will contain a <code>@context</code> and a <code>@type</code> field. These additional fields are \"tacked on\" automatically to each metadata object, and consequently, if you feed non-semantic metadata (either by hand, or using harvesters) into Metador schemas, you will obtain semantic metadata on the \"output\", meaning that tools and humans working with Metador containers will be able to make sense of your metadata with much less effort, given that the schemas you use are semantically \"enriched\" with the JSON-LD fields. For this to work, you have to do some preparations first.</p> Step 1: Understand the relevant object and field types in the standard(s) you use.  <p>A semantic standard such as an OWL ontology will define multiple kinds of entities and they can have various interrelationships and properties. Each schema you define should ideally be a representation of one such entity, which is exactly what will be declared as the <code>@type</code> for your schema. If your schema does not match a defined entity, this is also fine - you still can and should use a suitable ontology, but it just could require some more work on the <code>@context</code>. In any case, the first step is to have a conceptual understanding how schemas and fields are supposed to map onto the ontologies that you use.</p> Step 2: Find or define the context (i.e., the value assigned to @context) as valid JSON-LD.  <p>Semantics Beginner: You will usually find a default context you can use in the documentation of your semantic metadata standard. It will typically be a URL (which points to the context object), such as <code>https://w3id.org/ro/crate/1.1/context</code> or can be even as simple as just <code>https://schema.org</code>.</p> <p>Semantics Expert: If you are combining multiple standards or have another use-case for a custom context (e.g. building a schema that does not correspond to a defined <code>@type</code>), you can use an arbitrary JSON-like object as a context. For example, your context can be a Python <code>dict</code> that defines the interpretations for your fields, e.g.:</p>"},{"location":"notebooks/03b_SemanticsInteroperability/#adding-json-ld-annotations-to-schemas","title":"Adding JSON-LD annotations to schemas\u00b6","text":"<p>Now assuming that you understood and defined your semantic context, let us see how it can be used in schemas.</p> <p>Everything you need for development of semantic schemas lives in the <code>metador_core.schema.ld</code> module. If you intend to define a semantic schema and do not extend another already semantic schema, use <code>LDSchema</code> as the base class (instead of <code>MetadataSchema</code>) to distinguish it from non-semantic schemas.</p> <p>For a one-off way to attach some JSON-LD fields to a schema, you can use the default <code>ld</code> decorator:</p>"},{"location":"notebooks/03b_SemanticsInteroperability/#custom-ld-decorators","title":"Custom LD Decorators\u00b6","text":"<p>In most cases, you probably will have a context that you want to use for a whole collection of schemas, and ideally, each of them will represent a different <code>@type</code>. In this case, first you should define a decorator to be able to quickly attach your <code>@context</code> (and possibly the <code>@type</code>) to a schema with less redundancy:</p>"},{"location":"notebooks/03b_SemanticsInteroperability/#advanced-constant-fields-in-general","title":"Advanced: Constant fields in general\u00b6","text":""},{"location":"notebooks/03b_SemanticsInteroperability/#summary","title":"Summary\u00b6","text":""},{"location":"notebooks/03b_SemanticsInteroperability/#semantics","title":"Semantics\u00b6","text":"<ul> <li>Semantically aligned schemas must be based on <code>LDSchema</code> (or use a semantic parent schema)</li> <li>The JSON-LD <code>@id</code> field is accessed within Python as <code>id_</code> (but <code>@id</code> is expected in the input and used for output)</li> <li>Before aligning your schemas with semantic standards, understand your <code>@context</code> and <code>@type</code>s of entities that your schemas represent</li> <li>Use <code>ld_schema_decorator</code> to create a <code>@context</code>/<code>@type</code> decorator for your schemas that share the same context</li> <li>Use that decorator to attach these JSON-LD fields to a schema representing an entity type in your ontology</li> <li>Metador cannot check the correctness of your JSON-LD annotations</li> </ul>"},{"location":"notebooks/03b_SemanticsInteroperability/#constant-fields","title":"Constant Fields\u00b6","text":"<ul> <li>The general mechanism used for JSON-LD annotation of schemas is called constant fields</li> <li>Constant fields are fixed and equal for all objects created with the same schema</li> <li>Constant fields are not required and ignored if present when loading or creating a schema instance</li> <li>Constant fields of the schema are always attached when serializing a metadata object</li> </ul>"},{"location":"notebooks/04_Harvesters/","title":"Harvesters","text":""},{"location":"notebooks/04_Harvesters/#harvesters","title":"Harvesters\u00b6","text":""},{"location":"notebooks/05_Widgets/","title":"Dashboards and Widgets","text":"In\u00a0[1]: Copied! <pre>from pathlib import Path\n\nfiles = Path(\"files\")  # location of test files\n</pre> from pathlib import Path  files = Path(\"files\")  # location of test files In\u00a0[2]: Copied! <pre>from metador_core.plugins import schemas\n\nBibMeta = schemas[\"core.bib\"]\nDBMeta = schemas[\"core.dashboard\"]\nImgMeta = schemas[\"core.imagefile\"]\n</pre> from metador_core.plugins import schemas  BibMeta = schemas[\"core.bib\"] DBMeta = schemas[\"core.dashboard\"] ImgMeta = schemas[\"core.imagefile\"] In\u00a0[3]: Copied! <pre>from metador_core.plugins import harvesters\nfrom metador_core.harvester import harvest, metadata_loader, file_harvester_pipeline\n\nHrvFile = harvesters[\"core.file.generic\"]\nHrvImgDim = harvesters[\"core.imagefile.dim\"]\n\nImgMetaLoader = metadata_loader(ImgMeta, use_sidecar=True)\nimage_pipeline = file_harvester_pipeline(HrvFile, HrvImgDim, ImgMetaLoader)\n</pre> from metador_core.plugins import harvesters from metador_core.harvester import harvest, metadata_loader, file_harvester_pipeline  HrvFile = harvesters[\"core.file.generic\"] HrvImgDim = harvesters[\"core.imagefile.dim\"]  ImgMetaLoader = metadata_loader(ImgMeta, use_sidecar=True) image_pipeline = file_harvester_pipeline(HrvFile, HrvImgDim, ImgMetaLoader) In\u00a0[4]: Copied! <pre>from metador_core.container import MetadorContainer\nfrom metador_core.packer.utils import pack_file\n\nfilepath = Path(\"my-dashboardable-container.h5\")\nif filepath.is_file():\n    filepath.unlink()  # otherwise might fail to overwrite if previous attempt not closed\n\nwith MetadorContainer(filepath, \"w\") as m:\n    m.meta[\"core.bib\"] = BibMeta.parse_file(files / \"test.bibmeta.yaml\")\n    \n    imgmeta = harvest(ImgMeta, image_pipeline(files / \"test.png\"))\n    n = pack_file(m, files / \"test.png\", target=\"foo/bar\", metadata=imgmeta)\n    n.meta[DBMeta] = DBMeta.show(group=1)\n\n    n = pack_file(m, files / \"test.csv\", target=\"csvfile\")\n    n.meta[DBMeta] = DBMeta.show(group=1)\n\n    n = pack_file(m, files / \"test.tsv\")\n    n.meta[DBMeta] = DBMeta()\n    \n    n = pack_file(m, files / \"test.pdf\")\n    n.meta[DBMeta] = DBMeta.show(group=2, priority=10)\n    \n    n = pack_file(m, files / \"test.json\")\n    n.meta[DBMeta] = DBMeta.show(group=2, priority=5)\n    \n    n = pack_file(m, files / \"test.md\")\n    n.meta[DBMeta] = DBMeta()\n    \n    n = pack_file(m, files / \"test.html\")\n    n.meta[DBMeta] = DBMeta.show(\n        [DBMeta.widget(widget_name=\"core.file.text.code\", group=2), DBMeta.widget(group=2)]\n    )\n</pre> from metador_core.container import MetadorContainer from metador_core.packer.utils import pack_file  filepath = Path(\"my-dashboardable-container.h5\") if filepath.is_file():     filepath.unlink()  # otherwise might fail to overwrite if previous attempt not closed  with MetadorContainer(filepath, \"w\") as m:     m.meta[\"core.bib\"] = BibMeta.parse_file(files / \"test.bibmeta.yaml\")          imgmeta = harvest(ImgMeta, image_pipeline(files / \"test.png\"))     n = pack_file(m, files / \"test.png\", target=\"foo/bar\", metadata=imgmeta)     n.meta[DBMeta] = DBMeta.show(group=1)      n = pack_file(m, files / \"test.csv\", target=\"csvfile\")     n.meta[DBMeta] = DBMeta.show(group=1)      n = pack_file(m, files / \"test.tsv\")     n.meta[DBMeta] = DBMeta()          n = pack_file(m, files / \"test.pdf\")     n.meta[DBMeta] = DBMeta.show(group=2, priority=10)          n = pack_file(m, files / \"test.json\")     n.meta[DBMeta] = DBMeta.show(group=2, priority=5)          n = pack_file(m, files / \"test.md\")     n.meta[DBMeta] = DBMeta()          n = pack_file(m, files / \"test.html\")     n.meta[DBMeta] = DBMeta.show(         [DBMeta.widget(widget_name=\"core.file.text.code\", group=2), DBMeta.widget(group=2)]     )  In\u00a0[5]: Copied! <pre>from metador_core.widget.jupyter import Previewable\nfrom metador_core.plugins import widgets\n</pre> from metador_core.widget.jupyter import Previewable from metador_core.plugins import widgets In\u00a0[6]: Copied! <pre>m = Previewable(MetadorContainer(\"my-dashboardable-container.h5\"))\n</pre> m = Previewable(MetadorContainer(\"my-dashboardable-container.h5\")) <pre> * Serving Flask app 'metador_core.widget.jupyter.standalone'\n * Debug mode: off\n</pre> In\u00a0[7]: Copied! <pre>widgets[\"core.file.image\"](m[\"foo/bar\"]).show()\n</pre> widgets[\"core.file.image\"](m[\"foo/bar\"]).show() Out[7]: In\u00a0[8]: Copied! <pre>widgets[\"core.file.text.code\"](m[\"/test.html\"]).show()\n</pre> widgets[\"core.file.text.code\"](m[\"/test.html\"]).show() Out[8]: In\u00a0[9]: Copied! <pre>widgets[\"core.file.csv\"](m[\"/csvfile\"]).show()\n</pre> widgets[\"core.file.csv\"](m[\"/csvfile\"]).show() Out[9]: In\u00a0[10]: Copied! <pre>widgets[\"core.file.pdf\"](m[\"/test.pdf\"]).show()\n</pre> widgets[\"core.file.pdf\"](m[\"/test.pdf\"]).show() Out[10]: In\u00a0[11]: Copied! <pre>from metador_core.widget.dashboard import Dashboard\n\nDashboard(m).show()\n</pre> from metador_core.widget.dashboard import Dashboard  Dashboard(m).show() Out[11]: In\u00a0[12]: Copied! <pre>m.close()  # important! or we might get file lock issues\n</pre> m.close()  # important! or we might get file lock issues In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/05_Widgets/#dashboards-and-widgets","title":"Dashboards and Widgets\u00b6","text":""},{"location":"notebooks/about/","title":"About the Notebooks","text":"<p>Note</p> <p>The documentation for Metador is still under construction. The current set of tutorials is not suitable for end-users who necessarily need ready-to-use CLI / GUI. We will try to provide a set of more end-user oriented tutorials in the future.</p> <p>Target Audience:</p> <p>Software developers and technically skilled researchers interested in using Metador for a domain-specific use-case which is not already sufficiently supported by existing Metador tooling and plugins (such as metadata schemas, visualization widgets, and dataset packers).</p> <p>Prerequisites:</p> <ul> <li>basic knowledge of Python (writing scripts, using Jupyter notebooks)</li> <li>basic knowledge of <code>h5py</code> and <code>numpy</code> (creating and inspecting HDF5 files)</li> <li>installed <code>metador-core</code> package in your environment</li> </ul>"},{"location":"notebooks/files/test/","title":"test","text":"<p>some text</p>"},{"location":"notebooks/files/test/#some-more-text","title":"some more text","text":""},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>metador_core<ul> <li>cli<ul> <li>general</li> </ul> </li> <li>container<ul> <li>drivers</li> <li>interface</li> <li>protocols</li> <li>provider</li> <li>utils</li> <li>wrappers</li> </ul> </li> <li>harvester<ul> <li>common</li> </ul> </li> <li>ih5<ul> <li>container</li> <li>manifest</li> <li>overlay</li> <li>record</li> <li>skeleton</li> </ul> </li> <li>packer<ul> <li>example</li> <li>types</li> <li>utils</li> </ul> </li> <li>plugin<ul> <li>entrypoints</li> <li>interface</li> <li>metaclass</li> <li>types</li> <li>util</li> </ul> </li> <li>plugins</li> <li>rdf<ul> <li>lib</li> <li>skos</li> </ul> </li> <li>schema<ul> <li>base</li> <li>common<ul> <li>rocrate</li> <li>schemaorg</li> </ul> </li> <li>core</li> <li>decorators</li> <li>encoder</li> <li>examples<ul> <li>matsci</li> </ul> </li> <li>inspect</li> <li>jsonschema</li> <li>ld</li> <li>parser</li> <li>partial</li> <li>pg</li> <li>plugins</li> <li>types</li> </ul> </li> <li>util<ul> <li>diff</li> <li>enum</li> <li>hashsums</li> <li>models</li> <li>pytest</li> <li>typing</li> </ul> </li> <li>widget<ul> <li>common</li> <li>dashboard</li> <li>jupyter<ul> <li>standalone</li> </ul> </li> <li>server</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/metador_core/","title":"metador_core","text":"<p>metador_core package.</p>"},{"location":"reference/metador_core/plugins/","title":"plugins","text":"<p>Central place for convenient access to all registered plugin groups.</p> <p>For example, to access the <code>schema</code> plugingroup, you can use:</p> <p><code>from metador_core.plugins import schemas</code></p>"},{"location":"reference/metador_core/plugins/#metador_core.plugins.PGPluginGroup","title":"PGPluginGroup","text":"<p>             Bases: <code>ObjectProxy</code></p> <p>PluginGroup plugin group.</p> <p>This wrapper returns instances of other loaded plugin groups.</p> <p>In the esoteric case that you need to access the actual plugingroup class that gives out classes instead of instances (like all other plugingroups), request the \"plugingroup\" plugingroup. But usually you will not want this.</p> Source code in <code>src/metador_core/plugins.py</code> <pre><code>class PGPluginGroup(wrapt.ObjectProxy):\n\"\"\"PluginGroup plugin group.\n\n    This wrapper returns instances of other loaded plugin groups.\n\n    In the esoteric case that you need to access the actual plugingroup class\n    that gives out *classes* instead of instances (like all other plugingroups),\n    request the \"plugingroup\" plugingroup. But usually you will not want this.\n    \"\"\"\n\n    _self_groups: Dict[AnyPluginRef, PluginGroup]\n\n    def __reset__(self):\n        self._self_groups.clear()\n        self.__init__()\n\n    def __init__(self):\n        # initialize the meta-plugingroup\n        from .plugin.interface import _plugin_groups, create_pg\n\n        create_pg(PluginGroup)\n        pgpg_ref = AnyPluginRef(\n            group=PG_GROUP_NAME,\n            name=PluginGroup.Plugin.name,\n            version=PluginGroup.Plugin.version,\n        )\n\n        # wire it up with this wrapper\n        self._self_groups = _plugin_groups\n        self.__wrapped__ = _plugin_groups[pgpg_ref]\n\n    # ----\n    def get(self, key, version=None):\n\"\"\"Get a registered plugin group by name.\"\"\"\n        key_, vers = plugin_args(key, version)\n        if key_ == self.name and (vers is None or vers == self.Plugin.version):\n            return self\n        try:\n            if grp_cls := self.__wrapped__._get_unsafe(key_, vers):\n                # now if the PG was not existing, it is + is stored in _self_groups\n                return cast(S, self._self_groups.get(grp_cls.Plugin.ref()))\n        except KeyError:\n            return None\n\n    def __getitem__(self, key) -&gt; PluginGroup:\n        # call wrapped '__getitem__' with this object to use its 'get'\n        return PluginGroup.__getitem__(self, key)  # type: ignore\n\n    def values(self):\n        # same idea, this uses '__getitem__'\n        return PluginGroup.values(self)\n\n    def items(self):\n        # same idea, this uses '__getitem__'\n        return PluginGroup.items(self)\n\n    def is_plugin(self, obj):\n        return obj in self.values()\n</code></pre>"},{"location":"reference/metador_core/plugins/#metador_core.plugins.PGPluginGroup.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> Source code in <code>src/metador_core/plugins.py</code> <pre><code>def __init__(self):\n    # initialize the meta-plugingroup\n    from .plugin.interface import _plugin_groups, create_pg\n\n    create_pg(PluginGroup)\n    pgpg_ref = AnyPluginRef(\n        group=PG_GROUP_NAME,\n        name=PluginGroup.Plugin.name,\n        version=PluginGroup.Plugin.version,\n    )\n\n    # wire it up with this wrapper\n    self._self_groups = _plugin_groups\n    self.__wrapped__ = _plugin_groups[pgpg_ref]\n</code></pre>"},{"location":"reference/metador_core/plugins/#metador_core.plugins.PGPluginGroup.get","title":"get","text":"<pre><code>get(key, version = None)\n</code></pre> <p>Get a registered plugin group by name.</p> Source code in <code>src/metador_core/plugins.py</code> <pre><code>def get(self, key, version=None):\n\"\"\"Get a registered plugin group by name.\"\"\"\n    key_, vers = plugin_args(key, version)\n    if key_ == self.name and (vers is None or vers == self.Plugin.version):\n        return self\n    try:\n        if grp_cls := self.__wrapped__._get_unsafe(key_, vers):\n            # now if the PG was not existing, it is + is stored in _self_groups\n            return cast(S, self._self_groups.get(grp_cls.Plugin.ref()))\n    except KeyError:\n        return None\n</code></pre>"},{"location":"reference/metador_core/cli/","title":"cli","text":"<p>Metador CLI for system introspection.</p>"},{"location":"reference/metador_core/cli/general/","title":"general","text":""},{"location":"reference/metador_core/cli/general/#metador_core.cli.general.info","title":"info","text":"<pre><code>info()\n</code></pre> <p>Show information about the system and Python environment.</p> Source code in <code>src/metador_core/cli/general.py</code> <pre><code>@app.command(\"info\")\ndef info():\n\"\"\"Show information about the system and Python environment.\"\"\"\n    un = platform.uname()\n    print(f\"[b]System:[/b] {un.system} {un.release} {un.version}\")\n    print(\n        f\"[b]Python:[/b] {platform.python_version()} ({platform .python_implementation()})\"\n    )\n    print(\"[b]Env:[/b]\")\n    # TODO: print versions of all relevant packages / starting with metador-*\n    print(\"metador-core\", __version__)\n</code></pre>"},{"location":"reference/metador_core/cli/general/#metador_core.cli.general.check","title":"check","text":"<pre><code>check()\n</code></pre> <p>Run a self-test to ensure that central metador subsystems work correctly.</p> Source code in <code>src/metador_core/cli/general.py</code> <pre><code>@app.command(\"check\")\ndef check():\n\"\"\"Run a self-test to ensure that central metador subsystems work correctly.\"\"\"\n    from datetime import datetime\n\n    from metador_core.container import MetadorContainer\n    from metador_core.packer.utils import pack_file\n    from metador_core.plugins import schemas\n    from metador_core.widget.dashboard import Dashboard\n    from metador_core.widget.jupyter import Previewable\n\n    today = datetime.today().isoformat()\n\n    print(\"Loading schema plugins...\")\n\n    BibMeta = schemas[\"core.bib\"]\n    Person = schemas[\"core.person\"]\n    DBMeta = schemas[\"core.dashboard\"]\n\n    Material = schemas[\"example.matsci.material\"]\n    Method = schemas[\"example.matsci.method\"]\n    Instrument = schemas[\"example.matsci.instrument\"]\n    Specimen = schemas[\"example.matsci.specimen\"]\n    MSInfo = schemas[\"example.matsci.info\"]\n\n    print(\"Constructing metadata objects...\")\n\n    author = Person(\n        id_=\"https://orcid.org/0000-0002-1825-0097\",\n        givenName=\"Josiah\",\n        familyName=\"Carberry\",\n    )\n    my_bibmeta = BibMeta(\n        name=\"Title for my container\",\n        abstract=\"This is a Metador-compliant container\",\n        author=[author],\n        dateCreated=today,\n    )\n\n    msi = MSInfo(\n        abstract=\"hello\",\n        author=[Person(name=\"Anton Pirogov\")],\n        dateCreated=today,\n        material=[\n            Material(\n                materialName=\"bla\",\n                chemicalComposition=\"bla\",\n                density=1,\n                crystalGrainType=\"single_crystal\",\n            )\n        ],\n        method=[\n            Method(\n                instrument=Instrument(\n                    instrumentName=\"microscope\", instrumentModel=\"micro2000\"\n                ),\n                specimen=Specimen(diameter=2.5, gaugeLength=123),\n            )\n        ],\n    )\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        csvfile = Path(tmpdir) / \"testfile.csv\"\n        with open(csvfile, \"w\") as f:\n            f.write(\n\"\"\"AtomID,Time,PosX,PosY,PosZ\nAtomA,0,0,0,0\nAtomB,0,0,1,0\nAtomC,0,0,0,1\nAtomA,1,1,0,0\nAtomB,1,0,0.5,0\nAtomC,1,0,0,0.25\n\"\"\"\n            )\n\n        print(\"Creating Metador container with test (meta)data...\")\n        container_path = Path(tmpdir) / \"test_container.h5\"\n        with MetadorContainer(container_path, \"w\") as mc:\n            # Attach the bibliographic metadata to the very top\n            mc[\"/\"].meta[\"core.bib\"] = my_bibmeta\n\n            # add a file\n            node = pack_file(mc, csvfile)\n            # add more specific metadata\n            node.meta[MSInfo] = msi\n            # make it visible in the dashboard with high prio\n            node.meta[DBMeta] = DBMeta.show(group=1, priority=10)\n\n        print(\"Opening Metador container...\")\n        with Previewable(MetadorContainer(container_path)) as mc:\n            print(\"Try to access metadata...\")\n            mc[csvfile.name].meta[MSInfo]\n            print(\"Try instantiating dashboard...\")\n            Dashboard(mc).show()\n\n    print(\"[b][green]Self-check successfully completed![/green][/b]\")\n</code></pre>"},{"location":"reference/metador_core/container/","title":"container","text":"<p>Metador interface to manage metadata in HDF5 containers.</p> <p>Works with plain h5py.File and IH5Record subclasses and can be extended to work with any type of archive providing the required functions in a h5py-like interface.</p> <p>When assembling a container, the compliance with the Metador container specification is ensured by using it through the MetadorContainer interface.</p> <p>Technical Metador container specification (not required for users):</p> <p>Metador uses only HDF5 Groups and Datasets. We call both kinds of objects Nodes. Notice that HardLinks, SymLinks, ExternalLinks or region references cannot be used.</p> <p>Users are free to lay out data in the container as they please, with one exception: a user-defined Node MUST NOT have a name starting with \"metador_\". \"metador_\" is a reserved prefix for Group and Dataset names used to manage technical bookkeeping structures that are needed for providing all container features.</p> <p>For each HDF5 Group or Dataset there MAY exist a corresponding Group for Metador-compatible metadata that is prefixed with \"metador_meta_\".</p> <p>For \"/foo/bar\" the metadata is to be found...     ...in a group \"/foo/metador_meta_bar\", if \"/foo/bar\" is a dataset,     ...in a group \"/foo/bar/metador_meta_\" if it is a group. We write meta(\"/foo/bar\") to denote that group.</p> <p>Given schemas with entrypoint names X, Y and Z such that X is the parent schema of Y, and Y is the parent schema of Z and a node \"/foo/bar\" annotated by a JSON object of type Z, that JSON object MUST be stored as a newline-terminated, utf-8 encoded byte sequence at the path meta(\"/foo/bar\")/X/Y/Z/=UUID, where the UUID is unique in the container.</p> <p>For metadata attached to an object we expect the following to hold:</p> <p>Node instance uniqueness: Each schema MAY be instantiated explicitly for each node at most ONCE. Collections thus must be represented on schema-level whenever needed.</p> <p>Parent Validity: Any object of a subschema MUST also be a valid instance of all its parent schemas. The schema developers are responsible to ensure this by correct implementation of subschemas.</p> <p>Parent Consistency: Any objects of a subtype of schema X that stored at the same node SHOULD result in the same object when parsed as X (they agree on the \"common\" information). Thus, any child object can be used to retrieve the same parent view on the data. The container creator is responsible for ensuring this property. In case it is not fulfilled, retrieving data for a more abstract type will yield it from ANY present subtype instance (but always the same one, as long as the container does not change)!</p> <p>If at least one metadata object it stored, a container MUST have a \"/metador_toc\" Group, containing a lookup index of all metadata objects following a registered metadata schema. This index structure MUST be in sync with the node metadata annotations. Keeping this structure in sync is responsibility of the container interface.</p> <p>This means (using the previous example) that for \"/foo/bar\" annotated by Z there also exists a dataset \"/metador_toc/X/Y/Z/=UUID\" containing the full path to the metadata node, i.e. \"meta(/foo/bar)/X/Y/Z/=UUID\". Conversely, there must not be any empty entry-point named Groups, and all listed paths in the TOC must point to an existing node.</p> <p>A valid container MUST contain a dataset /metador_version string of the form \"X.Y\"</p> <p>A correctly implemented library supporting an older minor version MUST be able open a container with increased minor version without problems (by ignoring unknown data), so for a minor update of this specification only new entities may be defined.</p> <p>Known technical limitations:</p> <p>Due to the fact that versioning of plugins such as schemas is coupled to the versioning of the respective Python packages, it is not (directly) possible to use two different versions of the same schema in the same environment (with the exception of mappings, as they may bring their own equivalent schema classes).</p> <p>Minor version updates of packages providing schemas must ensure that the classes providing schemas are backward-compatible (i.e. can parse instances of older minor versions).</p> <p>Major version updates must also provide mappings migrating between the old and new schema versions. In case that the schema did not change, the mapping is simply the identity.</p>"},{"location":"reference/metador_core/container/#metador_core.container.ContainerProxy","title":"ContainerProxy","text":"<p>             Bases: <code>Protocol[T]</code></p> <p>Abstract interface for Metador container providers.</p> <p>This interface acts like a proxy to access containers by some identifier.</p> <p>The identifier type parameter T is in the simplest case the Metador container UUID. In more complex cases, it could be a different unique identifier with a non-trivial relationship to Metador container UUIDs (many-to-many). Therefore, T is implementation-specific.</p> <p>There are many ways to store and organize containers, this interface serves as the implementation target for generic service components such as container-centric Flask blueprints, so they can be easier reused in different backends and services.</p> <p>Note that only containment and retrieval are possible - on purpose. Knowing and iterating over all containers in a system is not always possible.</p> Source code in <code>src/metador_core/container/provider.py</code> <pre><code>class ContainerProxy(Protocol[T]):\n\"\"\"Abstract interface for Metador container providers.\n\n    This interface acts like a proxy to access containers by some identifier.\n\n    The identifier type parameter T is in the simplest case the Metador\n    container UUID. In more complex cases, it could be a different unique\n    identifier with a non-trivial relationship to Metador container UUIDs\n    (many-to-many). Therefore, T is implementation-specific.\n\n    There are many ways to store and organize containers, this interface serves\n    as the implementation target for generic service components such as\n    container-centric Flask blueprints, so they can be easier reused in\n    different backends and services.\n\n    Note that only containment and retrieval are possible - on purpose.\n    Knowing and iterating over all containers in a system is not always possible.\n    \"\"\"\n\n    def __contains__(self, key: T) -&gt; bool:\n\"\"\"Return whether a resource key is known to the proxy.\"\"\"\n        # return self.get(key) is not None\n\n    def get(self, key: T) -&gt; Optional[MetadorContainer]:\n\"\"\"Get a container instance, if resource key is known to the proxy.\n\n        Implement this method in subclasses to support the minimal interface.\n        \"\"\"\n\n    def __getitem__(self, key: T) -&gt; T:\n\"\"\"Get a container instance, if resource key is known to the proxy.\n\n        Default implementation is in terms of `get`.\n        \"\"\"\n        if ret := self.get(key):\n            return ret\n        raise KeyError(key)\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.ContainerProxy.__contains__","title":"__contains__","text":"<pre><code>__contains__(key: T) -&gt; bool\n</code></pre> <p>Return whether a resource key is known to the proxy.</p> Source code in <code>src/metador_core/container/provider.py</code> <pre><code>def __contains__(self, key: T) -&gt; bool:\n\"\"\"Return whether a resource key is known to the proxy.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.ContainerProxy.get","title":"get","text":"<pre><code>get(key: T) -&gt; Optional[MetadorContainer]\n</code></pre> <p>Get a container instance, if resource key is known to the proxy.</p> <p>Implement this method in subclasses to support the minimal interface.</p> Source code in <code>src/metador_core/container/provider.py</code> <pre><code>def get(self, key: T) -&gt; Optional[MetadorContainer]:\n\"\"\"Get a container instance, if resource key is known to the proxy.\n\n    Implement this method in subclasses to support the minimal interface.\n    \"\"\"\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.ContainerProxy.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: T) -&gt; T\n</code></pre> <p>Get a container instance, if resource key is known to the proxy.</p> <p>Default implementation is in terms of <code>get</code>.</p> Source code in <code>src/metador_core/container/provider.py</code> <pre><code>def __getitem__(self, key: T) -&gt; T:\n\"\"\"Get a container instance, if resource key is known to the proxy.\n\n    Default implementation is in terms of `get`.\n    \"\"\"\n    if ret := self.get(key):\n        return ret\n    raise KeyError(key)\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorNode","title":"MetadorNode","text":"<p>             Bases: <code>ObjectProxy</code></p> <p>Wrapper for h5py and IH5 Groups and Datasets providing Metador-specific features.</p> <p>In addition to the Metadata management, also provides helpers to reduce possible mistakes in implementing interfaces by allowing to mark nodes as</p> <ul> <li>read_only (regardless of the writability of the underlying opened container) and</li> <li>local_only (preventing access to (meta)data above this node)</li> </ul> <p>Note that these are \"soft\" restrictions to prevent errors and can be bypassed.</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>class MetadorNode(wrapt.ObjectProxy):\n\"\"\"Wrapper for h5py and IH5 Groups and Datasets providing Metador-specific features.\n\n    In addition to the Metadata management, also provides helpers to reduce possible\n    mistakes in implementing interfaces by allowing to mark nodes as\n\n    * read_only (regardless of the writability of the underlying opened container) and\n    * local_only (preventing access to (meta)data above this node)\n\n    Note that these are \"soft\" restrictions to prevent errors and can be bypassed.\n    \"\"\"\n\n    __wrapped__: H5NodeLike\n\n    @staticmethod\n    def _parse_access_flags(kwargs) -&gt; NodeAclFlags:\n        # NOTE: mutating kwargs, removes keys that are inspected!\n        return {flag: kwargs.pop(flag.name, False) for flag in iter(NodeAcl)}\n\n    def __init__(self, mc: MetadorContainer, node: H5NodeLike, **kwargs):\n        flags = self._parse_access_flags(kwargs)\n        lp = kwargs.pop(\"local_parent\", None)\n        if kwargs:\n            raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n\n        super().__init__(node)\n        self._self_container: MetadorContainer = mc\n\n        self._self_flags: NodeAclFlags = flags\n        self._self_local_parent: Optional[MetadorGroup] = lp\n\n    def _child_node_kwargs(self):\n\"\"\"Return kwargs to be passed to a child node.\n\n        Ensures that {read,skel,local}_only status is passed down correctly.\n        \"\"\"\n        return {\n            \"local_parent\": self if self.acl[NodeAcl.local_only] else None,\n            **{k.name: v for k, v in self.acl.items() if v},\n        }\n\n    def restrict(self, **kwargs) -&gt; MetadorNode:\n\"\"\"Restrict this object to be local_only or read_only.\n\n        Pass local_only=True and/or read_only=True to enable the restriction.\n\n        local_only means that the node may not access the parent or file objects.\n        read_only means that mutable actions cannot be done (even if container is mutable).\n        \"\"\"\n        added_flags = self._parse_access_flags(kwargs)\n        if added_flags[NodeAcl.local_only]:\n            # was set as local explicitly for this node -&gt;\n            self._self_local_parent = None  # remove its ability to go up\n\n        # can only set, but not unset!\n        self._self_flags.update({k: True for k, v in added_flags.items() if v})\n        if kwargs:\n            raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n\n        return self\n\n    @property\n    def acl(self) -&gt; Dict[NodeAcl, bool]:\n\"\"\"Return ACL flags of current node.\"\"\"\n        return dict(self._self_flags)\n\n    def _guard_path(self, path: str):\n        if M.is_internal_path(path):\n            msg = f\"Trying to use a Metador-internal path: '{path}'\"\n            raise ValueError(msg)\n        if self.acl[NodeAcl.local_only] and path[0] == \"/\":\n            msg = f\"Node is marked as local_only, cannot use absolute path '{path}'!\"\n            raise ValueError(msg)\n\n    def _guard_acl(self, flag: NodeAcl, method: str = \"this method\"):\n        if self.acl[flag]:\n            msg = f\"Cannot use {method}, the node is marked as {flag.name}!\"\n            raise UnsupportedOperationError(msg)\n\n    # helpers\n\n    def _wrap_if_node(self, val):\n\"\"\"Wrap value into a metador node wrapper, if it is a suitable group or dataset.\"\"\"\n        if isinstance(val, H5GroupLike):\n            return MetadorGroup(self._self_container, val, **self._child_node_kwargs())\n        elif isinstance(val, H5DatasetLike):\n            return MetadorDataset(\n                self._self_container, val, **self._child_node_kwargs()\n            )\n        else:\n            return val\n\n    def _destroy_meta(self, _unlink: bool = True):\n\"\"\"Destroy all attached metadata at and below this node.\"\"\"\n        self.meta._destroy(_unlink=_unlink)\n\n    # need that to add our new methods\n\n    def __dir__(self):\n        names = set.union(\n            *map(\n                lambda x: set(x.__dict__.keys()),\n                takewhile(lambda x: issubclass(x, MetadorNode), type(self).mro()),\n            )\n        )\n        return list(set(super().__dir__()).union(names))\n\n    # make wrapper transparent\n\n    def __repr__(self):\n        return repr(self.__wrapped__)\n\n    # added features\n\n    @property\n    def meta(self) -&gt; MetadorMeta:\n\"\"\"Access the interface to metadata attached to this node.\"\"\"\n        return MetadorMeta(self)\n\n    @property\n    def metador(self) -&gt; MetadorContainerTOC:\n\"\"\"Access the info about the container this node belongs to.\"\"\"\n        return WithDefaultQueryStartNode(self._self_container.metador, self)\n\n    # wrap existing methods as needed\n\n    @property\n    def name(self) -&gt; str:\n        return self.__wrapped__.name  # just for type checker not to complain\n\n    @property\n    def attrs(self):\n        if self.acl[NodeAcl.read_only] or self.acl[NodeAcl.skel_only]:\n            return WrappedAttributeManager(self.__wrapped__.attrs, self.acl)\n        return self.__wrapped__.attrs\n\n    @property\n    def parent(self) -&gt; MetadorGroup:\n        if self.acl[NodeAcl.local_only]:\n            # allow child nodes of local-only nodes to go up to the marked parent\n            # (or it is None, if this is the local root)\n            if lp := self._self_local_parent:\n                return lp\n            else:\n                # raise exception (illegal non-local access)\n                self._guard_acl(NodeAcl.local_only, \"parent\")\n\n        return MetadorGroup(\n            self._self_container,\n            self.__wrapped__.parent,\n            **self._child_node_kwargs(),\n        )\n\n    @property\n    def file(self) -&gt; MetadorContainer:\n        if self.acl[NodeAcl.local_only]:\n            # raise exception (illegal non-local access)\n            self._guard_acl(NodeAcl.local_only, \"parent\")\n        return self._self_container\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorNode.acl","title":"acl  <code>property</code>","text":"<pre><code>acl: Dict[NodeAcl, bool]\n</code></pre> <p>Return ACL flags of current node.</p>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorNode.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: MetadorMeta\n</code></pre> <p>Access the interface to metadata attached to this node.</p>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorNode.metador","title":"metador  <code>property</code>","text":"<pre><code>metador: MetadorContainerTOC\n</code></pre> <p>Access the info about the container this node belongs to.</p>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorNode.restrict","title":"restrict","text":"<pre><code>restrict(**kwargs) -&gt; MetadorNode\n</code></pre> <p>Restrict this object to be local_only or read_only.</p> <p>Pass local_only=True and/or read_only=True to enable the restriction.</p> <p>local_only means that the node may not access the parent or file objects. read_only means that mutable actions cannot be done (even if container is mutable).</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>def restrict(self, **kwargs) -&gt; MetadorNode:\n\"\"\"Restrict this object to be local_only or read_only.\n\n    Pass local_only=True and/or read_only=True to enable the restriction.\n\n    local_only means that the node may not access the parent or file objects.\n    read_only means that mutable actions cannot be done (even if container is mutable).\n    \"\"\"\n    added_flags = self._parse_access_flags(kwargs)\n    if added_flags[NodeAcl.local_only]:\n        # was set as local explicitly for this node -&gt;\n        self._self_local_parent = None  # remove its ability to go up\n\n    # can only set, but not unset!\n    self._self_flags.update({k: True for k, v in added_flags.items() if v})\n    if kwargs:\n        raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n\n    return self\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorMeta","title":"MetadorMeta","text":"<p>Interface to Metador metadata objects stored at a single HDF5 node.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>class MetadorMeta:\n\"\"\"Interface to Metador metadata objects stored at a single HDF5 node.\"\"\"\n\n    # helpers for __getitem__ and __setitem__\n\n    @staticmethod\n    def _require_schema(\n        schema_name: str, schema_ver: Optional[SemVerTuple]\n    ) -&gt; Type[MetadataSchema]:\n\"\"\"Return compatible installed schema class, if possible.\n\n        Raises KeyError if no suitable schema was found.\n\n        Raises TypeError if an auxiliary schema is requested.\n        \"\"\"\n        schema_class = schemas._get_unsafe(\n            schema_name, schema_ver\n        )  # can raise KeyError\n        if schema_class.Plugin.auxiliary:  # reject auxiliary schemas in container\n            msg = f\"Cannot attach instances of auxiliary schema '{schema_name}' to a node!\"\n            raise TypeError(msg)\n        return schema_class\n\n    @staticmethod\n    def _parse_obj(\n        schema: Type[S], obj: Union[str, bytes, Dict[str, Any], MetadataSchema]\n    ) -&gt; S:\n\"\"\"Return original object if it is an instance of passed schema, or else parse it.\n\n        Raises ValidationError if parsing fails.\n        \"\"\"\n        if isinstance(obj, schema):\n            return obj  # skip validation, already correct model!\n        # try to convert/parse it:\n        if isinstance(obj, (str, bytes)):\n            return schema.parse_raw(obj)\n        if isinstance(obj, MetadataSchema):\n            return schema.parse_obj(obj.dict())\n        else:  # dict\n            return schema.parse_obj(obj)\n\n    # raw getters and setters don't care about the environment,\n    # they work only based on what objects are available and compatible\n    # and do not perform validation etc.\n\n    def _get_raw(\n        self, schema_name: str, version: Optional[SemVerTuple] = None\n    ) -&gt; Optional[StoredMetadata]:\n\"\"\"Return stored metadata for given schema at this node (or None).\n\n        If a version is passed, the stored version must also be compatible.\n        \"\"\"\n        # retrieve stored instance (if suitable)\n        ret: Optional[StoredMetadata] = self._objs.get(schema_name)\n        if not version:\n            return ret  # no specified version -&gt; anything goes\n        # otherwise: only return if it is compatible\n        req_ref: Optional[PluginRef] = None\n        req_ref = schemas.PluginRef(name=schema_name, version=version)\n        return ret if ret and req_ref.supports(ret.schema) else None\n\n    def _set_raw(self, schema_ref: PluginRef, obj: MetadataSchema) -&gt; None:\n\"\"\"Store metadata object as instance of passed schema at this node.\"\"\"\n        # reserve UUID, construct dataset path and store metadata object\n        obj_uuid = self._mc.metador._links.fresh_uuid()\n        obj_path = f\"{self._base_dir}/{_ep_name_for(schema_ref)}={str(obj_uuid)}\"\n        # store object\n        self._mc.__wrapped__[obj_path] = bytes(obj)\n        obj_node = self._mc.__wrapped__[obj_path]\n        assert isinstance(obj_node, H5DatasetLike)\n        stored_obj = StoredMetadata(uuid=obj_uuid, schema=schema_ref, node=obj_node)\n        self._objs[schema_ref] = stored_obj\n        # update TOC\n        self._mc.metador._links.register(stored_obj)\n        return\n\n    def _del_raw(self, schema_name: str, *, _unlink: bool = True) -&gt; None:\n\"\"\"Delete stored metadata for given schema at this node.\"\"\"\n        # NOTE: _unlink is only for the destroy method\n        stored_obj = self._objs[schema_name]\n        # unregister in TOC (will also trigger clean up there)\n        if _unlink:\n            self._mc.metador._links.unregister(stored_obj.uuid)\n        # remove metadata object\n        del self._objs[stored_obj.schema.name]\n        del self._mc.__wrapped__[stored_obj.node.name]\n        # no metadata objects left -&gt; remove metadata dir\n        if not self._objs:\n            del self._mc.__wrapped__[self._base_dir]\n        return\n\n    # helpers for container-level opertions (move, copy, delete etc)\n\n    def _destroy(self, *, _unlink: bool = True):\n\"\"\"Unregister and delete all metadata objects attached to this node.\"\"\"\n        # NOTE: _unlink is only set to false for node copy without metadata\n        for schema_name in list(self.keys()):\n            self._del_raw(schema_name, _unlink=_unlink)\n\n    # ----\n\n    def __init__(self, node: MetadorNode):\n        self._mc: MetadorContainer = node._self_container\n\"\"\"Underlying container (for convenience).\"\"\"\n\n        self._node: MetadorNode = node\n\"\"\"Underlying actual user node.\"\"\"\n\n        is_dataset = isinstance(node, H5DatasetLike)\n        self._base_dir: str = M.to_meta_base_path(node.name, is_dataset)\n\"\"\"Path of this metador metadata group node.\n\n        Actual node exists iff any metadata is stored for the node.\n        \"\"\"\n\n        self._objs: Dict[str, StoredMetadata] = {}\n\"\"\"Information about available metadata objects.\"\"\"\n\n        # load available object metadata encoded in the node names\n        meta_grp = cast(H5GroupLike, self._mc.__wrapped__.get(self._base_dir, {}))\n        for obj_node in meta_grp.values():\n            assert isinstance(obj_node, H5DatasetLike)\n            obj = StoredMetadata.from_node(obj_node)\n            self._objs[obj.schema.name] = obj\n\n    # ----\n\n    def keys(self) -&gt; KeysView[str]:\n\"\"\"Return names of explicitly attached metadata objects.\n\n        Transitive parent schemas are not included.\n        \"\"\"\n        return self._objs.keys()\n\n    def values(self) -&gt; ValuesView[StoredMetadata]:\n        self._node._guard_acl(NodeAcl.skel_only)\n        return self._objs.values()\n\n    def items(self) -&gt; ItemsView[str, StoredMetadata]:\n        self._node._guard_acl(NodeAcl.skel_only)\n        return self._objs.items()\n\n    # ----\n\n    def __len__(self) -&gt; int:\n\"\"\"Return number of explicitly attached metadata objects.\n\n        Transitive parent schemas are not counted.\n        \"\"\"\n        return len(self.keys())\n\n    def __iter__(self) -&gt; Iterator[str]:\n\"\"\"Iterate listing schema names of all actually attached metadata objects.\n\n        Transitive parent schemas are not included.\n        \"\"\"\n        return iter(self.keys())\n\n    # ----\n\n    def query(\n        self,\n        schema: Union[\n            str, Tuple[str, Optional[SemVerTuple]], PluginRef, Type[MetadataSchema]\n        ] = \"\",\n        version: Optional[SemVerTuple] = None,\n    ) -&gt; Iterator[PluginRef]:\n\"\"\"Return schema names for which objects at this node are compatible with passed schema.\n\n        Will also consider compatible child schema instances.\n\n        Returned iterator will yield passed schema first, if an object is available.\n        Apart from this, the order is not specified.\n        \"\"\"\n        schema_name, schema_ver = plugin_args(schema, version)\n        # no schema selected -&gt; list everything\n        if not schema_name:\n            for obj in self.values():\n                yield obj.schema\n            return\n\n        # try exact schema (in any compatible version, if version specified)\n        if obj := self._get_raw(schema_name, schema_ver):\n            yield obj.schema\n\n        # next, try compatible child schemas of compatible versions of requested schema\n        compat = set().union(\n            *(\n                self._mc.metador.schemas.children(ref)\n                for ref in self._mc.metador.schemas.versions(schema_name, schema_ver)\n            )\n        )\n        avail = {self._get_raw(s).schema for s in self.keys()}\n        for s_ref in avail.intersection(compat):\n            yield s_ref\n\n    def __contains__(\n        self,\n        schema: Union[\n            str, Tuple[str, Optional[SemVerTuple]], PluginRef, Type[MetadataSchema]\n        ],\n    ) -&gt; bool:\n\"\"\"Check whether a compatible metadata object for given schema exists.\n\n        Will also consider compatible child schema instances.\n        \"\"\"\n        if schema == \"\" or isinstance(schema, tuple) and schema[0] == \"\":\n            return False  # empty query lists everything, here the logic is inverted!\n        return next(self.query(schema), None) is not None\n\n    @overload\n    def __getitem__(self, schema: str) -&gt; MetadataSchema:\n        ...\n\n    @overload\n    def __getitem__(self, schema: Type[S]) -&gt; S:\n        ...\n\n    def __getitem__(self, schema: Union[str, Type[S]]) -&gt; Union[S, MetadataSchema]:\n\"\"\"Like get, but will raise KeyError on failure.\"\"\"\n        if ret := self.get(schema):\n            return ret\n        raise KeyError(schema)\n\n    @overload\n    def get(\n        self, schema: str, version: Optional[SemVerTuple] = None\n    ) -&gt; Optional[MetadataSchema]:\n        ...\n\n    @overload\n    def get(\n        self, schema: Type[S], version: Optional[SemVerTuple] = None\n    ) -&gt; Optional[S]:\n        ...\n\n    def get(\n        self, schema: Union[str, Type[S]], version: Optional[SemVerTuple] = None\n    ) -&gt; Optional[Union[MetadataSchema, S]]:\n\"\"\"Get a parsed metadata object matching the given schema (if it exists).\n\n        Will also consider compatible child schema instances.\n        \"\"\"\n        self._node._guard_acl(NodeAcl.skel_only)\n\n        # normalize arguments\n        schema_name, schema_ver = plugin_args(schema, version)\n\n        # get a compatible schema instance that is available at this node\n        compat_schema = next(self.query(schema_name, schema_ver), None)\n        if not compat_schema:\n            return None  # not found\n\n        # get class of schema and parse object\n        schema_class = self._require_schema(schema_name, schema_ver)\n        if obj := self._get_raw(compat_schema.name, compat_schema.version):\n            return cast(S, self._parse_obj(schema_class, obj.node[()]))\n        return None\n\n    def __setitem__(\n        self, schema: Union[str, Type[S]], value: Union[Dict[str, Any], MetadataSchema]\n    ) -&gt; None:\n\"\"\"Store metadata object as instance of given schema.\n\n        Raises KeyError if passed schema is not installed in environment.\n\n        Raises TypeError if passed schema is marked auxiliary.\n\n        Raises ValueError if an object for the schema already exists.\n\n        Raises ValidationError if passed object is not valid for the schema.\n        \"\"\"\n        self._node._guard_acl(NodeAcl.read_only)\n        schema_name, schema_ver = plugin_args(schema)\n\n        # if self.get(schema_name, schema_ver):  # &lt;- also subclass schemas\n        # NOTE: for practical reasons let's be more lenient here and allow redundancy\n        # hence only check if exact schema (modulo version) is already there\n        if self._get_raw(schema_name):  # &lt;- only same schema\n            msg = f\"Metadata object for schema {schema_name} already exists!\"\n            raise ValueError(msg)\n\n        schema_class = self._require_schema(schema_name, schema_ver)\n        checked_obj = self._parse_obj(schema_class, value)\n        self._set_raw(schema_class.Plugin.ref(), checked_obj)\n\n    def __delitem__(self, schema: Union[str, Type[MetadataSchema]]) -&gt; None:\n\"\"\"Delete metadata object explicitly stored for the passed schema.\n\n        If a schema class is passed, its version is ignored,\n        as each node may contain at most one explicit instance per schema.\n\n        Raises KeyError if no metadata object for that schema exists.\n        \"\"\"\n        self._node._guard_acl(NodeAcl.read_only)\n        schema_name, _ = plugin_args(schema)\n\n        if self._get_raw(schema_name) is None:\n            raise KeyError(schema_name)  # no (explicit) metadata object\n\n        self._del_raw(schema_name)\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorMeta.keys","title":"keys","text":"<pre><code>keys() -&gt; KeysView[str]\n</code></pre> <p>Return names of explicitly attached metadata objects.</p> <p>Transitive parent schemas are not included.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def keys(self) -&gt; KeysView[str]:\n\"\"\"Return names of explicitly attached metadata objects.\n\n    Transitive parent schemas are not included.\n    \"\"\"\n    return self._objs.keys()\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorMeta.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return number of explicitly attached metadata objects.</p> <p>Transitive parent schemas are not counted.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Return number of explicitly attached metadata objects.\n\n    Transitive parent schemas are not counted.\n    \"\"\"\n    return len(self.keys())\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorMeta.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[str]\n</code></pre> <p>Iterate listing schema names of all actually attached metadata objects.</p> <p>Transitive parent schemas are not included.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:\n\"\"\"Iterate listing schema names of all actually attached metadata objects.\n\n    Transitive parent schemas are not included.\n    \"\"\"\n    return iter(self.keys())\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorMeta.query","title":"query","text":"<pre><code>query(\n    schema: Union[\n        str,\n        Tuple[str, Optional[SemVerTuple]],\n        PluginRef,\n        Type[MetadataSchema],\n    ] = \"\",\n    version: Optional[SemVerTuple] = None,\n) -&gt; Iterator[PluginRef]\n</code></pre> <p>Return schema names for which objects at this node are compatible with passed schema.</p> <p>Will also consider compatible child schema instances.</p> <p>Returned iterator will yield passed schema first, if an object is available. Apart from this, the order is not specified.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def query(\n    self,\n    schema: Union[\n        str, Tuple[str, Optional[SemVerTuple]], PluginRef, Type[MetadataSchema]\n    ] = \"\",\n    version: Optional[SemVerTuple] = None,\n) -&gt; Iterator[PluginRef]:\n\"\"\"Return schema names for which objects at this node are compatible with passed schema.\n\n    Will also consider compatible child schema instances.\n\n    Returned iterator will yield passed schema first, if an object is available.\n    Apart from this, the order is not specified.\n    \"\"\"\n    schema_name, schema_ver = plugin_args(schema, version)\n    # no schema selected -&gt; list everything\n    if not schema_name:\n        for obj in self.values():\n            yield obj.schema\n        return\n\n    # try exact schema (in any compatible version, if version specified)\n    if obj := self._get_raw(schema_name, schema_ver):\n        yield obj.schema\n\n    # next, try compatible child schemas of compatible versions of requested schema\n    compat = set().union(\n        *(\n            self._mc.metador.schemas.children(ref)\n            for ref in self._mc.metador.schemas.versions(schema_name, schema_ver)\n        )\n    )\n    avail = {self._get_raw(s).schema for s in self.keys()}\n    for s_ref in avail.intersection(compat):\n        yield s_ref\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorMeta.__contains__","title":"__contains__","text":"<pre><code>__contains__(\n    schema: Union[\n        str,\n        Tuple[str, Optional[SemVerTuple]],\n        PluginRef,\n        Type[MetadataSchema],\n    ]\n) -&gt; bool\n</code></pre> <p>Check whether a compatible metadata object for given schema exists.</p> <p>Will also consider compatible child schema instances.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def __contains__(\n    self,\n    schema: Union[\n        str, Tuple[str, Optional[SemVerTuple]], PluginRef, Type[MetadataSchema]\n    ],\n) -&gt; bool:\n\"\"\"Check whether a compatible metadata object for given schema exists.\n\n    Will also consider compatible child schema instances.\n    \"\"\"\n    if schema == \"\" or isinstance(schema, tuple) and schema[0] == \"\":\n        return False  # empty query lists everything, here the logic is inverted!\n    return next(self.query(schema), None) is not None\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorMeta.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(\n    schema: Union[str, Type[S]]\n) -&gt; Union[S, MetadataSchema]\n</code></pre> <p>Like get, but will raise KeyError on failure.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def __getitem__(self, schema: Union[str, Type[S]]) -&gt; Union[S, MetadataSchema]:\n\"\"\"Like get, but will raise KeyError on failure.\"\"\"\n    if ret := self.get(schema):\n        return ret\n    raise KeyError(schema)\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorMeta.get","title":"get","text":"<pre><code>get(\n    schema: Union[str, Type[S]],\n    version: Optional[SemVerTuple] = None,\n) -&gt; Optional[Union[MetadataSchema, S]]\n</code></pre> <p>Get a parsed metadata object matching the given schema (if it exists).</p> <p>Will also consider compatible child schema instances.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def get(\n    self, schema: Union[str, Type[S]], version: Optional[SemVerTuple] = None\n) -&gt; Optional[Union[MetadataSchema, S]]:\n\"\"\"Get a parsed metadata object matching the given schema (if it exists).\n\n    Will also consider compatible child schema instances.\n    \"\"\"\n    self._node._guard_acl(NodeAcl.skel_only)\n\n    # normalize arguments\n    schema_name, schema_ver = plugin_args(schema, version)\n\n    # get a compatible schema instance that is available at this node\n    compat_schema = next(self.query(schema_name, schema_ver), None)\n    if not compat_schema:\n        return None  # not found\n\n    # get class of schema and parse object\n    schema_class = self._require_schema(schema_name, schema_ver)\n    if obj := self._get_raw(compat_schema.name, compat_schema.version):\n        return cast(S, self._parse_obj(schema_class, obj.node[()]))\n    return None\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorMeta.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(\n    schema: Union[str, Type[S]],\n    value: Union[Dict[str, Any], MetadataSchema],\n) -&gt; None\n</code></pre> <p>Store metadata object as instance of given schema.</p> <p>Raises KeyError if passed schema is not installed in environment.</p> <p>Raises TypeError if passed schema is marked auxiliary.</p> <p>Raises ValueError if an object for the schema already exists.</p> <p>Raises ValidationError if passed object is not valid for the schema.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def __setitem__(\n    self, schema: Union[str, Type[S]], value: Union[Dict[str, Any], MetadataSchema]\n) -&gt; None:\n\"\"\"Store metadata object as instance of given schema.\n\n    Raises KeyError if passed schema is not installed in environment.\n\n    Raises TypeError if passed schema is marked auxiliary.\n\n    Raises ValueError if an object for the schema already exists.\n\n    Raises ValidationError if passed object is not valid for the schema.\n    \"\"\"\n    self._node._guard_acl(NodeAcl.read_only)\n    schema_name, schema_ver = plugin_args(schema)\n\n    # if self.get(schema_name, schema_ver):  # &lt;- also subclass schemas\n    # NOTE: for practical reasons let's be more lenient here and allow redundancy\n    # hence only check if exact schema (modulo version) is already there\n    if self._get_raw(schema_name):  # &lt;- only same schema\n        msg = f\"Metadata object for schema {schema_name} already exists!\"\n        raise ValueError(msg)\n\n    schema_class = self._require_schema(schema_name, schema_ver)\n    checked_obj = self._parse_obj(schema_class, value)\n    self._set_raw(schema_class.Plugin.ref(), checked_obj)\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorMeta.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(\n    schema: Union[str, Type[MetadataSchema]]\n) -&gt; None\n</code></pre> <p>Delete metadata object explicitly stored for the passed schema.</p> <p>If a schema class is passed, its version is ignored, as each node may contain at most one explicit instance per schema.</p> <p>Raises KeyError if no metadata object for that schema exists.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def __delitem__(self, schema: Union[str, Type[MetadataSchema]]) -&gt; None:\n\"\"\"Delete metadata object explicitly stored for the passed schema.\n\n    If a schema class is passed, its version is ignored,\n    as each node may contain at most one explicit instance per schema.\n\n    Raises KeyError if no metadata object for that schema exists.\n    \"\"\"\n    self._node._guard_acl(NodeAcl.read_only)\n    schema_name, _ = plugin_args(schema)\n\n    if self._get_raw(schema_name) is None:\n        raise KeyError(schema_name)  # no (explicit) metadata object\n\n    self._del_raw(schema_name)\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorDataset","title":"MetadorDataset","text":"<p>             Bases: <code>MetadorNode</code></p> <p>Metador wrapper for a HDF5 Dataset.</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>class MetadorDataset(MetadorNode):\n\"\"\"Metador wrapper for a HDF5 Dataset.\"\"\"\n\n    __wrapped__: H5DatasetLike\n\n    # manually assembled from public methods which h5py.Dataset provides\n    _self_RO_FORBIDDEN = {\"resize\", \"make_scale\", \"write_direct\", \"flush\"}\n\n    def __getattr__(self, key):\n        if self.acl[NodeAcl.read_only] and key in self._self_RO_FORBIDDEN:\n            self._guard_acl(NodeAcl.read_only, key)\n        if self.acl[NodeAcl.skel_only] and key == \"get\":\n            self._guard_acl(NodeAcl.skel_only, key)\n\n        return getattr(self.__wrapped__, key)\n\n    # prevent getter of node if marked as skel_only\n    def __getitem__(self, *args, **kwargs):\n        self._guard_acl(NodeAcl.skel_only, \"__getitem__\")\n        return self.__wrapped__.__getitem__(*args, **kwargs)\n\n    # prevent mutating method calls of node is marked as read_only\n\n    def __setitem__(self, *args, **kwargs):\n        self._guard_acl(NodeAcl.read_only, \"__setitem__\")\n        return self.__wrapped__.__setitem__(*args, **kwargs)\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorGroup","title":"MetadorGroup","text":"<p>             Bases: <code>MetadorNode</code></p> <p>Wrapper for a HDF5 Group.</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>class MetadorGroup(MetadorNode):\n\"\"\"Wrapper for a HDF5 Group.\"\"\"\n\n    __wrapped__: H5GroupLike\n\n    def _destroy_meta(self, _unlink: bool = True):\n\"\"\"Destroy all attached metadata at and below this node (recursively).\"\"\"\n        super()._destroy_meta(_unlink=_unlink)  # this node\n        for child in self.values():  # recurse\n            child._destroy_meta(_unlink=_unlink)\n\n    # these access entities in read-only way:\n\n    get = _wrap_method(\"get\", is_read_only_method=True)\n    __getitem__ = _wrap_method(\"__getitem__\", is_read_only_method=True)\n\n    # these just create new entities with no metadata attached:\n\n    create_group = _wrap_method(\"create_group\")\n    require_group = _wrap_method(\"require_group\")\n    create_dataset = _wrap_method(\"create_dataset\")\n    require_dataset = _wrap_method(\"require_dataset\")\n\n    def __setitem__(self, name, value):\n        if any(map(lambda x: isinstance(value, x), _H5_REF_TYPES)):\n            raise ValueError(f\"Unsupported reference type: {type(value).__name__}\")\n\n        return _wrap_method(\"__setitem__\")(self, name, value)\n\n    # following all must be filtered to hide metador-specific structures:\n\n    # must wrap nodes passed into the callback function and filter visited names\n    def visititems(self, func):\n        def wrapped_func(name, node):\n            if M.is_internal_path(node.name):\n                return  # skip path/node\n            return func(name, self._wrap_if_node(node))\n\n        return self.__wrapped__.visititems(wrapped_func)  # RAW\n\n    # paths passed to visit also must be filtered, so must override this one too\n    def visit(self, func):\n        def wrapped_func(name, _):\n            return func(name)\n\n        return self.visititems(wrapped_func)\n\n    # following also depend on the filtered sequence, directly\n    # filter the items, derive other related functions based on that\n\n    def items(self):\n        for k, v in self.__wrapped__.items():\n            if v is None:\n                # NOTE: e.g. when nodes are deleted/moved during iteration,\n                # v can suddenly be None -&gt; we need to catch this case!\n                continue\n            if not M.is_internal_path(v.name):\n                yield (k, self._wrap_if_node(v))\n\n    def values(self):\n        return map(lambda x: x[1], self.items())\n\n    def keys(self):\n        return map(lambda x: x[0], self.items())\n\n    def __iter__(self):\n        return iter(self.keys())\n\n    def __len__(self):\n        return len(list(self.keys()))\n\n    def __contains__(self, name: str):\n        self._guard_path(name)\n        if name[0] == \"/\" and self.name != \"/\":\n            return name in self[\"/\"]\n        segs = name.lstrip(\"/\").split(\"/\")\n        has_first_seg = segs[0] in self.keys()\n        if len(segs) == 1:\n            return has_first_seg\n        else:\n            if nxt := self.get(segs[0]):\n                return \"/\".join(segs[1:]) in nxt\n            return False\n\n    # these we can take care of but are a bit more tricky to think through\n\n    def __delitem__(self, name: str):\n        self._guard_acl(NodeAcl.read_only, \"__delitem__\")\n        self._guard_path(name)\n\n        node = self[name]\n        # clean up metadata (recursively, if a group)\n        node._destroy_meta()\n        # kill the actual data\n        return _wrap_method(\"__delitem__\")(self, name)\n\n    def move(self, source: str, dest: str):\n        self._guard_acl(NodeAcl.read_only, \"move\")\n        self._guard_path(source)\n        self._guard_path(dest)\n\n        src_metadir = self[source].meta._base_dir\n        # if actual data move fails, an exception will prevent the rest\n        self.__wrapped__.move(source, dest)  # RAW\n\n        # if we're here, no problems -&gt; proceed with moving metadata\n        dst_node = self[dest]\n        if isinstance(dst_node, MetadorDataset):\n            dst_metadir = dst_node.meta._base_dir\n            # dataset has its metadata stored in parallel -&gt; need to take care of it\n            meta_base = dst_metadir\n            if src_metadir in self.__wrapped__:  # RAW\n                self.__wrapped__.move(src_metadir, dst_metadir)  # RAW\n        else:\n            # directory where to fix up metadata object TOC links\n            # when a group was moved, all metadata is contained in dest -&gt; search it\n            meta_base = dst_node.name\n\n        # re-link metadata object TOC links\n        if meta_base_node := self.__wrapped__.get(meta_base):\n            assert isinstance(meta_base_node, H5GroupLike)\n            missing = self._self_container.metador._links.find_missing(meta_base_node)\n            self._self_container.metador._links.repair_missing(missing, update=True)\n\n    def copy(\n        self,\n        source: Union[str, MetadorGroup, MetadorDataset],\n        dest: Union[str, MetadorGroup],\n        **kwargs,\n    ):\n        self._guard_acl(NodeAcl.read_only, \"copy\")\n\n        # get source node and its name without the path and its type\n        src_node: MetadorNode\n        if isinstance(source, str):\n            self._guard_path(source)\n            src_node = self[source]\n        elif isinstance(source, MetadorNode):\n            src_node = source\n        else:\n            raise ValueError(\"Copy source must be path, Group or Dataset!\")\n        src_is_dataset: bool = isinstance(src_node, MetadorDataset)\n        src_name: str = src_node.name.split(\"/\")[-1]\n        # user can override name at target\n        dst_name: str = kwargs.pop(\"name\", src_name)\n\n        # fix up target path\n        dst_path: str\n        if isinstance(dest, str):\n            self._guard_path(dest)\n            dst_path = dest\n        elif isinstance(dest, MetadorGroup):\n            dst_path = dest.name + f\"/{dst_name}\"\n        else:\n            raise ValueError(\"Copy dest must be path or Group!\")\n\n        # get other allowed options\n        without_attrs: bool = kwargs.pop(\"without_attrs\", False)\n        without_meta: bool = kwargs.pop(\"without_meta\", False)\n        if kwargs:\n            raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n\n        # perform copy\n        copy_kwargs = {\n            \"name\": None,\n            \"shallow\": False,\n            \"expand_soft\": True,\n            \"expand_external\": True,\n            \"expand_refs\": True,\n            \"without_attrs\": without_attrs,\n        }\n        self.__wrapped__.copy(source, dst_path, **copy_kwargs)  # RAW\n        dst_node = self[dst_path]  # exists now\n\n        if src_is_dataset and not without_meta:\n            # because metadata lives in parallel group, need to copy separately:\n            src_meta: str = src_node.meta._base_dir\n            dst_meta: str = dst_node.meta._base_dir  # node will not exist yet\n            self.__wrapped__.copy(src_meta, dst_meta, **copy_kwargs)  # RAW\n\n            # register in TOC:\n            dst_meta_node = self.__wrapped__[dst_meta]\n            assert isinstance(dst_meta_node, H5GroupLike)\n            missing = self._self_container.metador._links.find_missing(dst_meta_node)\n            self._self_container.metador._links.repair_missing(missing)\n\n        if not src_is_dataset:\n            if without_meta:\n                # need to destroy copied metadata copied with the source group\n                # but keep TOC links (they point to original copy!)\n                dst_node._destroy_meta(_unlink=False)\n            else:\n                # register copied metadata objects under new uuids\n                missing = self._self_container.metador._links.find_missing(dst_node)\n                self._self_container.metador._links.repair_missing(missing)\n\n    def __getattr__(self, key):\n        if hasattr(self.__wrapped__, key):\n            raise UnsupportedOperationError(key)  # deliberately unsupported\n        else:\n            msg = f\"'{type(self).__name__}' object has no attribute '{key}'\"\n            raise AttributeError(msg)\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorContainer","title":"MetadorContainer","text":"<p>             Bases: <code>MetadorGroup</code></p> <p>Wrapper class adding Metador container interface to h5py.File-like objects.</p> <p>The wrapper ensures that any actions done to IH5Records through this interface also work with plain h5py.Files.</p> <p>There are no guarantees about behaviour with h5py methods not supported by IH5Records.</p> <p>Given <code>old: MetadorContainer</code>, <code>MetadorContainer(old.data_source, driver=old.data_driver)</code> should be able to construct another object to access the same data (assuming it is not locked).</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>class MetadorContainer(MetadorGroup):\n\"\"\"Wrapper class adding Metador container interface to h5py.File-like objects.\n\n    The wrapper ensures that any actions done to IH5Records through this interface\n    also work with plain h5py.Files.\n\n    There are no guarantees about behaviour with h5py methods not supported by IH5Records.\n\n    Given `old: MetadorContainer`, `MetadorContainer(old.data_source, driver=old.data_driver)`\n    should be able to construct another object to access the same data (assuming it is not locked).\n    \"\"\"\n\n    __wrapped__: H5FileLike\n\n    _self_SUPPORTED: Set[str] = {\"mode\", \"flush\", \"close\"}\n\n    # ---- new container-level interface ----\n\n    _self_toc: MetadorContainerTOC\n\n    @property\n    def metador(self) -&gt; MetadorContainerTOC:\n\"\"\"Access interface to Metador metadata object index.\"\"\"\n        return self._self_toc\n\n    def __init__(\n        self,\n        name_or_obj: Union[MetadorDriver, Any],\n        mode: Optional[OpenMode] = \"r\",\n        *,\n        # NOTE: driver takes class instead of enum to also allow subclasses\n        driver: Optional[Type[MetadorDriver]] = None,\n    ):\n\"\"\"Initialize a MetadorContainer instance from file(s) or a supported object.\n\n        The `mode` argument is ignored when simply wrapping an object.\n\n        If a data source such as a path is passed, will instantiate the object first,\n        using the default H5File driver or the passed `driver` keyword argument.\n        \"\"\"\n        # wrap the h5file-like object (will set self.__wrapped__)\n        super().__init__(self, to_h5filelike(name_or_obj, mode, driver=driver))\n        # initialize metador-specific stuff\n        self._self_toc = MetadorContainerTOC(self)\n\n    # not clear if we want these in the public interface. keep this private for now:\n\n    # def _find_orphan_meta(self) -&gt; List[str]:\n    #     \"\"\"Return list of paths to metadata that has no corresponding user node anymore.\"\"\"\n    #     ret: List[str] = []\n\n    #     def collect_orphans(name: str):\n    #         if M.is_meta_base_path(name):\n    #             if M.to_data_node_path(name) not in self:\n    #                 ret.append(name)\n\n    #     self.__wrapped__.visit(collect_orphans)\n    #     return ret\n\n    # def _repair(self, remove_orphans: bool = False):\n    #     \"\"\"Repair container structure on best-effort basis.\n\n    #     This will ensure that the TOC points to existing metadata objects\n    #     and that all metadata objects are listed in the TOC.\n\n    #     If remove_orphans is set, will erase metadata not belonging to an existing node.\n\n    #     Notice that missing schema plugin dependency metadata cannot be restored.\n    #     \"\"\"\n    #     if remove_orphans:\n    #         for path in self._find_orphan_meta():\n    #             del self.__wrapped__[path]\n    #     self.toc._links.find_broken(repair=True)\n    #     missing = self.toc._links._find_missing(\"/\")\n    #     self.toc._links.repair_missing(missing)\n\n    # ---- pass through HDF5 group methods to a wrapped root group instance ----\n\n    def __getattr__(self, key: str):\n        if key in self._self_SUPPORTED:\n            return getattr(self.__wrapped__, key)\n        return super().__getattr__(key)  # ask group for method\n\n    # context manager: return the wrapper back, not the raw thing:\n\n    def __enter__(self):\n        self.__wrapped__.__enter__()\n        return self\n\n    def __exit__(self, *args):\n        return self.__wrapped__.__exit__(*args)\n\n    # we want these also to be forwarded to the wrapped group, not the raw object:\n\n    def __dir__(self):\n        return list(set(super().__dir__()).union(type(self).__dict__.keys()))\n\n    # make wrapper transparent:\n\n    def __repr__(self) -&gt; str:\n        return repr(self.__wrapped__)  # shows that its a File, not just a Group\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorContainer.metador","title":"metador  <code>property</code>","text":"<pre><code>metador: MetadorContainerTOC\n</code></pre> <p>Access interface to Metador metadata object index.</p>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorContainer.__init__","title":"__init__","text":"<pre><code>__init__(\n    name_or_obj: Union[MetadorDriver, Any],\n    mode: Optional[OpenMode] = \"r\",\n    *,\n    driver: Optional[Type[MetadorDriver]] = None\n)\n</code></pre> <p>Initialize a MetadorContainer instance from file(s) or a supported object.</p> <p>The <code>mode</code> argument is ignored when simply wrapping an object.</p> <p>If a data source such as a path is passed, will instantiate the object first, using the default H5File driver or the passed <code>driver</code> keyword argument.</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>def __init__(\n    self,\n    name_or_obj: Union[MetadorDriver, Any],\n    mode: Optional[OpenMode] = \"r\",\n    *,\n    # NOTE: driver takes class instead of enum to also allow subclasses\n    driver: Optional[Type[MetadorDriver]] = None,\n):\n\"\"\"Initialize a MetadorContainer instance from file(s) or a supported object.\n\n    The `mode` argument is ignored when simply wrapping an object.\n\n    If a data source such as a path is passed, will instantiate the object first,\n    using the default H5File driver or the passed `driver` keyword argument.\n    \"\"\"\n    # wrap the h5file-like object (will set self.__wrapped__)\n    super().__init__(self, to_h5filelike(name_or_obj, mode, driver=driver))\n    # initialize metador-specific stuff\n    self._self_toc = MetadorContainerTOC(self)\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorContainerTOC","title":"MetadorContainerTOC","text":"<p>Interface to the Metador metadata index (table of contents) of a container.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>class MetadorContainerTOC:\n\"\"\"Interface to the Metador metadata index (table of contents) of a container.\"\"\"\n\n    def __init__(self, container: MetadorContainer):\n        self._container = container\n        self._raw = self._container.__wrapped__\n\n        ver = self.spec_version if M.METADOR_VERSION_PATH in self._raw else None\n        if ver:\n            if ver &gt;= [2]:\n                msg = f\"Unsupported Metador container version: {ver}\"\n                raise ValueError(msg)\n        else:\n            if self._container.acl[NodeAcl.read_only]:\n                msg = \"Container is read-only and does not look like a Metador container! \"\n                msg += \"Please open in writable mode to initialize Metador structures!\"\n                raise ValueError(msg)\n\n            # writable + no version = fresh (for metador), initialize it\n            self._raw[M.METADOR_VERSION_PATH] = M.METADOR_SPEC_VERSION\n            self._raw[M.METADOR_UUID_PATH] = str(uuid1())\n\n        # if we're here, we have a prepared container TOC structure\n\n        # proceed to initialize TOC\n        self._driver_type: MetadorDriverEnum = get_driver_type(self._raw)\n\n        self._packages = TOCPackages(self._raw)\n        self._schemas = TOCSchemas(self._raw, self._packages)\n        self._links = TOCLinks(self._raw, self._schemas)\n\n    # ----\n\n    @property\n    def driver_type(self) -&gt; MetadorDriverEnum:\n\"\"\"Return the type of the container driver.\"\"\"\n        return self._driver_type\n\n    @property\n    def driver(self) -&gt; Type[MetadorDriver]:\n\"\"\"Return the container driver class used by the container.\"\"\"\n        return METADOR_DRIVERS[self.driver_type]\n\n    @property\n    def source(self) -&gt; Any:\n\"\"\"Return data underlying thes container (file, set of files, etc. used with the driver).\"\"\"\n        return get_source(self._raw, self.driver_type)\n\n    # ----\n\n    @property\n    def container_uuid(self) -&gt; UUID:\n\"\"\"Return UUID of the container.\"\"\"\n        uuid = self._raw[M.METADOR_UUID_PATH]\n        uuid_ds = cast(H5DatasetLike, uuid)\n        return UUID(uuid_ds[()].decode(\"utf-8\"))\n\n    @property\n    def spec_version(self) -&gt; List[int]:\n\"\"\"Return Metador container specification version of the container.\"\"\"\n        ver = cast(H5DatasetLike, self._raw[M.METADOR_VERSION_PATH])\n        return list(map(int, ver[()].decode(\"utf-8\").split(\".\")))\n\n    @property\n    def schemas(self):\n\"\"\"Information about all schemas used for metadata objects in this container.\"\"\"\n        return self._schemas\n\n    def query(\n        self,\n        schema: Union[str, Type[S]],\n        version: Optional[SemVerTuple] = None,\n        *,\n        node: Optional[MetadorNode] = None,\n    ) -&gt; Iterator[MetadorNode]:\n\"\"\"Return nodes that contain a metadata object compatible with the given schema.\"\"\"\n        schema_name, schema_ver = plugin_args(schema, version)\n        if not schema_name:  # could be e.g. empty string\n            msg = \"A schema name, plugin reference or class must be provided!\"\n            raise ValueError(msg)\n\n        start_node: MetadorNode = node or self._container[\"/\"]\n\n        # check start node metadata explicitly\n        if (schema_name, schema_ver) in start_node.meta:\n            yield start_node\n\n        if not isinstance(start_node, H5GroupLike):\n            return  # the node is not group-like, cannot be traversed down\n\n        # collect nodes below start node recursively\n        # NOTE: yielding from the collect_nodes does not work :'(\n        # so we have to actually materialize the list &gt;.&lt;\n        # but we expose only the generator interface anyway (better design)\n        # (maybe consider replacing visititems with a custom traversal here)\n        ret: List[MetadorNode] = []\n\n        def collect_nodes(_, node: MetadorNode):\n            if (schema_name, schema_ver) in node.meta:\n                ret.append(node)\n\n        start_node.visititems(collect_nodes)\n        yield from iter(ret)\n</code></pre>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorContainerTOC.driver_type","title":"driver_type  <code>property</code>","text":"<pre><code>driver_type: MetadorDriverEnum\n</code></pre> <p>Return the type of the container driver.</p>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorContainerTOC.driver","title":"driver  <code>property</code>","text":"<pre><code>driver: Type[MetadorDriver]\n</code></pre> <p>Return the container driver class used by the container.</p>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorContainerTOC.source","title":"source  <code>property</code>","text":"<pre><code>source: Any\n</code></pre> <p>Return data underlying thes container (file, set of files, etc. used with the driver).</p>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorContainerTOC.container_uuid","title":"container_uuid  <code>property</code>","text":"<pre><code>container_uuid: UUID\n</code></pre> <p>Return UUID of the container.</p>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorContainerTOC.spec_version","title":"spec_version  <code>property</code>","text":"<pre><code>spec_version: List[int]\n</code></pre> <p>Return Metador container specification version of the container.</p>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorContainerTOC.schemas","title":"schemas  <code>property</code>","text":"<pre><code>schemas\n</code></pre> <p>Information about all schemas used for metadata objects in this container.</p>"},{"location":"reference/metador_core/container/#metador_core.container.MetadorContainerTOC.query","title":"query","text":"<pre><code>query(\n    schema: Union[str, Type[S]],\n    version: Optional[SemVerTuple] = None,\n    *,\n    node: Optional[MetadorNode] = None\n) -&gt; Iterator[MetadorNode]\n</code></pre> <p>Return nodes that contain a metadata object compatible with the given schema.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def query(\n    self,\n    schema: Union[str, Type[S]],\n    version: Optional[SemVerTuple] = None,\n    *,\n    node: Optional[MetadorNode] = None,\n) -&gt; Iterator[MetadorNode]:\n\"\"\"Return nodes that contain a metadata object compatible with the given schema.\"\"\"\n    schema_name, schema_ver = plugin_args(schema, version)\n    if not schema_name:  # could be e.g. empty string\n        msg = \"A schema name, plugin reference or class must be provided!\"\n        raise ValueError(msg)\n\n    start_node: MetadorNode = node or self._container[\"/\"]\n\n    # check start node metadata explicitly\n    if (schema_name, schema_ver) in start_node.meta:\n        yield start_node\n\n    if not isinstance(start_node, H5GroupLike):\n        return  # the node is not group-like, cannot be traversed down\n\n    # collect nodes below start node recursively\n    # NOTE: yielding from the collect_nodes does not work :'(\n    # so we have to actually materialize the list &gt;.&lt;\n    # but we expose only the generator interface anyway (better design)\n    # (maybe consider replacing visititems with a custom traversal here)\n    ret: List[MetadorNode] = []\n\n    def collect_nodes(_, node: MetadorNode):\n        if (schema_name, schema_ver) in node.meta:\n            ret.append(node)\n\n    start_node.visititems(collect_nodes)\n    yield from iter(ret)\n</code></pre>"},{"location":"reference/metador_core/container/drivers/","title":"drivers","text":"<p>Metador driver abstraction in order to enable different underlying implementations.</p>"},{"location":"reference/metador_core/container/drivers/#metador_core.container.drivers.MetadorDriver","title":"MetadorDriver  <code>module-attribute</code>","text":"<pre><code>MetadorDriver = Union[h5py.File, IH5Record]\n</code></pre> <p>Union of all supported classes (for static type check).</p>"},{"location":"reference/metador_core/container/drivers/#metador_core.container.drivers.METADOR_DRIVERS","title":"METADOR_DRIVERS  <code>module-attribute</code>","text":"<pre><code>METADOR_DRIVERS = MetadorDriverEnum.to_dict()\n</code></pre> <p>Dict representation of MetadorDriverEnum.</p>"},{"location":"reference/metador_core/container/drivers/#metador_core.container.drivers.METADOR_DRIVER_CLASSES","title":"METADOR_DRIVER_CLASSES  <code>module-attribute</code>","text":"<pre><code>METADOR_DRIVER_CLASSES = tuple(METADOR_DRIVERS.values())\n</code></pre> <p>Tuple of all supported classes (for instance check).</p>"},{"location":"reference/metador_core/container/drivers/#metador_core.container.drivers.MetadorDriverEnum","title":"MetadorDriverEnum","text":"<p>             Bases: <code>Enum</code></p> <p>Supported classes that work with MetadorContainer.</p> <p>Note that they must be unrelated (i.e. not subclasses of each other).</p> Source code in <code>src/metador_core/container/drivers.py</code> <pre><code>class MetadorDriverEnum(Enum):\n\"\"\"Supported classes that work with MetadorContainer.\n\n    Note that they must be unrelated (i.e. not subclasses of each other).\n    \"\"\"\n\n    HDF5 = h5py.File\n    IH5 = IH5Record\n\n    @classmethod\n    def to_dict(cls):\n        return {x: x.value for x in iter(cls)}\n</code></pre>"},{"location":"reference/metador_core/container/drivers/#metador_core.container.drivers.get_driver_type","title":"get_driver_type","text":"<pre><code>get_driver_type(\n    raw_cont: MetadorDriver,\n) -&gt; MetadorDriverEnum\n</code></pre> <p>Return the driver type of container (if it is a suitable (sub)class).</p> Source code in <code>src/metador_core/container/drivers.py</code> <pre><code>def get_driver_type(raw_cont: MetadorDriver) -&gt; MetadorDriverEnum:\n\"\"\"Return the driver type of container (if it is a suitable (sub)class).\"\"\"\n    for val, cls in METADOR_DRIVERS.items():\n        if isinstance(raw_cont, cls):\n            return val\n    raise ValueError(f\"Object not of known container driver type: {raw_cont}\")\n</code></pre>"},{"location":"reference/metador_core/container/drivers/#metador_core.container.drivers.get_source","title":"get_source","text":"<pre><code>get_source(\n    raw_cont: MetadorDriver,\n    driver: MetadorDriverEnum = None,\n) -&gt; Any\n</code></pre> <p>Return an object (i.e. input resource(s)) needed to re-open the given container.</p> Source code in <code>src/metador_core/container/drivers.py</code> <pre><code>def get_source(raw_cont: MetadorDriver, driver: MetadorDriverEnum = None) -&gt; Any:\n\"\"\"Return an object (i.e. input resource(s)) needed to re-open the given container.\"\"\"\n    c = cast(Any, raw_cont)\n    driver = driver or get_driver_type(raw_cont)\n    if driver == MetadorDriverEnum.HDF5:\n        return c.filename\n    elif driver == MetadorDriverEnum.IH5:\n        return c.ih5_files\n</code></pre>"},{"location":"reference/metador_core/container/drivers/#metador_core.container.drivers.to_h5filelike","title":"to_h5filelike","text":"<pre><code>to_h5filelike(\n    name_or_obj: Union[MetadorDriver, Any],\n    mode: OpenMode = \"r\",\n    *,\n    driver: Optional[Type[MetadorDriver]] = None\n) -&gt; H5FileLike\n</code></pre> <p>Given a container or a resource with a driver, try to return a H5FileLike.</p> <p>If first argument is instance of a known driver is returned unchanged. Otherwise, will try to open it using the driver (h5py.File by default).</p> <p>Returns a H5FileLike compatible with MetadorContainer, or raises ValueError.</p> Source code in <code>src/metador_core/container/drivers.py</code> <pre><code>def to_h5filelike(\n    name_or_obj: Union[MetadorDriver, Any],\n    mode: OpenMode = \"r\",\n    *,\n    # NOTE: driver takes actual class instead of enum, to also allow subclasses\n    driver: Optional[Type[MetadorDriver]] = None,\n) -&gt; H5FileLike:\n\"\"\"Given a container or a resource with a driver, try to return a H5FileLike.\n\n    If first argument is instance of a known driver is returned unchanged.\n    Otherwise, will try to open it using the driver (h5py.File by default).\n\n    Returns a H5FileLike compatible with MetadorContainer, or raises ValueError.\n    \"\"\"\n    if isinstance(name_or_obj, METADOR_DRIVER_CLASSES):\n        # user has passed a h5file-like object already\n        return cast(H5FileLike, name_or_obj)\n    else:\n        # user passed arguments to try instantiating a container object\n        driver = driver or h5py.File\n        if not issubclass(driver, METADOR_DRIVER_CLASSES):\n            msg = f\"Passed driver class not supported: {driver}\"\n            raise ValueError(msg)\n        return cast(H5FileLike, driver(cast(Any, name_or_obj), mode))\n</code></pre>"},{"location":"reference/metador_core/container/interface/","title":"interface","text":""},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.NodeAcl","title":"NodeAcl","text":"<p>             Bases: <code>Enum</code></p> <p>Metador node soft access control flags.</p> <p>Soft means - they can be bypassed, it is about trying to prevent errors.</p> <p>Group nodes inherit their ACL flags to child nodes.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>class NodeAcl(Enum):\n\"\"\"Metador node soft access control flags.\n\n    Soft means - they can be bypassed, it is about trying to prevent errors.\n\n    Group nodes inherit their ACL flags to child nodes.\n    \"\"\"\n\n    # NOTE: maybe refactor this to IntFlag? Then e.g. restrict() interface can be like:\n    # node.restrict(acl=NodeAcl.read_only | NodeAcl.local_only)\n\n    read_only = auto()\n\"\"\"Forbid calling methods mutating contents of (meta)data.\"\"\"\n\n    local_only = auto()\n\"\"\"Forbid access to parents beyond the initial local node.\"\"\"\n\n    skel_only = auto()\n\"\"\"Forbid reading datasets and metadata, only existence can be checked.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.NodeAcl.read_only","title":"read_only  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>read_only = auto()\n</code></pre> <p>Forbid calling methods mutating contents of (meta)data.</p>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.NodeAcl.local_only","title":"local_only  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>local_only = auto()\n</code></pre> <p>Forbid access to parents beyond the initial local node.</p>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.NodeAcl.skel_only","title":"skel_only  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>skel_only = auto()\n</code></pre> <p>Forbid reading datasets and metadata, only existence can be checked.</p>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.StoredMetadata","title":"StoredMetadata  <code>dataclass</code>","text":"<p>Information about a metadata schema instance stored at a node.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>@dataclass\nclass StoredMetadata:\n\"\"\"Information about a metadata schema instance stored at a node.\"\"\"\n\n    uuid: UUID\n\"\"\"UUID identifying the metadata object in the container.\n\n    Used for bookkeeping, i.e. keeping the container TOC in sync.\n    \"\"\"\n\n    schema: PluginRef\n\"\"\"Schema the object is an instance of.\"\"\"\n\n    node: H5DatasetLike\n\"\"\"Node with serialized metadata object.\"\"\"\n\n    def to_path(self):\n\"\"\"Return path of metadata object.\n\n        (E.g. to return canonical path for copying TOC link nodes)\n        \"\"\"\n        prefix = self.node.parent.name\n        ep_name = to_ep_name(self.schema.name, self.schema.version)\n        return f\"{prefix}/{ep_name}={self.uuid}\"\n\n    @staticmethod\n    def from_node(obj: H5DatasetLike) -&gt; StoredMetadata:\n\"\"\"Instantiate info about a stored metadata node.\"\"\"\n        path = obj.name\n        segs = path.lstrip(\"/\").split(\"/\")\n        ep_name, uuid_str = segs.pop().split(\"=\")\n        s_name, s_vers = from_ep_name(EPName(ep_name))\n        uuid = UUID(uuid_str)\n        s_ref = schemas.PluginRef(name=s_name, version=s_vers)\n        return StoredMetadata(uuid=uuid, schema=s_ref, node=obj)\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.StoredMetadata.uuid","title":"uuid  <code>instance-attribute</code>","text":"<pre><code>uuid: UUID\n</code></pre> <p>UUID identifying the metadata object in the container.</p> <p>Used for bookkeeping, i.e. keeping the container TOC in sync.</p>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.StoredMetadata.schema","title":"schema  <code>instance-attribute</code>","text":"<pre><code>schema: PluginRef\n</code></pre> <p>Schema the object is an instance of.</p>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.StoredMetadata.node","title":"node  <code>instance-attribute</code>","text":"<pre><code>node: H5DatasetLike\n</code></pre> <p>Node with serialized metadata object.</p>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.StoredMetadata.to_path","title":"to_path","text":"<pre><code>to_path()\n</code></pre> <p>Return path of metadata object.</p> <p>(E.g. to return canonical path for copying TOC link nodes)</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def to_path(self):\n\"\"\"Return path of metadata object.\n\n    (E.g. to return canonical path for copying TOC link nodes)\n    \"\"\"\n    prefix = self.node.parent.name\n    ep_name = to_ep_name(self.schema.name, self.schema.version)\n    return f\"{prefix}/{ep_name}={self.uuid}\"\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.StoredMetadata.from_node","title":"from_node  <code>staticmethod</code>","text":"<pre><code>from_node(obj: H5DatasetLike) -&gt; StoredMetadata\n</code></pre> <p>Instantiate info about a stored metadata node.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>@staticmethod\ndef from_node(obj: H5DatasetLike) -&gt; StoredMetadata:\n\"\"\"Instantiate info about a stored metadata node.\"\"\"\n    path = obj.name\n    segs = path.lstrip(\"/\").split(\"/\")\n    ep_name, uuid_str = segs.pop().split(\"=\")\n    s_name, s_vers = from_ep_name(EPName(ep_name))\n    uuid = UUID(uuid_str)\n    s_ref = schemas.PluginRef(name=s_name, version=s_vers)\n    return StoredMetadata(uuid=uuid, schema=s_ref, node=obj)\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorMeta","title":"MetadorMeta","text":"<p>Interface to Metador metadata objects stored at a single HDF5 node.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>class MetadorMeta:\n\"\"\"Interface to Metador metadata objects stored at a single HDF5 node.\"\"\"\n\n    # helpers for __getitem__ and __setitem__\n\n    @staticmethod\n    def _require_schema(\n        schema_name: str, schema_ver: Optional[SemVerTuple]\n    ) -&gt; Type[MetadataSchema]:\n\"\"\"Return compatible installed schema class, if possible.\n\n        Raises KeyError if no suitable schema was found.\n\n        Raises TypeError if an auxiliary schema is requested.\n        \"\"\"\n        schema_class = schemas._get_unsafe(\n            schema_name, schema_ver\n        )  # can raise KeyError\n        if schema_class.Plugin.auxiliary:  # reject auxiliary schemas in container\n            msg = f\"Cannot attach instances of auxiliary schema '{schema_name}' to a node!\"\n            raise TypeError(msg)\n        return schema_class\n\n    @staticmethod\n    def _parse_obj(\n        schema: Type[S], obj: Union[str, bytes, Dict[str, Any], MetadataSchema]\n    ) -&gt; S:\n\"\"\"Return original object if it is an instance of passed schema, or else parse it.\n\n        Raises ValidationError if parsing fails.\n        \"\"\"\n        if isinstance(obj, schema):\n            return obj  # skip validation, already correct model!\n        # try to convert/parse it:\n        if isinstance(obj, (str, bytes)):\n            return schema.parse_raw(obj)\n        if isinstance(obj, MetadataSchema):\n            return schema.parse_obj(obj.dict())\n        else:  # dict\n            return schema.parse_obj(obj)\n\n    # raw getters and setters don't care about the environment,\n    # they work only based on what objects are available and compatible\n    # and do not perform validation etc.\n\n    def _get_raw(\n        self, schema_name: str, version: Optional[SemVerTuple] = None\n    ) -&gt; Optional[StoredMetadata]:\n\"\"\"Return stored metadata for given schema at this node (or None).\n\n        If a version is passed, the stored version must also be compatible.\n        \"\"\"\n        # retrieve stored instance (if suitable)\n        ret: Optional[StoredMetadata] = self._objs.get(schema_name)\n        if not version:\n            return ret  # no specified version -&gt; anything goes\n        # otherwise: only return if it is compatible\n        req_ref: Optional[PluginRef] = None\n        req_ref = schemas.PluginRef(name=schema_name, version=version)\n        return ret if ret and req_ref.supports(ret.schema) else None\n\n    def _set_raw(self, schema_ref: PluginRef, obj: MetadataSchema) -&gt; None:\n\"\"\"Store metadata object as instance of passed schema at this node.\"\"\"\n        # reserve UUID, construct dataset path and store metadata object\n        obj_uuid = self._mc.metador._links.fresh_uuid()\n        obj_path = f\"{self._base_dir}/{_ep_name_for(schema_ref)}={str(obj_uuid)}\"\n        # store object\n        self._mc.__wrapped__[obj_path] = bytes(obj)\n        obj_node = self._mc.__wrapped__[obj_path]\n        assert isinstance(obj_node, H5DatasetLike)\n        stored_obj = StoredMetadata(uuid=obj_uuid, schema=schema_ref, node=obj_node)\n        self._objs[schema_ref] = stored_obj\n        # update TOC\n        self._mc.metador._links.register(stored_obj)\n        return\n\n    def _del_raw(self, schema_name: str, *, _unlink: bool = True) -&gt; None:\n\"\"\"Delete stored metadata for given schema at this node.\"\"\"\n        # NOTE: _unlink is only for the destroy method\n        stored_obj = self._objs[schema_name]\n        # unregister in TOC (will also trigger clean up there)\n        if _unlink:\n            self._mc.metador._links.unregister(stored_obj.uuid)\n        # remove metadata object\n        del self._objs[stored_obj.schema.name]\n        del self._mc.__wrapped__[stored_obj.node.name]\n        # no metadata objects left -&gt; remove metadata dir\n        if not self._objs:\n            del self._mc.__wrapped__[self._base_dir]\n        return\n\n    # helpers for container-level opertions (move, copy, delete etc)\n\n    def _destroy(self, *, _unlink: bool = True):\n\"\"\"Unregister and delete all metadata objects attached to this node.\"\"\"\n        # NOTE: _unlink is only set to false for node copy without metadata\n        for schema_name in list(self.keys()):\n            self._del_raw(schema_name, _unlink=_unlink)\n\n    # ----\n\n    def __init__(self, node: MetadorNode):\n        self._mc: MetadorContainer = node._self_container\n\"\"\"Underlying container (for convenience).\"\"\"\n\n        self._node: MetadorNode = node\n\"\"\"Underlying actual user node.\"\"\"\n\n        is_dataset = isinstance(node, H5DatasetLike)\n        self._base_dir: str = M.to_meta_base_path(node.name, is_dataset)\n\"\"\"Path of this metador metadata group node.\n\n        Actual node exists iff any metadata is stored for the node.\n        \"\"\"\n\n        self._objs: Dict[str, StoredMetadata] = {}\n\"\"\"Information about available metadata objects.\"\"\"\n\n        # load available object metadata encoded in the node names\n        meta_grp = cast(H5GroupLike, self._mc.__wrapped__.get(self._base_dir, {}))\n        for obj_node in meta_grp.values():\n            assert isinstance(obj_node, H5DatasetLike)\n            obj = StoredMetadata.from_node(obj_node)\n            self._objs[obj.schema.name] = obj\n\n    # ----\n\n    def keys(self) -&gt; KeysView[str]:\n\"\"\"Return names of explicitly attached metadata objects.\n\n        Transitive parent schemas are not included.\n        \"\"\"\n        return self._objs.keys()\n\n    def values(self) -&gt; ValuesView[StoredMetadata]:\n        self._node._guard_acl(NodeAcl.skel_only)\n        return self._objs.values()\n\n    def items(self) -&gt; ItemsView[str, StoredMetadata]:\n        self._node._guard_acl(NodeAcl.skel_only)\n        return self._objs.items()\n\n    # ----\n\n    def __len__(self) -&gt; int:\n\"\"\"Return number of explicitly attached metadata objects.\n\n        Transitive parent schemas are not counted.\n        \"\"\"\n        return len(self.keys())\n\n    def __iter__(self) -&gt; Iterator[str]:\n\"\"\"Iterate listing schema names of all actually attached metadata objects.\n\n        Transitive parent schemas are not included.\n        \"\"\"\n        return iter(self.keys())\n\n    # ----\n\n    def query(\n        self,\n        schema: Union[\n            str, Tuple[str, Optional[SemVerTuple]], PluginRef, Type[MetadataSchema]\n        ] = \"\",\n        version: Optional[SemVerTuple] = None,\n    ) -&gt; Iterator[PluginRef]:\n\"\"\"Return schema names for which objects at this node are compatible with passed schema.\n\n        Will also consider compatible child schema instances.\n\n        Returned iterator will yield passed schema first, if an object is available.\n        Apart from this, the order is not specified.\n        \"\"\"\n        schema_name, schema_ver = plugin_args(schema, version)\n        # no schema selected -&gt; list everything\n        if not schema_name:\n            for obj in self.values():\n                yield obj.schema\n            return\n\n        # try exact schema (in any compatible version, if version specified)\n        if obj := self._get_raw(schema_name, schema_ver):\n            yield obj.schema\n\n        # next, try compatible child schemas of compatible versions of requested schema\n        compat = set().union(\n            *(\n                self._mc.metador.schemas.children(ref)\n                for ref in self._mc.metador.schemas.versions(schema_name, schema_ver)\n            )\n        )\n        avail = {self._get_raw(s).schema for s in self.keys()}\n        for s_ref in avail.intersection(compat):\n            yield s_ref\n\n    def __contains__(\n        self,\n        schema: Union[\n            str, Tuple[str, Optional[SemVerTuple]], PluginRef, Type[MetadataSchema]\n        ],\n    ) -&gt; bool:\n\"\"\"Check whether a compatible metadata object for given schema exists.\n\n        Will also consider compatible child schema instances.\n        \"\"\"\n        if schema == \"\" or isinstance(schema, tuple) and schema[0] == \"\":\n            return False  # empty query lists everything, here the logic is inverted!\n        return next(self.query(schema), None) is not None\n\n    @overload\n    def __getitem__(self, schema: str) -&gt; MetadataSchema:\n        ...\n\n    @overload\n    def __getitem__(self, schema: Type[S]) -&gt; S:\n        ...\n\n    def __getitem__(self, schema: Union[str, Type[S]]) -&gt; Union[S, MetadataSchema]:\n\"\"\"Like get, but will raise KeyError on failure.\"\"\"\n        if ret := self.get(schema):\n            return ret\n        raise KeyError(schema)\n\n    @overload\n    def get(\n        self, schema: str, version: Optional[SemVerTuple] = None\n    ) -&gt; Optional[MetadataSchema]:\n        ...\n\n    @overload\n    def get(\n        self, schema: Type[S], version: Optional[SemVerTuple] = None\n    ) -&gt; Optional[S]:\n        ...\n\n    def get(\n        self, schema: Union[str, Type[S]], version: Optional[SemVerTuple] = None\n    ) -&gt; Optional[Union[MetadataSchema, S]]:\n\"\"\"Get a parsed metadata object matching the given schema (if it exists).\n\n        Will also consider compatible child schema instances.\n        \"\"\"\n        self._node._guard_acl(NodeAcl.skel_only)\n\n        # normalize arguments\n        schema_name, schema_ver = plugin_args(schema, version)\n\n        # get a compatible schema instance that is available at this node\n        compat_schema = next(self.query(schema_name, schema_ver), None)\n        if not compat_schema:\n            return None  # not found\n\n        # get class of schema and parse object\n        schema_class = self._require_schema(schema_name, schema_ver)\n        if obj := self._get_raw(compat_schema.name, compat_schema.version):\n            return cast(S, self._parse_obj(schema_class, obj.node[()]))\n        return None\n\n    def __setitem__(\n        self, schema: Union[str, Type[S]], value: Union[Dict[str, Any], MetadataSchema]\n    ) -&gt; None:\n\"\"\"Store metadata object as instance of given schema.\n\n        Raises KeyError if passed schema is not installed in environment.\n\n        Raises TypeError if passed schema is marked auxiliary.\n\n        Raises ValueError if an object for the schema already exists.\n\n        Raises ValidationError if passed object is not valid for the schema.\n        \"\"\"\n        self._node._guard_acl(NodeAcl.read_only)\n        schema_name, schema_ver = plugin_args(schema)\n\n        # if self.get(schema_name, schema_ver):  # &lt;- also subclass schemas\n        # NOTE: for practical reasons let's be more lenient here and allow redundancy\n        # hence only check if exact schema (modulo version) is already there\n        if self._get_raw(schema_name):  # &lt;- only same schema\n            msg = f\"Metadata object for schema {schema_name} already exists!\"\n            raise ValueError(msg)\n\n        schema_class = self._require_schema(schema_name, schema_ver)\n        checked_obj = self._parse_obj(schema_class, value)\n        self._set_raw(schema_class.Plugin.ref(), checked_obj)\n\n    def __delitem__(self, schema: Union[str, Type[MetadataSchema]]) -&gt; None:\n\"\"\"Delete metadata object explicitly stored for the passed schema.\n\n        If a schema class is passed, its version is ignored,\n        as each node may contain at most one explicit instance per schema.\n\n        Raises KeyError if no metadata object for that schema exists.\n        \"\"\"\n        self._node._guard_acl(NodeAcl.read_only)\n        schema_name, _ = plugin_args(schema)\n\n        if self._get_raw(schema_name) is None:\n            raise KeyError(schema_name)  # no (explicit) metadata object\n\n        self._del_raw(schema_name)\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorMeta.keys","title":"keys","text":"<pre><code>keys() -&gt; KeysView[str]\n</code></pre> <p>Return names of explicitly attached metadata objects.</p> <p>Transitive parent schemas are not included.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def keys(self) -&gt; KeysView[str]:\n\"\"\"Return names of explicitly attached metadata objects.\n\n    Transitive parent schemas are not included.\n    \"\"\"\n    return self._objs.keys()\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorMeta.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Return number of explicitly attached metadata objects.</p> <p>Transitive parent schemas are not counted.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def __len__(self) -&gt; int:\n\"\"\"Return number of explicitly attached metadata objects.\n\n    Transitive parent schemas are not counted.\n    \"\"\"\n    return len(self.keys())\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorMeta.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[str]\n</code></pre> <p>Iterate listing schema names of all actually attached metadata objects.</p> <p>Transitive parent schemas are not included.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def __iter__(self) -&gt; Iterator[str]:\n\"\"\"Iterate listing schema names of all actually attached metadata objects.\n\n    Transitive parent schemas are not included.\n    \"\"\"\n    return iter(self.keys())\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorMeta.query","title":"query","text":"<pre><code>query(\n    schema: Union[\n        str,\n        Tuple[str, Optional[SemVerTuple]],\n        PluginRef,\n        Type[MetadataSchema],\n    ] = \"\",\n    version: Optional[SemVerTuple] = None,\n) -&gt; Iterator[PluginRef]\n</code></pre> <p>Return schema names for which objects at this node are compatible with passed schema.</p> <p>Will also consider compatible child schema instances.</p> <p>Returned iterator will yield passed schema first, if an object is available. Apart from this, the order is not specified.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def query(\n    self,\n    schema: Union[\n        str, Tuple[str, Optional[SemVerTuple]], PluginRef, Type[MetadataSchema]\n    ] = \"\",\n    version: Optional[SemVerTuple] = None,\n) -&gt; Iterator[PluginRef]:\n\"\"\"Return schema names for which objects at this node are compatible with passed schema.\n\n    Will also consider compatible child schema instances.\n\n    Returned iterator will yield passed schema first, if an object is available.\n    Apart from this, the order is not specified.\n    \"\"\"\n    schema_name, schema_ver = plugin_args(schema, version)\n    # no schema selected -&gt; list everything\n    if not schema_name:\n        for obj in self.values():\n            yield obj.schema\n        return\n\n    # try exact schema (in any compatible version, if version specified)\n    if obj := self._get_raw(schema_name, schema_ver):\n        yield obj.schema\n\n    # next, try compatible child schemas of compatible versions of requested schema\n    compat = set().union(\n        *(\n            self._mc.metador.schemas.children(ref)\n            for ref in self._mc.metador.schemas.versions(schema_name, schema_ver)\n        )\n    )\n    avail = {self._get_raw(s).schema for s in self.keys()}\n    for s_ref in avail.intersection(compat):\n        yield s_ref\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorMeta.__contains__","title":"__contains__","text":"<pre><code>__contains__(\n    schema: Union[\n        str,\n        Tuple[str, Optional[SemVerTuple]],\n        PluginRef,\n        Type[MetadataSchema],\n    ]\n) -&gt; bool\n</code></pre> <p>Check whether a compatible metadata object for given schema exists.</p> <p>Will also consider compatible child schema instances.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def __contains__(\n    self,\n    schema: Union[\n        str, Tuple[str, Optional[SemVerTuple]], PluginRef, Type[MetadataSchema]\n    ],\n) -&gt; bool:\n\"\"\"Check whether a compatible metadata object for given schema exists.\n\n    Will also consider compatible child schema instances.\n    \"\"\"\n    if schema == \"\" or isinstance(schema, tuple) and schema[0] == \"\":\n        return False  # empty query lists everything, here the logic is inverted!\n    return next(self.query(schema), None) is not None\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorMeta.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(\n    schema: Union[str, Type[S]]\n) -&gt; Union[S, MetadataSchema]\n</code></pre> <p>Like get, but will raise KeyError on failure.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def __getitem__(self, schema: Union[str, Type[S]]) -&gt; Union[S, MetadataSchema]:\n\"\"\"Like get, but will raise KeyError on failure.\"\"\"\n    if ret := self.get(schema):\n        return ret\n    raise KeyError(schema)\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorMeta.get","title":"get","text":"<pre><code>get(\n    schema: Union[str, Type[S]],\n    version: Optional[SemVerTuple] = None,\n) -&gt; Optional[Union[MetadataSchema, S]]\n</code></pre> <p>Get a parsed metadata object matching the given schema (if it exists).</p> <p>Will also consider compatible child schema instances.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def get(\n    self, schema: Union[str, Type[S]], version: Optional[SemVerTuple] = None\n) -&gt; Optional[Union[MetadataSchema, S]]:\n\"\"\"Get a parsed metadata object matching the given schema (if it exists).\n\n    Will also consider compatible child schema instances.\n    \"\"\"\n    self._node._guard_acl(NodeAcl.skel_only)\n\n    # normalize arguments\n    schema_name, schema_ver = plugin_args(schema, version)\n\n    # get a compatible schema instance that is available at this node\n    compat_schema = next(self.query(schema_name, schema_ver), None)\n    if not compat_schema:\n        return None  # not found\n\n    # get class of schema and parse object\n    schema_class = self._require_schema(schema_name, schema_ver)\n    if obj := self._get_raw(compat_schema.name, compat_schema.version):\n        return cast(S, self._parse_obj(schema_class, obj.node[()]))\n    return None\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorMeta.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(\n    schema: Union[str, Type[S]],\n    value: Union[Dict[str, Any], MetadataSchema],\n) -&gt; None\n</code></pre> <p>Store metadata object as instance of given schema.</p> <p>Raises KeyError if passed schema is not installed in environment.</p> <p>Raises TypeError if passed schema is marked auxiliary.</p> <p>Raises ValueError if an object for the schema already exists.</p> <p>Raises ValidationError if passed object is not valid for the schema.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def __setitem__(\n    self, schema: Union[str, Type[S]], value: Union[Dict[str, Any], MetadataSchema]\n) -&gt; None:\n\"\"\"Store metadata object as instance of given schema.\n\n    Raises KeyError if passed schema is not installed in environment.\n\n    Raises TypeError if passed schema is marked auxiliary.\n\n    Raises ValueError if an object for the schema already exists.\n\n    Raises ValidationError if passed object is not valid for the schema.\n    \"\"\"\n    self._node._guard_acl(NodeAcl.read_only)\n    schema_name, schema_ver = plugin_args(schema)\n\n    # if self.get(schema_name, schema_ver):  # &lt;- also subclass schemas\n    # NOTE: for practical reasons let's be more lenient here and allow redundancy\n    # hence only check if exact schema (modulo version) is already there\n    if self._get_raw(schema_name):  # &lt;- only same schema\n        msg = f\"Metadata object for schema {schema_name} already exists!\"\n        raise ValueError(msg)\n\n    schema_class = self._require_schema(schema_name, schema_ver)\n    checked_obj = self._parse_obj(schema_class, value)\n    self._set_raw(schema_class.Plugin.ref(), checked_obj)\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorMeta.__delitem__","title":"__delitem__","text":"<pre><code>__delitem__(\n    schema: Union[str, Type[MetadataSchema]]\n) -&gt; None\n</code></pre> <p>Delete metadata object explicitly stored for the passed schema.</p> <p>If a schema class is passed, its version is ignored, as each node may contain at most one explicit instance per schema.</p> <p>Raises KeyError if no metadata object for that schema exists.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def __delitem__(self, schema: Union[str, Type[MetadataSchema]]) -&gt; None:\n\"\"\"Delete metadata object explicitly stored for the passed schema.\n\n    If a schema class is passed, its version is ignored,\n    as each node may contain at most one explicit instance per schema.\n\n    Raises KeyError if no metadata object for that schema exists.\n    \"\"\"\n    self._node._guard_acl(NodeAcl.read_only)\n    schema_name, _ = plugin_args(schema)\n\n    if self._get_raw(schema_name) is None:\n        raise KeyError(schema_name)  # no (explicit) metadata object\n\n    self._del_raw(schema_name)\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCLinks","title":"TOCLinks","text":"<p>Link management for synchronizing metadata objects and container TOC.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>class TOCLinks:\n\"\"\"Link management for synchronizing metadata objects and container TOC.\"\"\"\n\n    # NOTE: This is not exposed to the end-user\n\n    @staticmethod\n    def _link_path_for(schema_ref: PluginRef) -&gt; str:\n        return f\"{M.METADOR_LINKS_PATH}/{_ep_name_for(schema_ref)}\"\n\n    def __init__(self, raw_cont: H5FileLike, toc_schemas: TOCSchemas):\n        self._raw: H5FileLike = raw_cont\n\"\"\"Raw underlying container (for quick access).\"\"\"\n\n        self._toc_schemas = toc_schemas\n\"\"\"Schemas used in container (to (un)register).\"\"\"\n\n        self._toc_path: Dict[UUID, str] = {}\n\"\"\"Maps metadata object UUIDs to paths of respective pseudo-symlink in TOC.\"\"\"\n\n        # load links into memory\n        if M.METADOR_LINKS_PATH in self._raw:\n            link_grp = self._raw.require_group(M.METADOR_LINKS_PATH)\n            assert isinstance(link_grp, H5GroupLike)\n            for schema_link_grp in link_grp.values():\n                assert isinstance(schema_link_grp, H5GroupLike)\n                for uuid, link_node in schema_link_grp.items():\n                    assert isinstance(link_node, H5DatasetLike)\n                    self._toc_path[UUID(uuid)] = link_node.name\n\n    def fresh_uuid(self) -&gt; UUID:\n\"\"\"Return a UUID string not used for a metadata object in the container yet.\"\"\"\n        fresh = False\n        ret: UUID\n        # NOTE: here a very unlikely race condition is present if parallelized\n        while not fresh:\n            ret = uuid1()\n            fresh = ret not in self._toc_path\n        self._toc_path[ret] = None  # not assigned yet, but \"reserved\"\n        # ----\n        return ret\n\n    def resolve(self, uuid: UUID) -&gt; str:\n\"\"\"Get the path a UUID in the TOC points to.\"\"\"\n        link_path = self._toc_path[uuid]\n        link_node = cast(H5DatasetLike, self._raw[link_path])\n        return link_node[()].decode(\"utf-8\")\n\n    def update(self, uuid: UUID, new_target: str):\n\"\"\"Update target of an existing link to point to a new location.\"\"\"\n        link_path = self._toc_path[uuid]\n        del self._raw[link_path]\n        self._raw[link_path] = new_target\n\n    def register(self, obj: StoredMetadata) -&gt; None:\n\"\"\"Create a link for a metadata object in container TOC.\n\n        The link points to the metadata object.\n        \"\"\"\n        self._toc_schemas._register(obj.schema)\n\n        toc_path = f\"{self._link_path_for(obj.schema)}/{obj.uuid}\"\n        self._toc_path[obj.uuid] = toc_path\n        self._raw[toc_path] = str(obj.node.name)\n\n    def unregister(self, uuid: UUID) -&gt; None:\n\"\"\"Unregister metadata object in TOC given its UUID.\n\n        Will remove the object and clean up empty directories in the TOC.\n        \"\"\"\n        # delete the link itself and free the UUID\n        toc_path = self._toc_path[uuid]\n\n        schema_group = self._raw[toc_path].parent\n        assert isinstance(schema_group, H5GroupLike)\n        link_group = schema_group.parent\n        assert link_group.name == M.METADOR_LINKS_PATH\n\n        del self._raw[toc_path]\n        del self._toc_path[uuid]\n        if len(schema_group):\n            return  # schema still has instances\n\n        s_name_vers: str = schema_group.name.split(\"/\")[-1]\n        # delete empty group for schema\n        del self._raw[schema_group.name]\n        # notify schema manager (cleans up schema + package info)\n        self._toc_schemas._unregister(_schema_ref_for(s_name_vers))\n\n        if len(link_group.keys()):\n            return  # container still has metadata\n        else:\n            # remove the link dir itself (no known metadata in container left)\n            del self._raw[link_group.name]\n\n    # ----\n\n    def find_broken(self, repair: bool = False) -&gt; List[UUID]:\n\"\"\"Return list of UUIDs in TOC not pointing to an existing metadata object.\n\n        Will use loaded cache of UUIDs and check them, without scanning the container.\n\n        If repair is set, will remove those broken links.\n        \"\"\"\n        broken = []\n        for uuid in self._toc_path.keys():\n            target = self.resolve(uuid)\n            if target not in self._raw:\n                broken.append(uuid)\n        if repair:\n            for uuid in broken:\n                self.unregister(uuid)\n        return broken\n\n    def find_missing(self, path: H5GroupLike) -&gt; List[H5DatasetLike]:\n\"\"\"Return list of metadata objects not listed in TOC.\"\"\"\n        missing = []\n\n        def collect_missing(_, node):\n            if not M.is_internal_path(node.name, M.METADOR_META_PREF):\n                return  # not a metador metadata path\n            if M.is_meta_base_path(node.name):\n                # top dir, not a \"link dataset\",\n                # e.g. /.../foo/metador_meta_ or /.../metador_meta_foo\n                return\n\n            # now we assume we have a path to a metadata link object in the group\n            obj = StoredMetadata.from_node(node)\n            known = obj.uuid in self._toc_path\n            # check UUID collision: i.e., used in TOC, but points elsewhere\n            # (requires fixing up the name of this object / new UUID)\n            # implies that THIS object IS missing in the TOC\n            collision = known and self.resolve(obj.uuid) != node.name\n            if not known or collision:\n                missing.append(node)\n\n        # ensure its a group and collect\n        self._raw.require_group(path.name).visititems(collect_missing)\n        return missing\n\n    def repair_missing(\n        self, missing: List[H5DatasetLike], update: bool = False\n    ) -&gt; None:\n\"\"\"Repair links (objects get new UUIDs, unless update is true).\"\"\"\n        # NOTE: needed for correct copy and move of nodes with their metadata\n        for node in missing:\n            obj = StoredMetadata.from_node(node)\n            if update and obj.uuid in self._toc_path:\n                # update target of existing link (e.g. for move)\n                self.update(obj.uuid, node.name)\n            else:\n                # assign new UUID (e.g. for copy)\n                # (copied metadata node refers to some other uuid in the name)\n                obj.uuid = self.fresh_uuid()\n                new_path = obj.to_path()\n                # rename the metadata node to point to the new UUID\n                self._raw.move(node.name, new_path)\n                obj.node = cast(H5DatasetLike, self._raw[new_path])\n                # register the object with the new UUID in the TOC\n                self.register(obj)\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCLinks.fresh_uuid","title":"fresh_uuid","text":"<pre><code>fresh_uuid() -&gt; UUID\n</code></pre> <p>Return a UUID string not used for a metadata object in the container yet.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def fresh_uuid(self) -&gt; UUID:\n\"\"\"Return a UUID string not used for a metadata object in the container yet.\"\"\"\n    fresh = False\n    ret: UUID\n    # NOTE: here a very unlikely race condition is present if parallelized\n    while not fresh:\n        ret = uuid1()\n        fresh = ret not in self._toc_path\n    self._toc_path[ret] = None  # not assigned yet, but \"reserved\"\n    # ----\n    return ret\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCLinks.resolve","title":"resolve","text":"<pre><code>resolve(uuid: UUID) -&gt; str\n</code></pre> <p>Get the path a UUID in the TOC points to.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def resolve(self, uuid: UUID) -&gt; str:\n\"\"\"Get the path a UUID in the TOC points to.\"\"\"\n    link_path = self._toc_path[uuid]\n    link_node = cast(H5DatasetLike, self._raw[link_path])\n    return link_node[()].decode(\"utf-8\")\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCLinks.update","title":"update","text":"<pre><code>update(uuid: UUID, new_target: str)\n</code></pre> <p>Update target of an existing link to point to a new location.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def update(self, uuid: UUID, new_target: str):\n\"\"\"Update target of an existing link to point to a new location.\"\"\"\n    link_path = self._toc_path[uuid]\n    del self._raw[link_path]\n    self._raw[link_path] = new_target\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCLinks.register","title":"register","text":"<pre><code>register(obj: StoredMetadata) -&gt; None\n</code></pre> <p>Create a link for a metadata object in container TOC.</p> <p>The link points to the metadata object.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def register(self, obj: StoredMetadata) -&gt; None:\n\"\"\"Create a link for a metadata object in container TOC.\n\n    The link points to the metadata object.\n    \"\"\"\n    self._toc_schemas._register(obj.schema)\n\n    toc_path = f\"{self._link_path_for(obj.schema)}/{obj.uuid}\"\n    self._toc_path[obj.uuid] = toc_path\n    self._raw[toc_path] = str(obj.node.name)\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCLinks.unregister","title":"unregister","text":"<pre><code>unregister(uuid: UUID) -&gt; None\n</code></pre> <p>Unregister metadata object in TOC given its UUID.</p> <p>Will remove the object and clean up empty directories in the TOC.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def unregister(self, uuid: UUID) -&gt; None:\n\"\"\"Unregister metadata object in TOC given its UUID.\n\n    Will remove the object and clean up empty directories in the TOC.\n    \"\"\"\n    # delete the link itself and free the UUID\n    toc_path = self._toc_path[uuid]\n\n    schema_group = self._raw[toc_path].parent\n    assert isinstance(schema_group, H5GroupLike)\n    link_group = schema_group.parent\n    assert link_group.name == M.METADOR_LINKS_PATH\n\n    del self._raw[toc_path]\n    del self._toc_path[uuid]\n    if len(schema_group):\n        return  # schema still has instances\n\n    s_name_vers: str = schema_group.name.split(\"/\")[-1]\n    # delete empty group for schema\n    del self._raw[schema_group.name]\n    # notify schema manager (cleans up schema + package info)\n    self._toc_schemas._unregister(_schema_ref_for(s_name_vers))\n\n    if len(link_group.keys()):\n        return  # container still has metadata\n    else:\n        # remove the link dir itself (no known metadata in container left)\n        del self._raw[link_group.name]\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCLinks.find_broken","title":"find_broken","text":"<pre><code>find_broken(repair: bool = False) -&gt; List[UUID]\n</code></pre> <p>Return list of UUIDs in TOC not pointing to an existing metadata object.</p> <p>Will use loaded cache of UUIDs and check them, without scanning the container.</p> <p>If repair is set, will remove those broken links.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def find_broken(self, repair: bool = False) -&gt; List[UUID]:\n\"\"\"Return list of UUIDs in TOC not pointing to an existing metadata object.\n\n    Will use loaded cache of UUIDs and check them, without scanning the container.\n\n    If repair is set, will remove those broken links.\n    \"\"\"\n    broken = []\n    for uuid in self._toc_path.keys():\n        target = self.resolve(uuid)\n        if target not in self._raw:\n            broken.append(uuid)\n    if repair:\n        for uuid in broken:\n            self.unregister(uuid)\n    return broken\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCLinks.find_missing","title":"find_missing","text":"<pre><code>find_missing(path: H5GroupLike) -&gt; List[H5DatasetLike]\n</code></pre> <p>Return list of metadata objects not listed in TOC.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def find_missing(self, path: H5GroupLike) -&gt; List[H5DatasetLike]:\n\"\"\"Return list of metadata objects not listed in TOC.\"\"\"\n    missing = []\n\n    def collect_missing(_, node):\n        if not M.is_internal_path(node.name, M.METADOR_META_PREF):\n            return  # not a metador metadata path\n        if M.is_meta_base_path(node.name):\n            # top dir, not a \"link dataset\",\n            # e.g. /.../foo/metador_meta_ or /.../metador_meta_foo\n            return\n\n        # now we assume we have a path to a metadata link object in the group\n        obj = StoredMetadata.from_node(node)\n        known = obj.uuid in self._toc_path\n        # check UUID collision: i.e., used in TOC, but points elsewhere\n        # (requires fixing up the name of this object / new UUID)\n        # implies that THIS object IS missing in the TOC\n        collision = known and self.resolve(obj.uuid) != node.name\n        if not known or collision:\n            missing.append(node)\n\n    # ensure its a group and collect\n    self._raw.require_group(path.name).visititems(collect_missing)\n    return missing\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCLinks.repair_missing","title":"repair_missing","text":"<pre><code>repair_missing(\n    missing: List[H5DatasetLike], update: bool = False\n) -&gt; None\n</code></pre> <p>Repair links (objects get new UUIDs, unless update is true).</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def repair_missing(\n    self, missing: List[H5DatasetLike], update: bool = False\n) -&gt; None:\n\"\"\"Repair links (objects get new UUIDs, unless update is true).\"\"\"\n    # NOTE: needed for correct copy and move of nodes with their metadata\n    for node in missing:\n        obj = StoredMetadata.from_node(node)\n        if update and obj.uuid in self._toc_path:\n            # update target of existing link (e.g. for move)\n            self.update(obj.uuid, node.name)\n        else:\n            # assign new UUID (e.g. for copy)\n            # (copied metadata node refers to some other uuid in the name)\n            obj.uuid = self.fresh_uuid()\n            new_path = obj.to_path()\n            # rename the metadata node to point to the new UUID\n            self._raw.move(node.name, new_path)\n            obj.node = cast(H5DatasetLike, self._raw[new_path])\n            # register the object with the new UUID in the TOC\n            self.register(obj)\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCSchemas","title":"TOCSchemas","text":"<p>Schema management for schemas used in the container.</p> <p>Interface is made to mimic PGSchema wherever it makes sense.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>class TOCSchemas:\n\"\"\"Schema management for schemas used in the container.\n\n    Interface is made to mimic PGSchema wherever it makes sense.\n    \"\"\"\n\n    @classmethod\n    def _schema_path_for(cls, s_ref: PluginRef) -&gt; str:\n        return f\"{M.METADOR_SCHEMAS_PATH}/{to_ep_name(s_ref.name, s_ref.version)}\"\n\n    @classmethod\n    def _jsonschema_path_for(cls, s_ref: PluginRef) -&gt; str:\n        return f\"{cls._schema_path_for(s_ref)}/jsonschema.json\"\n\n    @staticmethod\n    def _load_json(node: H5DatasetLike):\n        return json.loads(node[()].decode(\"utf-8\"))\n\n    def _update_parents_children(\n        self, schema_ref: PluginRef, parents: Optional[List[PluginRef]]\n    ):\n        if parents is None:  # remove schema\n            for parent in self._parents[schema_ref]:\n                if parent in self._schemas:\n                    self._children[parent].remove(schema_ref)\n                elif all(\n                    (child not in self._schemas for child in self._children[parent])\n                ):\n                    del self._parents[parent]\n                    del self._children[parent]\n        else:  # add schema\n            for i, parent in enumerate(parents):\n                if parent not in self._parents:\n                    self._parents[parent] = parents[: i + 1]\n                if parent not in self._children:\n                    self._children[parent] = set()\n                if parent != schema_ref:\n                    self._children[parent].add(schema_ref)\n\n    def _register(self, schema_ref: PluginRef):\n\"\"\"Notify that a schema is used in the container (metadata object is created/updated).\n\n        If the schema has not been used before in the container, will store metadata about it.\n        \"\"\"\n        if schema_ref in self._schemas:\n            return  # nothing to do\n\n        # store json schema\n        schema_cls = schemas.get(schema_ref.name, schema_ref.version)\n        jsonschema_dat = schema_cls.schema_json().encode(\"utf-8\")\n        jsonschema_path = self._jsonschema_path_for(schema_ref)\n        self._raw[jsonschema_path] = jsonschema_dat\n\n        # store parent schema refs\n        compat_path = f\"{self._schema_path_for(schema_ref)}/compat\"\n        parents = schemas.parent_path(schema_ref.name, schema_ref.version)\n        parents_dat: bytes = json.dumps(list(map(lambda x: x.dict(), parents))).encode(\n            \"utf-8\"\n        )\n\n        self._raw[compat_path] = parents_dat\n        self._schemas.add(schema_ref)\n        self._update_parents_children(schema_ref, parents)\n\n        # add providing package (if no stored package provides it)\n        if not self._pkgs._providers.get(schema_ref, []):\n            env_pkg_info: PluginPkgMeta = schemas.provider(schema_cls.Plugin.ref())\n            pkg_name_ver = (str(env_pkg_info.name), env_pkg_info.version)\n            self._pkgs._register(pkg_name_ver, env_pkg_info)\n            self._used[pkg_name_ver] = set()\n\n        # update used schemas tracker for all packages providing this schema\n        for pkg in self._pkgs._providers[schema_ref]:\n            self._used[pkg].add(schema_ref)\n\n    def _unregister(self, schema_ref: PluginRef):\n\"\"\"Notify that a schema is not used at any container node anymore.\n\n        If after that no schema of a listed dep package is used,\n        this dependency will be removed from the container.\n        \"\"\"\n        del self._raw[self._schema_path_for(schema_ref)]\n        self._schemas.remove(schema_ref)\n        self._update_parents_children(schema_ref, None)\n\n        providers = set(self._pkgs._providers[schema_ref])\n        for pkg in providers:\n            pkg_used = self._used[pkg]\n            if schema_ref in pkg_used:\n                # remove schema from list of used schemas of pkg\n                pkg_used.remove(schema_ref)\n            if not len(pkg_used):\n                # package not used anymore in container -&gt; clean up\n                self._pkgs._unregister(pkg)\n\n        # remove schemas group if it is empty (no schemas used in container)\n        if not self._raw.require_group(M.METADOR_SCHEMAS_PATH).keys():\n            del self._raw[M.METADOR_SCHEMAS_PATH]\n\n    def __init__(self, raw_cont: H5FileLike, toc_packages: TOCPackages):\n        self._raw: H5FileLike = raw_cont\n\"\"\"Raw underlying container (for quick access).\"\"\"\n\n        self._pkgs = toc_packages\n\"\"\"TOC package metadata manager object.\"\"\"\n\n        self._schemas: Set[PluginRef] = set()\n\"\"\"Stored JSON Schemas of actually used schemas.\"\"\"\n\n        self._parents: Dict[PluginRef, List[PluginRef]] = {}\n\"\"\"Parents of a used json schema (i.e. other partially compatible schemas).\"\"\"\n\n        self._children: Dict[PluginRef, Set[PluginRef]] = {}\n\"\"\"Children of a used json schema (i.e. other fully compatible schemas).\"\"\"\n\n        self._used: Dict[PythonDep, Set[PluginRef]] = {}\n\"\"\"package name + version -&gt; name of schemas used in container\"\"\"\n\n        for pkg in self._pkgs.keys():\n            self._used[pkg] = set()\n\n        if M.METADOR_SCHEMAS_PATH in self._raw:\n            schema_grp = self._raw.require_group(M.METADOR_SCHEMAS_PATH)\n            for name, node in schema_grp.items():\n                s_ref: PluginRef = _schema_ref_for(name)\n                assert isinstance(node, H5GroupLike)\n                compat = node[\"compat\"]\n                assert isinstance(compat, H5DatasetLike)\n\n                reflist = json.loads(compat[()].decode(\"utf-8\"))\n                parents = list(map(PluginRef.parse_obj, reflist))\n\n                self._schemas.add(s_ref)\n                self._update_parents_children(s_ref, parents)\n                for pkg in self._pkgs._providers[s_ref]:\n                    self._used[pkg].add(s_ref)\n\n    @property\n    def packages(self) -&gt; TOCPackages:\n\"\"\"Like PluginGroup.packages, but with respect to schemas used in container.\"\"\"\n        return self._pkgs\n\n    def provider(self, schema_ref: PluginRef) -&gt; PluginPkgMeta:\n\"\"\"Like PluginGroup.provider, but with respect to container deps.\"\"\"\n        pkg_name_ver = next(iter(self._pkgs._providers.get(schema_ref, [])), None)\n        if pkg_name_ver is None:\n            msg = f\"Did not find metadata of a package providing schema: '{schema_ref}'\"\n            raise KeyError(msg)\n        return self._pkgs[pkg_name_ver]\n\n    def parent_path(\n        self, schema, version: Optional[SemVerTuple] = None\n    ) -&gt; List[PluginRef]:\n\"\"\"Like PGSchema.parent_path, but with respect to container deps.\"\"\"\n        name, vers = plugin_args(schema, version, require_version=True)\n        s_ref = schemas.PluginRef(name=name, version=vers)\n        return self._parents[s_ref]\n\n    def versions(\n        self, p_name: str, version: Optional[SemVerTuple] = None\n    ) -&gt; List[PluginRef]:\n\"\"\"Like PGSchema.versions, but with respect to container deps.\"\"\"\n        # NOTE: using _children instead of _schemas because some are only listed\n        # due to their appearance in the parent_path of some actually used schema\n        # but we need them here for \"parent compatibility\" to work right.\n        refs = list(filter(lambda s: s.name == p_name, self._children))\n\n        if version is None:\n            return refs\n        # filter plugins for compatible version\n        requested = schemas.PluginRef(name=p_name, version=version)\n        # NOTE: here \"supports\" arguments are reversed (compared to \"plugin versions\")!\n        # because its about instances (that must be \"below\" the requested schema version)\n        return [ref for ref in refs if requested.supports(ref)]\n\n    def children(self, schema, version: Optional[SemVerTuple] = None) -&gt; Set[PluginRef]:\n\"\"\"Like PGSchema.children, but with respect to container deps.\"\"\"\n        name, vers = plugin_args(schema, version)\n        if vers is not None:\n            s_refs = [schemas.PluginRef(name=name, version=vers)]\n        else:\n            # if no version is given, collect all possibilities\n            s_refs = [ref for ref in self._children.keys() if ref.name == name]\n        # return all that can be actually retrieved\n        return set().union(\n            *filter(lambda x: x is not None, map(self._children.get, s_refs))\n        )\n\n    # ----\n\n    def __len__(self):\n        return len(self._schemas)\n\n    def __iter__(self):\n        return iter(self.keys())\n\n    def __contains__(self, schema_ref: PluginRef):\n        return schema_ref in self._schemas\n\n    def __getitem__(self, schema_ref: PluginRef):\n        node_path = self._jsonschema_path_for(schema_ref)\n        assert node_path in self._raw\n        return self._load_json(cast(H5DatasetLike, self._raw[node_path]))\n\n    def get(self, schema_ref: PluginRef):\n        try:\n            self[schema_ref]\n        except KeyError:\n            return None\n\n    def keys(self):\n        return set(self._schemas)\n\n    def values(self):\n        return [self[k] for k in self.keys()]\n\n    def items(self):\n        return [(k, self[k]) for k in self.keys()]\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCSchemas.packages","title":"packages  <code>property</code>","text":"<pre><code>packages: TOCPackages\n</code></pre> <p>Like PluginGroup.packages, but with respect to schemas used in container.</p>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCSchemas.provider","title":"provider","text":"<pre><code>provider(schema_ref: PluginRef) -&gt; PluginPkgMeta\n</code></pre> <p>Like PluginGroup.provider, but with respect to container deps.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def provider(self, schema_ref: PluginRef) -&gt; PluginPkgMeta:\n\"\"\"Like PluginGroup.provider, but with respect to container deps.\"\"\"\n    pkg_name_ver = next(iter(self._pkgs._providers.get(schema_ref, [])), None)\n    if pkg_name_ver is None:\n        msg = f\"Did not find metadata of a package providing schema: '{schema_ref}'\"\n        raise KeyError(msg)\n    return self._pkgs[pkg_name_ver]\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCSchemas.parent_path","title":"parent_path","text":"<pre><code>parent_path(\n    schema, version: Optional[SemVerTuple] = None\n) -&gt; List[PluginRef]\n</code></pre> <p>Like PGSchema.parent_path, but with respect to container deps.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def parent_path(\n    self, schema, version: Optional[SemVerTuple] = None\n) -&gt; List[PluginRef]:\n\"\"\"Like PGSchema.parent_path, but with respect to container deps.\"\"\"\n    name, vers = plugin_args(schema, version, require_version=True)\n    s_ref = schemas.PluginRef(name=name, version=vers)\n    return self._parents[s_ref]\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCSchemas.versions","title":"versions","text":"<pre><code>versions(\n    p_name: str, version: Optional[SemVerTuple] = None\n) -&gt; List[PluginRef]\n</code></pre> <p>Like PGSchema.versions, but with respect to container deps.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def versions(\n    self, p_name: str, version: Optional[SemVerTuple] = None\n) -&gt; List[PluginRef]:\n\"\"\"Like PGSchema.versions, but with respect to container deps.\"\"\"\n    # NOTE: using _children instead of _schemas because some are only listed\n    # due to their appearance in the parent_path of some actually used schema\n    # but we need them here for \"parent compatibility\" to work right.\n    refs = list(filter(lambda s: s.name == p_name, self._children))\n\n    if version is None:\n        return refs\n    # filter plugins for compatible version\n    requested = schemas.PluginRef(name=p_name, version=version)\n    # NOTE: here \"supports\" arguments are reversed (compared to \"plugin versions\")!\n    # because its about instances (that must be \"below\" the requested schema version)\n    return [ref for ref in refs if requested.supports(ref)]\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCSchemas.children","title":"children","text":"<pre><code>children(\n    schema, version: Optional[SemVerTuple] = None\n) -&gt; Set[PluginRef]\n</code></pre> <p>Like PGSchema.children, but with respect to container deps.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def children(self, schema, version: Optional[SemVerTuple] = None) -&gt; Set[PluginRef]:\n\"\"\"Like PGSchema.children, but with respect to container deps.\"\"\"\n    name, vers = plugin_args(schema, version)\n    if vers is not None:\n        s_refs = [schemas.PluginRef(name=name, version=vers)]\n    else:\n        # if no version is given, collect all possibilities\n        s_refs = [ref for ref in self._children.keys() if ref.name == name]\n    # return all that can be actually retrieved\n    return set().union(\n        *filter(lambda x: x is not None, map(self._children.get, s_refs))\n    )\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.TOCPackages","title":"TOCPackages","text":"<p>Package metadata management for schemas used in the container.</p> <p>The container will always store for each schema used in the information about one package providing that schema.</p> <p>If there are multiple providers of the same schema, the first/existing one is preferred.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>class TOCPackages:\n\"\"\"Package metadata management for schemas used in the container.\n\n    The container will always store for each schema used in the\n    information about one package providing that schema.\n\n    If there are multiple providers of the same schema,\n    the first/existing one is preferred.\n    \"\"\"\n\n    @staticmethod\n    def _pkginfo_path_for(pkg_name: str, pkg_version: SemVerTuple) -&gt; str:\n        return f\"{M.METADOR_PACKAGES_PATH}/{to_ep_name(pkg_name, pkg_version)}\"\n\n    def _add_providers(self, pkg: PythonDep, pkginfo: PluginPkgMeta):\n        # fill schema -&gt; package lookup table for provided package\n        for schema_ref in pkginfo.plugins[schemas.name]:\n            if schema_ref not in self._providers:\n                self._providers[schema_ref] = set()\n            self._providers[schema_ref].add(pkg)\n\n    def _register(self, pkg: PythonDep, info: PluginPkgMeta):\n        pkg_path = self._pkginfo_path_for(*pkg)\n        self._raw[pkg_path] = bytes(info)\n        self._pkginfos[pkg] = info\n        self._add_providers(pkg, info)\n\n    def _unregister(self, pkg: PythonDep):\n        pkg_path = self._pkginfo_path_for(*pkg)\n        del self._raw[pkg_path]\n        info = self._pkginfos.pop(pkg)\n        # unregister providers\n        for schema_ref in info.plugins[schemas.name]:\n            providers = self._providers[schema_ref]\n            providers.remove(pkg)\n            if not providers:  # schema not provided by any package\n                del self._providers[schema_ref]\n\n        # remove schemas group if it is empty (no schemas used in container)\n        if not self._raw.require_group(M.METADOR_PACKAGES_PATH).keys():\n            del self._raw[M.METADOR_PACKAGES_PATH]\n\n    def __init__(self, raw_container: H5FileLike):\n        self._raw: H5FileLike = raw_container\n\"\"\"Raw underlying container (for quick access).\"\"\"\n\n        self._pkginfos: Dict[PythonDep, PluginPkgMeta] = {}\n\"\"\"Package name + version -&gt; package info\"\"\"\n\n        self._providers: Dict[PluginRef, Set[PythonDep]] = {}\n\"\"\"schema reference -&gt; package name + version\"\"\"\n\n        # parse package infos if they exist\n        if M.METADOR_PACKAGES_PATH in self._raw:\n            deps_grp = self._raw.require_group(M.METADOR_PACKAGES_PATH)\n            for name, node in deps_grp.items():\n                pkg: PythonDep = from_ep_name(EPName(name))\n                info = PluginPkgMeta.parse_raw(cast(H5DatasetLike, node)[()])\n                self._pkginfos[pkg] = info\n                self._add_providers(pkg, info)\n\n    # ----\n\n    def __len__(self):\n        return len(self._pkginfos)\n\n    def __iter__(self):\n        return iter(self._pkginfos)\n\n    def __contains__(self, pkg: PythonDep):\n        return pkg in self._pkginfos\n\n    def __getitem__(self, pkg: PythonDep):\n        return self._pkginfos[pkg]\n\n    def keys(self):\n        return self._pkginfos.keys()\n\n    def values(self):\n        return self._pkginfos.values()\n\n    def items(self):\n        return self._pkginfos.items()\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorContainerTOC","title":"MetadorContainerTOC","text":"<p>Interface to the Metador metadata index (table of contents) of a container.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>class MetadorContainerTOC:\n\"\"\"Interface to the Metador metadata index (table of contents) of a container.\"\"\"\n\n    def __init__(self, container: MetadorContainer):\n        self._container = container\n        self._raw = self._container.__wrapped__\n\n        ver = self.spec_version if M.METADOR_VERSION_PATH in self._raw else None\n        if ver:\n            if ver &gt;= [2]:\n                msg = f\"Unsupported Metador container version: {ver}\"\n                raise ValueError(msg)\n        else:\n            if self._container.acl[NodeAcl.read_only]:\n                msg = \"Container is read-only and does not look like a Metador container! \"\n                msg += \"Please open in writable mode to initialize Metador structures!\"\n                raise ValueError(msg)\n\n            # writable + no version = fresh (for metador), initialize it\n            self._raw[M.METADOR_VERSION_PATH] = M.METADOR_SPEC_VERSION\n            self._raw[M.METADOR_UUID_PATH] = str(uuid1())\n\n        # if we're here, we have a prepared container TOC structure\n\n        # proceed to initialize TOC\n        self._driver_type: MetadorDriverEnum = get_driver_type(self._raw)\n\n        self._packages = TOCPackages(self._raw)\n        self._schemas = TOCSchemas(self._raw, self._packages)\n        self._links = TOCLinks(self._raw, self._schemas)\n\n    # ----\n\n    @property\n    def driver_type(self) -&gt; MetadorDriverEnum:\n\"\"\"Return the type of the container driver.\"\"\"\n        return self._driver_type\n\n    @property\n    def driver(self) -&gt; Type[MetadorDriver]:\n\"\"\"Return the container driver class used by the container.\"\"\"\n        return METADOR_DRIVERS[self.driver_type]\n\n    @property\n    def source(self) -&gt; Any:\n\"\"\"Return data underlying thes container (file, set of files, etc. used with the driver).\"\"\"\n        return get_source(self._raw, self.driver_type)\n\n    # ----\n\n    @property\n    def container_uuid(self) -&gt; UUID:\n\"\"\"Return UUID of the container.\"\"\"\n        uuid = self._raw[M.METADOR_UUID_PATH]\n        uuid_ds = cast(H5DatasetLike, uuid)\n        return UUID(uuid_ds[()].decode(\"utf-8\"))\n\n    @property\n    def spec_version(self) -&gt; List[int]:\n\"\"\"Return Metador container specification version of the container.\"\"\"\n        ver = cast(H5DatasetLike, self._raw[M.METADOR_VERSION_PATH])\n        return list(map(int, ver[()].decode(\"utf-8\").split(\".\")))\n\n    @property\n    def schemas(self):\n\"\"\"Information about all schemas used for metadata objects in this container.\"\"\"\n        return self._schemas\n\n    def query(\n        self,\n        schema: Union[str, Type[S]],\n        version: Optional[SemVerTuple] = None,\n        *,\n        node: Optional[MetadorNode] = None,\n    ) -&gt; Iterator[MetadorNode]:\n\"\"\"Return nodes that contain a metadata object compatible with the given schema.\"\"\"\n        schema_name, schema_ver = plugin_args(schema, version)\n        if not schema_name:  # could be e.g. empty string\n            msg = \"A schema name, plugin reference or class must be provided!\"\n            raise ValueError(msg)\n\n        start_node: MetadorNode = node or self._container[\"/\"]\n\n        # check start node metadata explicitly\n        if (schema_name, schema_ver) in start_node.meta:\n            yield start_node\n\n        if not isinstance(start_node, H5GroupLike):\n            return  # the node is not group-like, cannot be traversed down\n\n        # collect nodes below start node recursively\n        # NOTE: yielding from the collect_nodes does not work :'(\n        # so we have to actually materialize the list &gt;.&lt;\n        # but we expose only the generator interface anyway (better design)\n        # (maybe consider replacing visititems with a custom traversal here)\n        ret: List[MetadorNode] = []\n\n        def collect_nodes(_, node: MetadorNode):\n            if (schema_name, schema_ver) in node.meta:\n                ret.append(node)\n\n        start_node.visititems(collect_nodes)\n        yield from iter(ret)\n</code></pre>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorContainerTOC.driver_type","title":"driver_type  <code>property</code>","text":"<pre><code>driver_type: MetadorDriverEnum\n</code></pre> <p>Return the type of the container driver.</p>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorContainerTOC.driver","title":"driver  <code>property</code>","text":"<pre><code>driver: Type[MetadorDriver]\n</code></pre> <p>Return the container driver class used by the container.</p>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorContainerTOC.source","title":"source  <code>property</code>","text":"<pre><code>source: Any\n</code></pre> <p>Return data underlying thes container (file, set of files, etc. used with the driver).</p>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorContainerTOC.container_uuid","title":"container_uuid  <code>property</code>","text":"<pre><code>container_uuid: UUID\n</code></pre> <p>Return UUID of the container.</p>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorContainerTOC.spec_version","title":"spec_version  <code>property</code>","text":"<pre><code>spec_version: List[int]\n</code></pre> <p>Return Metador container specification version of the container.</p>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorContainerTOC.schemas","title":"schemas  <code>property</code>","text":"<pre><code>schemas\n</code></pre> <p>Information about all schemas used for metadata objects in this container.</p>"},{"location":"reference/metador_core/container/interface/#metador_core.container.interface.MetadorContainerTOC.query","title":"query","text":"<pre><code>query(\n    schema: Union[str, Type[S]],\n    version: Optional[SemVerTuple] = None,\n    *,\n    node: Optional[MetadorNode] = None\n) -&gt; Iterator[MetadorNode]\n</code></pre> <p>Return nodes that contain a metadata object compatible with the given schema.</p> Source code in <code>src/metador_core/container/interface.py</code> <pre><code>def query(\n    self,\n    schema: Union[str, Type[S]],\n    version: Optional[SemVerTuple] = None,\n    *,\n    node: Optional[MetadorNode] = None,\n) -&gt; Iterator[MetadorNode]:\n\"\"\"Return nodes that contain a metadata object compatible with the given schema.\"\"\"\n    schema_name, schema_ver = plugin_args(schema, version)\n    if not schema_name:  # could be e.g. empty string\n        msg = \"A schema name, plugin reference or class must be provided!\"\n        raise ValueError(msg)\n\n    start_node: MetadorNode = node or self._container[\"/\"]\n\n    # check start node metadata explicitly\n    if (schema_name, schema_ver) in start_node.meta:\n        yield start_node\n\n    if not isinstance(start_node, H5GroupLike):\n        return  # the node is not group-like, cannot be traversed down\n\n    # collect nodes below start node recursively\n    # NOTE: yielding from the collect_nodes does not work :'(\n    # so we have to actually materialize the list &gt;.&lt;\n    # but we expose only the generator interface anyway (better design)\n    # (maybe consider replacing visititems with a custom traversal here)\n    ret: List[MetadorNode] = []\n\n    def collect_nodes(_, node: MetadorNode):\n        if (schema_name, schema_ver) in node.meta:\n            ret.append(node)\n\n    start_node.visititems(collect_nodes)\n    yield from iter(ret)\n</code></pre>"},{"location":"reference/metador_core/container/protocols/","title":"protocols","text":"<p>Protocol roughly formalizing the overlap of h5py.File and IH5Record API.</p> <p>We build the MetadorContainer interface assuming only these methods.</p>"},{"location":"reference/metador_core/container/protocols/#metador_core.container.protocols.OpenMode","title":"OpenMode  <code>module-attribute</code>","text":"<pre><code>OpenMode = Literal['r', 'r+', 'a', 'w', 'w-', 'x']\n</code></pre> <p>User open modes that can be passed during initialization.</p>"},{"location":"reference/metador_core/container/protocols/#metador_core.container.protocols.H5NodeLike","title":"H5NodeLike","text":"<p>             Bases: <code>Protocol</code></p> <p>HDF5 Files, Groups and Datasets are all Nodes.</p> Source code in <code>src/metador_core/container/protocols.py</code> <pre><code>@runtime_checkable\nclass H5NodeLike(Protocol):  # pragma: no cover\n\"\"\"HDF5 Files, Groups and Datasets are all Nodes.\"\"\"\n\n    @property\n    def name(self) -&gt; str:\n\"\"\"Absolute path of the node.\"\"\"\n\n    @property\n    def attrs(self) -&gt; MutableMapping:\n\"\"\"Attached HDF5 attributes.\"\"\"\n\n    @property\n    def parent(self) -&gt; H5GroupLike:\n\"\"\"Parent group.\"\"\"\n\n    @property\n    def file(self) -&gt; H5FileLike:\n\"\"\"Original file-like object this node belongs to.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/container/protocols/#metador_core.container.protocols.H5NodeLike.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Absolute path of the node.</p>"},{"location":"reference/metador_core/container/protocols/#metador_core.container.protocols.H5NodeLike.attrs","title":"attrs  <code>property</code>","text":"<pre><code>attrs: MutableMapping\n</code></pre> <p>Attached HDF5 attributes.</p>"},{"location":"reference/metador_core/container/protocols/#metador_core.container.protocols.H5NodeLike.parent","title":"parent  <code>property</code>","text":"<pre><code>parent: H5GroupLike\n</code></pre> <p>Parent group.</p>"},{"location":"reference/metador_core/container/protocols/#metador_core.container.protocols.H5NodeLike.file","title":"file  <code>property</code>","text":"<pre><code>file: H5FileLike\n</code></pre> <p>Original file-like object this node belongs to.</p>"},{"location":"reference/metador_core/container/protocols/#metador_core.container.protocols.H5DatasetLike","title":"H5DatasetLike","text":"<p>             Bases: <code>H5NodeLike</code>, <code>Protocol</code></p> <p>Datasets provide numpy-style indexing into data.</p> <p>Metador containers use it for storing bytes, and for getting bytes out again using [()].</p> Source code in <code>src/metador_core/container/protocols.py</code> <pre><code>@runtime_checkable\nclass H5DatasetLike(H5NodeLike, Protocol):  # pragma: no cover\n\"\"\"Datasets provide numpy-style indexing into data.\n\n    Metador containers use it for storing bytes,\n    and for getting bytes out again using [()].\n    \"\"\"\n\n    def __getitem__(self, key: Any) -&gt; Any:\n        ...\n\n    def __setitem__(self, key: Any, value) -&gt; None:\n        ...\n\n    # needed to distinguish from other types:\n    @property\n    def ndim(self) -&gt; int:\n\"\"\"Numpy-style dimensionality.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/container/protocols/#metador_core.container.protocols.H5DatasetLike.ndim","title":"ndim  <code>property</code>","text":"<pre><code>ndim: int\n</code></pre> <p>Numpy-style dimensionality.</p>"},{"location":"reference/metador_core/container/protocols/#metador_core.container.protocols.H5FileLike","title":"H5FileLike","text":"<p>             Bases: <code>H5GroupLike</code>, <code>Protocol</code></p> <p>A HDF5 File acts like the root group and has some extra features.</p> Source code in <code>src/metador_core/container/protocols.py</code> <pre><code>@runtime_checkable\nclass H5FileLike(H5GroupLike, Protocol):  # pragma: no cover\n\"\"\"A HDF5 File acts like the root group and has some extra features.\"\"\"\n\n    @property\n    def mode(self) -&gt; Literal[\"r\", \"r+\"]:\n\"\"\"Return 'r' if container is immutable, otherwise 'r+'.\"\"\"\n\n    def close(self) -&gt; None:\n        ...\n\n    # context manager (`with` notation)\n\n    def __enter__(self) -&gt; H5FileLike:\n        ...\n\n    def __exit__(self, ex_type, ex_value, ex_traceback) -&gt; None:\n        ...\n</code></pre>"},{"location":"reference/metador_core/container/protocols/#metador_core.container.protocols.H5FileLike.mode","title":"mode  <code>property</code>","text":"<pre><code>mode: Literal['r', 'r+']\n</code></pre> <p>Return 'r' if container is immutable, otherwise 'r+'.</p>"},{"location":"reference/metador_core/container/provider/","title":"provider","text":"<p>Abstract Metador container provider interface.</p>"},{"location":"reference/metador_core/container/provider/#metador_core.container.provider.ContainerArgs","title":"ContainerArgs  <code>module-attribute</code>","text":"<pre><code>ContainerArgs = Tuple[Type[MetadorDriver], Any]\n</code></pre> <p>Pair of (driver class, suitable driver arguments).</p> <p>Must be such that <code>MetadorContainer(driver(source))</code> yields a working container.</p>"},{"location":"reference/metador_core/container/provider/#metador_core.container.provider.ContainerProxy","title":"ContainerProxy","text":"<p>             Bases: <code>Protocol[T]</code></p> <p>Abstract interface for Metador container providers.</p> <p>This interface acts like a proxy to access containers by some identifier.</p> <p>The identifier type parameter T is in the simplest case the Metador container UUID. In more complex cases, it could be a different unique identifier with a non-trivial relationship to Metador container UUIDs (many-to-many). Therefore, T is implementation-specific.</p> <p>There are many ways to store and organize containers, this interface serves as the implementation target for generic service components such as container-centric Flask blueprints, so they can be easier reused in different backends and services.</p> <p>Note that only containment and retrieval are possible - on purpose. Knowing and iterating over all containers in a system is not always possible.</p> Source code in <code>src/metador_core/container/provider.py</code> <pre><code>class ContainerProxy(Protocol[T]):\n\"\"\"Abstract interface for Metador container providers.\n\n    This interface acts like a proxy to access containers by some identifier.\n\n    The identifier type parameter T is in the simplest case the Metador\n    container UUID. In more complex cases, it could be a different unique\n    identifier with a non-trivial relationship to Metador container UUIDs\n    (many-to-many). Therefore, T is implementation-specific.\n\n    There are many ways to store and organize containers, this interface serves\n    as the implementation target for generic service components such as\n    container-centric Flask blueprints, so they can be easier reused in\n    different backends and services.\n\n    Note that only containment and retrieval are possible - on purpose.\n    Knowing and iterating over all containers in a system is not always possible.\n    \"\"\"\n\n    def __contains__(self, key: T) -&gt; bool:\n\"\"\"Return whether a resource key is known to the proxy.\"\"\"\n        # return self.get(key) is not None\n\n    def get(self, key: T) -&gt; Optional[MetadorContainer]:\n\"\"\"Get a container instance, if resource key is known to the proxy.\n\n        Implement this method in subclasses to support the minimal interface.\n        \"\"\"\n\n    def __getitem__(self, key: T) -&gt; T:\n\"\"\"Get a container instance, if resource key is known to the proxy.\n\n        Default implementation is in terms of `get`.\n        \"\"\"\n        if ret := self.get(key):\n            return ret\n        raise KeyError(key)\n</code></pre>"},{"location":"reference/metador_core/container/provider/#metador_core.container.provider.ContainerProxy.__contains__","title":"__contains__","text":"<pre><code>__contains__(key: T) -&gt; bool\n</code></pre> <p>Return whether a resource key is known to the proxy.</p> Source code in <code>src/metador_core/container/provider.py</code> <pre><code>def __contains__(self, key: T) -&gt; bool:\n\"\"\"Return whether a resource key is known to the proxy.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/container/provider/#metador_core.container.provider.ContainerProxy.get","title":"get","text":"<pre><code>get(key: T) -&gt; Optional[MetadorContainer]\n</code></pre> <p>Get a container instance, if resource key is known to the proxy.</p> <p>Implement this method in subclasses to support the minimal interface.</p> Source code in <code>src/metador_core/container/provider.py</code> <pre><code>def get(self, key: T) -&gt; Optional[MetadorContainer]:\n\"\"\"Get a container instance, if resource key is known to the proxy.\n\n    Implement this method in subclasses to support the minimal interface.\n    \"\"\"\n</code></pre>"},{"location":"reference/metador_core/container/provider/#metador_core.container.provider.ContainerProxy.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: T) -&gt; T\n</code></pre> <p>Get a container instance, if resource key is known to the proxy.</p> <p>Default implementation is in terms of <code>get</code>.</p> Source code in <code>src/metador_core/container/provider.py</code> <pre><code>def __getitem__(self, key: T) -&gt; T:\n\"\"\"Get a container instance, if resource key is known to the proxy.\n\n    Default implementation is in terms of `get`.\n    \"\"\"\n    if ret := self.get(key):\n        return ret\n    raise KeyError(key)\n</code></pre>"},{"location":"reference/metador_core/container/provider/#metador_core.container.provider.SimpleContainerProvider","title":"SimpleContainerProvider","text":"<p>             Bases: <code>Generic[T]</code>, <code>ContainerProxy[T]</code></p> <p>Dict-backed container proxy.</p> <p>It is a minimal reasonable implementation for the interface that can be used in small apps and does not depend on the container driver, thus can support all container interface implementations.</p> Source code in <code>src/metador_core/container/provider.py</code> <pre><code>class SimpleContainerProvider(Generic[T], ContainerProxy[T]):\n\"\"\"Dict-backed container proxy.\n\n    It is a minimal reasonable implementation for the interface that can be\n    used in small apps and does not depend on the container driver,\n    thus can support all container interface implementations.\n    \"\"\"\n\n    _known: Dict[T, ContainerArgs]\n\"\"\"Mapping from container identifier to MetadorContainer constructor args.\"\"\"\n\n    def __init__(self):\n        self._known = {}\n\n    def __contains__(self, key: T) -&gt; bool:\n        return key in self._known\n\n    def get(self, key: T) -&gt; Optional[MetadorContainer]:\n\"\"\"Get an open container file to access data and metadata, if it exists.\"\"\"\n        if key not in self._known:\n            return None\n        driver, source = self._known[key]\n        return MetadorContainer(driver(source))\n\n    # ----\n\n    def __delitem__(self, key: T):\n        del self._known[key]\n\n    def __setitem__(self, key: T, value: Union[ContainerArgs, MetadorContainer]):\n        # NOTE: can't do instance check here, because MetadorContainer is a wrapper itself\n        # so we check if a container is passed by presence of the .metador attribute\n        if container_toc := getattr(value, \"metador\", None):\n            self._known[key] = (container_toc.driver, container_toc.source)\n        else:\n            self._known[key] = value\n\n    def keys(self):\n        return self._known.keys()\n</code></pre>"},{"location":"reference/metador_core/container/provider/#metador_core.container.provider.SimpleContainerProvider.get","title":"get","text":"<pre><code>get(key: T) -&gt; Optional[MetadorContainer]\n</code></pre> <p>Get an open container file to access data and metadata, if it exists.</p> Source code in <code>src/metador_core/container/provider.py</code> <pre><code>def get(self, key: T) -&gt; Optional[MetadorContainer]:\n\"\"\"Get an open container file to access data and metadata, if it exists.\"\"\"\n    if key not in self._known:\n        return None\n    driver, source = self._known[key]\n    return MetadorContainer(driver(source))\n</code></pre>"},{"location":"reference/metador_core/container/utils/","title":"utils","text":"<p>Constants and helper functions.</p> <p>Provides syntactic path transformations to implement the Metador container layout.</p>"},{"location":"reference/metador_core/container/utils/#metador_core.container.utils.METADOR_SPEC_VERSION","title":"METADOR_SPEC_VERSION  <code>module-attribute</code>","text":"<pre><code>METADOR_SPEC_VERSION: Final[str] = '1.0'\n</code></pre> <p>Version of container spec created by this package.</p>"},{"location":"reference/metador_core/container/utils/#metador_core.container.utils.METADOR_PREF","title":"METADOR_PREF  <code>module-attribute</code>","text":"<pre><code>METADOR_PREF: Final[str] = 'metador_'\n</code></pre> <p>Reserved prefix for group and dataset names.</p>"},{"location":"reference/metador_core/container/utils/#metador_core.container.utils.METADOR_META_PREF","title":"METADOR_META_PREF  <code>module-attribute</code>","text":"<pre><code>METADOR_META_PREF: Final[str] = METADOR_PREF + 'meta_'\n</code></pre> <p>Sub-prefix for group that stores group or dataset metadata.</p>"},{"location":"reference/metador_core/container/utils/#metador_core.container.utils.METADOR_TOC_PATH","title":"METADOR_TOC_PATH  <code>module-attribute</code>","text":"<pre><code>METADOR_TOC_PATH: Final[str] = f'/{METADOR_PREF}container'\n</code></pre> <p>Path of group with the Metador metadata index structure of the container.</p>"},{"location":"reference/metador_core/container/utils/#metador_core.container.utils.METADOR_VERSION_PATH","title":"METADOR_VERSION_PATH  <code>module-attribute</code>","text":"<pre><code>METADOR_VERSION_PATH: Final[\n    str\n] = f\"{METADOR_TOC_PATH}/version\"\n</code></pre> <p>Path of dataset with the Metador container spec version of the container.</p>"},{"location":"reference/metador_core/container/utils/#metador_core.container.utils.METADOR_UUID_PATH","title":"METADOR_UUID_PATH  <code>module-attribute</code>","text":"<pre><code>METADOR_UUID_PATH: Final[str] = f'{METADOR_TOC_PATH}/uuid'\n</code></pre> <p>Path of dataset with the Metador container version of the container.</p>"},{"location":"reference/metador_core/container/utils/#metador_core.container.utils.METADOR_PACKAGES_PATH","title":"METADOR_PACKAGES_PATH  <code>module-attribute</code>","text":"<pre><code>METADOR_PACKAGES_PATH: Final[\n    str\n] = f\"{METADOR_TOC_PATH}/packages\"\n</code></pre> <p>Path of group with package info of packages providing used schemas in the container.</p>"},{"location":"reference/metador_core/container/utils/#metador_core.container.utils.METADOR_SCHEMAS_PATH","title":"METADOR_SCHEMAS_PATH  <code>module-attribute</code>","text":"<pre><code>METADOR_SCHEMAS_PATH: Final[\n    str\n] = f\"{METADOR_TOC_PATH}/schemas\"\n</code></pre> <p>Path of group with info about used schemas in the container.</p>"},{"location":"reference/metador_core/container/utils/#metador_core.container.utils.METADOR_LINKS_PATH","title":"METADOR_LINKS_PATH  <code>module-attribute</code>","text":"<pre><code>METADOR_LINKS_PATH: Final[str] = f\"{METADOR_TOC_PATH}/links\"\n</code></pre> <p>Path of group with links to schema instances in the container.</p>"},{"location":"reference/metador_core/container/utils/#metador_core.container.utils.is_internal_path","title":"is_internal_path","text":"<pre><code>is_internal_path(\n    path: str, pref: str = METADOR_PREF\n) -&gt; bool\n</code></pre> <p>Return whether the path of this node is Metador-internal (metador_*).</p> <p>Optional argument can set a different prefix that path segments are checked for.</p> Source code in <code>src/metador_core/container/utils.py</code> <pre><code>def is_internal_path(path: str, pref: str = METADOR_PREF) -&gt; bool:\n\"\"\"Return whether the path of this node is Metador-internal (metador_*).\n\n    Optional argument can set a different prefix that path segments are checked for.\n    \"\"\"\n    # first case is for relative paths, second for later path segments and absolute paths\n    return path.startswith(pref) or path.find(f\"/{pref}\") &gt;= 0\n</code></pre>"},{"location":"reference/metador_core/container/utils/#metador_core.container.utils.is_meta_base_path","title":"is_meta_base_path","text":"<pre><code>is_meta_base_path(path: str) -&gt; bool\n</code></pre> <p>Return whether the path is a metadata base dir (but not an inner path!).</p> Source code in <code>src/metador_core/container/utils.py</code> <pre><code>def is_meta_base_path(path: str) -&gt; bool:\n\"\"\"Return whether the path is a metadata base dir (but not an inner path!).\"\"\"\n    return path.split(\"/\")[-1].startswith(METADOR_META_PREF)\n</code></pre>"},{"location":"reference/metador_core/container/utils/#metador_core.container.utils.to_meta_base_path","title":"to_meta_base_path","text":"<pre><code>to_meta_base_path(node_path: str, is_dataset: bool) -&gt; str\n</code></pre> <p>Return path to base group containing metadata for given node.</p> Source code in <code>src/metador_core/container/utils.py</code> <pre><code>def to_meta_base_path(node_path: str, is_dataset: bool) -&gt; str:\n\"\"\"Return path to base group containing metadata for given node.\"\"\"\n    segs = node_path.split(\"/\")\n    if is_dataset:\n        segs[-1] = METADOR_META_PREF + segs[-1]\n    elif segs == [\"\", \"\"]:  # name was \"/\"\n        segs[-1] = METADOR_META_PREF\n    else:\n        segs.append(METADOR_META_PREF)\n    return \"/\".join(segs)\n</code></pre>"},{"location":"reference/metador_core/container/utils/#metador_core.container.utils.to_data_node_path","title":"to_data_node_path","text":"<pre><code>to_data_node_path(meta_dir_path: str) -&gt; str\n</code></pre> <p>Given a metadata group path, infer the correct node path.</p> <p>Path can be relative or absolute. Will not check validity of the passed path, assumes it is fitting the scheme!</p> Source code in <code>src/metador_core/container/utils.py</code> <pre><code>def to_data_node_path(meta_dir_path: str) -&gt; str:\n\"\"\"Given a metadata group path, infer the correct node path.\n\n    Path can be relative or absolute.\n    Will not check validity of the passed path, assumes it is fitting the scheme!\n    \"\"\"\n    segs = meta_dir_path.split(\"/\")\n    pl = len(METADOR_META_PREF)\n    segs[-1] = segs[-1][pl:]\n    if segs[-1] == \"\" and (len(segs) &gt; 2 or segs[0] != \"\"):\n        segs.pop()\n    return \"/\".join(segs)\n</code></pre>"},{"location":"reference/metador_core/container/wrappers/","title":"wrappers","text":""},{"location":"reference/metador_core/container/wrappers/#metador_core.container.wrappers.UnsupportedOperationError","title":"UnsupportedOperationError","text":"<p>             Bases: <code>AttributeError</code></p> <p>Subclass to distinguish between actually missing attribute and unsupported one.</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>class UnsupportedOperationError(AttributeError):\n\"\"\"Subclass to distinguish between actually missing attribute and unsupported one.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/container/wrappers/#metador_core.container.wrappers.WrappedAttributeManager","title":"WrappedAttributeManager","text":"<p>             Bases: <code>ObjectProxy</code></p> <p>Wrapper for AttributeManager-like objects to prevent mutation (read-only) or inspection (skel-only).</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>class WrappedAttributeManager(wrapt.ObjectProxy):\n\"\"\"Wrapper for AttributeManager-like objects to prevent mutation (read-only) or inspection (skel-only).\"\"\"\n\n    __wrapped__: MutableMapping\n\n    _self_acl: NodeAclFlags\n    _self_acl_whitelist: Dict[NodeAcl, Set[str]] = {\n        NodeAcl.skel_only: {\"keys\"},\n        NodeAcl.read_only: {\"keys\", \"values\", \"items\", \"get\"},\n    }\n    _self_allowed: Set[str]\n\n    def __init__(self, obj, acl: NodeAclFlags):\n        super().__init__(obj)\n        # initialize whitelist based on passed ACL flags\n        self._self_acl = acl\n        allowed: Optional[Set[str]] = None\n        for flag, value in acl.items():\n            if not value or flag not in self._self_acl_whitelist:\n                continue\n            if allowed is None:\n                allowed = self._self_acl_whitelist[flag]\n            else:\n                allowed = allowed.intersection(self._self_acl_whitelist[flag])\n        self._self_allowed = allowed or set()\n\n    def _raise_illegal_op(self, flag_info: str):\n        raise UnsupportedOperationError(\n            f\"This attribute set belongs to a node marked as {flag_info}!\"\n        )\n\n    def __getattr__(self, key: str):\n        # NOTE: this will not restrict __contains__ because its a special method\n        # (which is desired behavior).\n        if (\n            hasattr(self.__wrapped__, key)\n            and self._self_allowed\n            and key not in self._self_allowed\n        ):\n            self._raise_illegal_op(str(self._self_acl))\n        return getattr(self.__wrapped__, key)\n\n    # this is not intercepted by getattr\n    def __getitem__(self, key: str):\n        if self._self_acl[NodeAcl.skel_only]:\n            self._raise_illegal_op(NodeAcl.skel_only.name)\n        return self.__wrapped__.__getitem__(key)\n\n    # this is not intercepted by getattr\n    def __setitem__(self, key: str, value):\n        if self._self_acl[NodeAcl.read_only]:\n            self._raise_illegal_op(NodeAcl.read_only.name)\n        return self.__wrapped__.__setitem__(key, value)\n\n    # this is not intercepted by getattr\n    def __delitem__(self, key: str):\n        if self._self_acl[NodeAcl.read_only]:\n            self._raise_illegal_op(NodeAcl.read_only.name)\n        return self.__wrapped__.__delitem__(key)\n\n    def __repr__(self) -&gt; str:\n        return repr(self.__wrapped__)\n</code></pre>"},{"location":"reference/metador_core/container/wrappers/#metador_core.container.wrappers.WithDefaultQueryStartNode","title":"WithDefaultQueryStartNode","text":"<p>             Bases: <code>ObjectProxy</code></p> <p>Cosmetic wrapper to search metadata below a H5LikeGroup.</p> Used to make sure that <p><code>group.metador.query(...)</code></p> <p>is equivalent to:     <code>container.metador.query(..., node=group)</code></p> <p>(without it, the default node will be the root \"/\" if not specified).</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>class WithDefaultQueryStartNode(wrapt.ObjectProxy):\n\"\"\"Cosmetic wrapper to search metadata below a H5LikeGroup.\n\n    Used to make sure that:\n        `group.metador.query(...)`\n    is equivalent to:\n        `container.metador.query(..., node=group)`\n\n    (without it, the default node will be the root \"/\" if not specified).\n    \"\"\"\n\n    __wrapped__: MetadorContainerTOC\n\n    def __init__(self, obj: MetadorContainerTOC, def_start_node):\n        super().__init__(obj)\n        self._self_query_start_node = def_start_node\n\n    def query(self, schema, version=None, *, node=None):\n        node = node or self._self_query_start_node\n        return self.__wrapped__.query(schema, version, node=node)\n</code></pre>"},{"location":"reference/metador_core/container/wrappers/#metador_core.container.wrappers.MetadorNode","title":"MetadorNode","text":"<p>             Bases: <code>ObjectProxy</code></p> <p>Wrapper for h5py and IH5 Groups and Datasets providing Metador-specific features.</p> <p>In addition to the Metadata management, also provides helpers to reduce possible mistakes in implementing interfaces by allowing to mark nodes as</p> <ul> <li>read_only (regardless of the writability of the underlying opened container) and</li> <li>local_only (preventing access to (meta)data above this node)</li> </ul> <p>Note that these are \"soft\" restrictions to prevent errors and can be bypassed.</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>class MetadorNode(wrapt.ObjectProxy):\n\"\"\"Wrapper for h5py and IH5 Groups and Datasets providing Metador-specific features.\n\n    In addition to the Metadata management, also provides helpers to reduce possible\n    mistakes in implementing interfaces by allowing to mark nodes as\n\n    * read_only (regardless of the writability of the underlying opened container) and\n    * local_only (preventing access to (meta)data above this node)\n\n    Note that these are \"soft\" restrictions to prevent errors and can be bypassed.\n    \"\"\"\n\n    __wrapped__: H5NodeLike\n\n    @staticmethod\n    def _parse_access_flags(kwargs) -&gt; NodeAclFlags:\n        # NOTE: mutating kwargs, removes keys that are inspected!\n        return {flag: kwargs.pop(flag.name, False) for flag in iter(NodeAcl)}\n\n    def __init__(self, mc: MetadorContainer, node: H5NodeLike, **kwargs):\n        flags = self._parse_access_flags(kwargs)\n        lp = kwargs.pop(\"local_parent\", None)\n        if kwargs:\n            raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n\n        super().__init__(node)\n        self._self_container: MetadorContainer = mc\n\n        self._self_flags: NodeAclFlags = flags\n        self._self_local_parent: Optional[MetadorGroup] = lp\n\n    def _child_node_kwargs(self):\n\"\"\"Return kwargs to be passed to a child node.\n\n        Ensures that {read,skel,local}_only status is passed down correctly.\n        \"\"\"\n        return {\n            \"local_parent\": self if self.acl[NodeAcl.local_only] else None,\n            **{k.name: v for k, v in self.acl.items() if v},\n        }\n\n    def restrict(self, **kwargs) -&gt; MetadorNode:\n\"\"\"Restrict this object to be local_only or read_only.\n\n        Pass local_only=True and/or read_only=True to enable the restriction.\n\n        local_only means that the node may not access the parent or file objects.\n        read_only means that mutable actions cannot be done (even if container is mutable).\n        \"\"\"\n        added_flags = self._parse_access_flags(kwargs)\n        if added_flags[NodeAcl.local_only]:\n            # was set as local explicitly for this node -&gt;\n            self._self_local_parent = None  # remove its ability to go up\n\n        # can only set, but not unset!\n        self._self_flags.update({k: True for k, v in added_flags.items() if v})\n        if kwargs:\n            raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n\n        return self\n\n    @property\n    def acl(self) -&gt; Dict[NodeAcl, bool]:\n\"\"\"Return ACL flags of current node.\"\"\"\n        return dict(self._self_flags)\n\n    def _guard_path(self, path: str):\n        if M.is_internal_path(path):\n            msg = f\"Trying to use a Metador-internal path: '{path}'\"\n            raise ValueError(msg)\n        if self.acl[NodeAcl.local_only] and path[0] == \"/\":\n            msg = f\"Node is marked as local_only, cannot use absolute path '{path}'!\"\n            raise ValueError(msg)\n\n    def _guard_acl(self, flag: NodeAcl, method: str = \"this method\"):\n        if self.acl[flag]:\n            msg = f\"Cannot use {method}, the node is marked as {flag.name}!\"\n            raise UnsupportedOperationError(msg)\n\n    # helpers\n\n    def _wrap_if_node(self, val):\n\"\"\"Wrap value into a metador node wrapper, if it is a suitable group or dataset.\"\"\"\n        if isinstance(val, H5GroupLike):\n            return MetadorGroup(self._self_container, val, **self._child_node_kwargs())\n        elif isinstance(val, H5DatasetLike):\n            return MetadorDataset(\n                self._self_container, val, **self._child_node_kwargs()\n            )\n        else:\n            return val\n\n    def _destroy_meta(self, _unlink: bool = True):\n\"\"\"Destroy all attached metadata at and below this node.\"\"\"\n        self.meta._destroy(_unlink=_unlink)\n\n    # need that to add our new methods\n\n    def __dir__(self):\n        names = set.union(\n            *map(\n                lambda x: set(x.__dict__.keys()),\n                takewhile(lambda x: issubclass(x, MetadorNode), type(self).mro()),\n            )\n        )\n        return list(set(super().__dir__()).union(names))\n\n    # make wrapper transparent\n\n    def __repr__(self):\n        return repr(self.__wrapped__)\n\n    # added features\n\n    @property\n    def meta(self) -&gt; MetadorMeta:\n\"\"\"Access the interface to metadata attached to this node.\"\"\"\n        return MetadorMeta(self)\n\n    @property\n    def metador(self) -&gt; MetadorContainerTOC:\n\"\"\"Access the info about the container this node belongs to.\"\"\"\n        return WithDefaultQueryStartNode(self._self_container.metador, self)\n\n    # wrap existing methods as needed\n\n    @property\n    def name(self) -&gt; str:\n        return self.__wrapped__.name  # just for type checker not to complain\n\n    @property\n    def attrs(self):\n        if self.acl[NodeAcl.read_only] or self.acl[NodeAcl.skel_only]:\n            return WrappedAttributeManager(self.__wrapped__.attrs, self.acl)\n        return self.__wrapped__.attrs\n\n    @property\n    def parent(self) -&gt; MetadorGroup:\n        if self.acl[NodeAcl.local_only]:\n            # allow child nodes of local-only nodes to go up to the marked parent\n            # (or it is None, if this is the local root)\n            if lp := self._self_local_parent:\n                return lp\n            else:\n                # raise exception (illegal non-local access)\n                self._guard_acl(NodeAcl.local_only, \"parent\")\n\n        return MetadorGroup(\n            self._self_container,\n            self.__wrapped__.parent,\n            **self._child_node_kwargs(),\n        )\n\n    @property\n    def file(self) -&gt; MetadorContainer:\n        if self.acl[NodeAcl.local_only]:\n            # raise exception (illegal non-local access)\n            self._guard_acl(NodeAcl.local_only, \"parent\")\n        return self._self_container\n</code></pre>"},{"location":"reference/metador_core/container/wrappers/#metador_core.container.wrappers.MetadorNode.acl","title":"acl  <code>property</code>","text":"<pre><code>acl: Dict[NodeAcl, bool]\n</code></pre> <p>Return ACL flags of current node.</p>"},{"location":"reference/metador_core/container/wrappers/#metador_core.container.wrappers.MetadorNode.meta","title":"meta  <code>property</code>","text":"<pre><code>meta: MetadorMeta\n</code></pre> <p>Access the interface to metadata attached to this node.</p>"},{"location":"reference/metador_core/container/wrappers/#metador_core.container.wrappers.MetadorNode.metador","title":"metador  <code>property</code>","text":"<pre><code>metador: MetadorContainerTOC\n</code></pre> <p>Access the info about the container this node belongs to.</p>"},{"location":"reference/metador_core/container/wrappers/#metador_core.container.wrappers.MetadorNode.restrict","title":"restrict","text":"<pre><code>restrict(**kwargs) -&gt; MetadorNode\n</code></pre> <p>Restrict this object to be local_only or read_only.</p> <p>Pass local_only=True and/or read_only=True to enable the restriction.</p> <p>local_only means that the node may not access the parent or file objects. read_only means that mutable actions cannot be done (even if container is mutable).</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>def restrict(self, **kwargs) -&gt; MetadorNode:\n\"\"\"Restrict this object to be local_only or read_only.\n\n    Pass local_only=True and/or read_only=True to enable the restriction.\n\n    local_only means that the node may not access the parent or file objects.\n    read_only means that mutable actions cannot be done (even if container is mutable).\n    \"\"\"\n    added_flags = self._parse_access_flags(kwargs)\n    if added_flags[NodeAcl.local_only]:\n        # was set as local explicitly for this node -&gt;\n        self._self_local_parent = None  # remove its ability to go up\n\n    # can only set, but not unset!\n    self._self_flags.update({k: True for k, v in added_flags.items() if v})\n    if kwargs:\n        raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n\n    return self\n</code></pre>"},{"location":"reference/metador_core/container/wrappers/#metador_core.container.wrappers.MetadorDataset","title":"MetadorDataset","text":"<p>             Bases: <code>MetadorNode</code></p> <p>Metador wrapper for a HDF5 Dataset.</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>class MetadorDataset(MetadorNode):\n\"\"\"Metador wrapper for a HDF5 Dataset.\"\"\"\n\n    __wrapped__: H5DatasetLike\n\n    # manually assembled from public methods which h5py.Dataset provides\n    _self_RO_FORBIDDEN = {\"resize\", \"make_scale\", \"write_direct\", \"flush\"}\n\n    def __getattr__(self, key):\n        if self.acl[NodeAcl.read_only] and key in self._self_RO_FORBIDDEN:\n            self._guard_acl(NodeAcl.read_only, key)\n        if self.acl[NodeAcl.skel_only] and key == \"get\":\n            self._guard_acl(NodeAcl.skel_only, key)\n\n        return getattr(self.__wrapped__, key)\n\n    # prevent getter of node if marked as skel_only\n    def __getitem__(self, *args, **kwargs):\n        self._guard_acl(NodeAcl.skel_only, \"__getitem__\")\n        return self.__wrapped__.__getitem__(*args, **kwargs)\n\n    # prevent mutating method calls of node is marked as read_only\n\n    def __setitem__(self, *args, **kwargs):\n        self._guard_acl(NodeAcl.read_only, \"__setitem__\")\n        return self.__wrapped__.__setitem__(*args, **kwargs)\n</code></pre>"},{"location":"reference/metador_core/container/wrappers/#metador_core.container.wrappers.MetadorGroup","title":"MetadorGroup","text":"<p>             Bases: <code>MetadorNode</code></p> <p>Wrapper for a HDF5 Group.</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>class MetadorGroup(MetadorNode):\n\"\"\"Wrapper for a HDF5 Group.\"\"\"\n\n    __wrapped__: H5GroupLike\n\n    def _destroy_meta(self, _unlink: bool = True):\n\"\"\"Destroy all attached metadata at and below this node (recursively).\"\"\"\n        super()._destroy_meta(_unlink=_unlink)  # this node\n        for child in self.values():  # recurse\n            child._destroy_meta(_unlink=_unlink)\n\n    # these access entities in read-only way:\n\n    get = _wrap_method(\"get\", is_read_only_method=True)\n    __getitem__ = _wrap_method(\"__getitem__\", is_read_only_method=True)\n\n    # these just create new entities with no metadata attached:\n\n    create_group = _wrap_method(\"create_group\")\n    require_group = _wrap_method(\"require_group\")\n    create_dataset = _wrap_method(\"create_dataset\")\n    require_dataset = _wrap_method(\"require_dataset\")\n\n    def __setitem__(self, name, value):\n        if any(map(lambda x: isinstance(value, x), _H5_REF_TYPES)):\n            raise ValueError(f\"Unsupported reference type: {type(value).__name__}\")\n\n        return _wrap_method(\"__setitem__\")(self, name, value)\n\n    # following all must be filtered to hide metador-specific structures:\n\n    # must wrap nodes passed into the callback function and filter visited names\n    def visititems(self, func):\n        def wrapped_func(name, node):\n            if M.is_internal_path(node.name):\n                return  # skip path/node\n            return func(name, self._wrap_if_node(node))\n\n        return self.__wrapped__.visititems(wrapped_func)  # RAW\n\n    # paths passed to visit also must be filtered, so must override this one too\n    def visit(self, func):\n        def wrapped_func(name, _):\n            return func(name)\n\n        return self.visititems(wrapped_func)\n\n    # following also depend on the filtered sequence, directly\n    # filter the items, derive other related functions based on that\n\n    def items(self):\n        for k, v in self.__wrapped__.items():\n            if v is None:\n                # NOTE: e.g. when nodes are deleted/moved during iteration,\n                # v can suddenly be None -&gt; we need to catch this case!\n                continue\n            if not M.is_internal_path(v.name):\n                yield (k, self._wrap_if_node(v))\n\n    def values(self):\n        return map(lambda x: x[1], self.items())\n\n    def keys(self):\n        return map(lambda x: x[0], self.items())\n\n    def __iter__(self):\n        return iter(self.keys())\n\n    def __len__(self):\n        return len(list(self.keys()))\n\n    def __contains__(self, name: str):\n        self._guard_path(name)\n        if name[0] == \"/\" and self.name != \"/\":\n            return name in self[\"/\"]\n        segs = name.lstrip(\"/\").split(\"/\")\n        has_first_seg = segs[0] in self.keys()\n        if len(segs) == 1:\n            return has_first_seg\n        else:\n            if nxt := self.get(segs[0]):\n                return \"/\".join(segs[1:]) in nxt\n            return False\n\n    # these we can take care of but are a bit more tricky to think through\n\n    def __delitem__(self, name: str):\n        self._guard_acl(NodeAcl.read_only, \"__delitem__\")\n        self._guard_path(name)\n\n        node = self[name]\n        # clean up metadata (recursively, if a group)\n        node._destroy_meta()\n        # kill the actual data\n        return _wrap_method(\"__delitem__\")(self, name)\n\n    def move(self, source: str, dest: str):\n        self._guard_acl(NodeAcl.read_only, \"move\")\n        self._guard_path(source)\n        self._guard_path(dest)\n\n        src_metadir = self[source].meta._base_dir\n        # if actual data move fails, an exception will prevent the rest\n        self.__wrapped__.move(source, dest)  # RAW\n\n        # if we're here, no problems -&gt; proceed with moving metadata\n        dst_node = self[dest]\n        if isinstance(dst_node, MetadorDataset):\n            dst_metadir = dst_node.meta._base_dir\n            # dataset has its metadata stored in parallel -&gt; need to take care of it\n            meta_base = dst_metadir\n            if src_metadir in self.__wrapped__:  # RAW\n                self.__wrapped__.move(src_metadir, dst_metadir)  # RAW\n        else:\n            # directory where to fix up metadata object TOC links\n            # when a group was moved, all metadata is contained in dest -&gt; search it\n            meta_base = dst_node.name\n\n        # re-link metadata object TOC links\n        if meta_base_node := self.__wrapped__.get(meta_base):\n            assert isinstance(meta_base_node, H5GroupLike)\n            missing = self._self_container.metador._links.find_missing(meta_base_node)\n            self._self_container.metador._links.repair_missing(missing, update=True)\n\n    def copy(\n        self,\n        source: Union[str, MetadorGroup, MetadorDataset],\n        dest: Union[str, MetadorGroup],\n        **kwargs,\n    ):\n        self._guard_acl(NodeAcl.read_only, \"copy\")\n\n        # get source node and its name without the path and its type\n        src_node: MetadorNode\n        if isinstance(source, str):\n            self._guard_path(source)\n            src_node = self[source]\n        elif isinstance(source, MetadorNode):\n            src_node = source\n        else:\n            raise ValueError(\"Copy source must be path, Group or Dataset!\")\n        src_is_dataset: bool = isinstance(src_node, MetadorDataset)\n        src_name: str = src_node.name.split(\"/\")[-1]\n        # user can override name at target\n        dst_name: str = kwargs.pop(\"name\", src_name)\n\n        # fix up target path\n        dst_path: str\n        if isinstance(dest, str):\n            self._guard_path(dest)\n            dst_path = dest\n        elif isinstance(dest, MetadorGroup):\n            dst_path = dest.name + f\"/{dst_name}\"\n        else:\n            raise ValueError(\"Copy dest must be path or Group!\")\n\n        # get other allowed options\n        without_attrs: bool = kwargs.pop(\"without_attrs\", False)\n        without_meta: bool = kwargs.pop(\"without_meta\", False)\n        if kwargs:\n            raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n\n        # perform copy\n        copy_kwargs = {\n            \"name\": None,\n            \"shallow\": False,\n            \"expand_soft\": True,\n            \"expand_external\": True,\n            \"expand_refs\": True,\n            \"without_attrs\": without_attrs,\n        }\n        self.__wrapped__.copy(source, dst_path, **copy_kwargs)  # RAW\n        dst_node = self[dst_path]  # exists now\n\n        if src_is_dataset and not without_meta:\n            # because metadata lives in parallel group, need to copy separately:\n            src_meta: str = src_node.meta._base_dir\n            dst_meta: str = dst_node.meta._base_dir  # node will not exist yet\n            self.__wrapped__.copy(src_meta, dst_meta, **copy_kwargs)  # RAW\n\n            # register in TOC:\n            dst_meta_node = self.__wrapped__[dst_meta]\n            assert isinstance(dst_meta_node, H5GroupLike)\n            missing = self._self_container.metador._links.find_missing(dst_meta_node)\n            self._self_container.metador._links.repair_missing(missing)\n\n        if not src_is_dataset:\n            if without_meta:\n                # need to destroy copied metadata copied with the source group\n                # but keep TOC links (they point to original copy!)\n                dst_node._destroy_meta(_unlink=False)\n            else:\n                # register copied metadata objects under new uuids\n                missing = self._self_container.metador._links.find_missing(dst_node)\n                self._self_container.metador._links.repair_missing(missing)\n\n    def __getattr__(self, key):\n        if hasattr(self.__wrapped__, key):\n            raise UnsupportedOperationError(key)  # deliberately unsupported\n        else:\n            msg = f\"'{type(self).__name__}' object has no attribute '{key}'\"\n            raise AttributeError(msg)\n</code></pre>"},{"location":"reference/metador_core/container/wrappers/#metador_core.container.wrappers.MetadorContainer","title":"MetadorContainer","text":"<p>             Bases: <code>MetadorGroup</code></p> <p>Wrapper class adding Metador container interface to h5py.File-like objects.</p> <p>The wrapper ensures that any actions done to IH5Records through this interface also work with plain h5py.Files.</p> <p>There are no guarantees about behaviour with h5py methods not supported by IH5Records.</p> <p>Given <code>old: MetadorContainer</code>, <code>MetadorContainer(old.data_source, driver=old.data_driver)</code> should be able to construct another object to access the same data (assuming it is not locked).</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>class MetadorContainer(MetadorGroup):\n\"\"\"Wrapper class adding Metador container interface to h5py.File-like objects.\n\n    The wrapper ensures that any actions done to IH5Records through this interface\n    also work with plain h5py.Files.\n\n    There are no guarantees about behaviour with h5py methods not supported by IH5Records.\n\n    Given `old: MetadorContainer`, `MetadorContainer(old.data_source, driver=old.data_driver)`\n    should be able to construct another object to access the same data (assuming it is not locked).\n    \"\"\"\n\n    __wrapped__: H5FileLike\n\n    _self_SUPPORTED: Set[str] = {\"mode\", \"flush\", \"close\"}\n\n    # ---- new container-level interface ----\n\n    _self_toc: MetadorContainerTOC\n\n    @property\n    def metador(self) -&gt; MetadorContainerTOC:\n\"\"\"Access interface to Metador metadata object index.\"\"\"\n        return self._self_toc\n\n    def __init__(\n        self,\n        name_or_obj: Union[MetadorDriver, Any],\n        mode: Optional[OpenMode] = \"r\",\n        *,\n        # NOTE: driver takes class instead of enum to also allow subclasses\n        driver: Optional[Type[MetadorDriver]] = None,\n    ):\n\"\"\"Initialize a MetadorContainer instance from file(s) or a supported object.\n\n        The `mode` argument is ignored when simply wrapping an object.\n\n        If a data source such as a path is passed, will instantiate the object first,\n        using the default H5File driver or the passed `driver` keyword argument.\n        \"\"\"\n        # wrap the h5file-like object (will set self.__wrapped__)\n        super().__init__(self, to_h5filelike(name_or_obj, mode, driver=driver))\n        # initialize metador-specific stuff\n        self._self_toc = MetadorContainerTOC(self)\n\n    # not clear if we want these in the public interface. keep this private for now:\n\n    # def _find_orphan_meta(self) -&gt; List[str]:\n    #     \"\"\"Return list of paths to metadata that has no corresponding user node anymore.\"\"\"\n    #     ret: List[str] = []\n\n    #     def collect_orphans(name: str):\n    #         if M.is_meta_base_path(name):\n    #             if M.to_data_node_path(name) not in self:\n    #                 ret.append(name)\n\n    #     self.__wrapped__.visit(collect_orphans)\n    #     return ret\n\n    # def _repair(self, remove_orphans: bool = False):\n    #     \"\"\"Repair container structure on best-effort basis.\n\n    #     This will ensure that the TOC points to existing metadata objects\n    #     and that all metadata objects are listed in the TOC.\n\n    #     If remove_orphans is set, will erase metadata not belonging to an existing node.\n\n    #     Notice that missing schema plugin dependency metadata cannot be restored.\n    #     \"\"\"\n    #     if remove_orphans:\n    #         for path in self._find_orphan_meta():\n    #             del self.__wrapped__[path]\n    #     self.toc._links.find_broken(repair=True)\n    #     missing = self.toc._links._find_missing(\"/\")\n    #     self.toc._links.repair_missing(missing)\n\n    # ---- pass through HDF5 group methods to a wrapped root group instance ----\n\n    def __getattr__(self, key: str):\n        if key in self._self_SUPPORTED:\n            return getattr(self.__wrapped__, key)\n        return super().__getattr__(key)  # ask group for method\n\n    # context manager: return the wrapper back, not the raw thing:\n\n    def __enter__(self):\n        self.__wrapped__.__enter__()\n        return self\n\n    def __exit__(self, *args):\n        return self.__wrapped__.__exit__(*args)\n\n    # we want these also to be forwarded to the wrapped group, not the raw object:\n\n    def __dir__(self):\n        return list(set(super().__dir__()).union(type(self).__dict__.keys()))\n\n    # make wrapper transparent:\n\n    def __repr__(self) -&gt; str:\n        return repr(self.__wrapped__)  # shows that its a File, not just a Group\n</code></pre>"},{"location":"reference/metador_core/container/wrappers/#metador_core.container.wrappers.MetadorContainer.metador","title":"metador  <code>property</code>","text":"<pre><code>metador: MetadorContainerTOC\n</code></pre> <p>Access interface to Metador metadata object index.</p>"},{"location":"reference/metador_core/container/wrappers/#metador_core.container.wrappers.MetadorContainer.__init__","title":"__init__","text":"<pre><code>__init__(\n    name_or_obj: Union[MetadorDriver, Any],\n    mode: Optional[OpenMode] = \"r\",\n    *,\n    driver: Optional[Type[MetadorDriver]] = None\n)\n</code></pre> <p>Initialize a MetadorContainer instance from file(s) or a supported object.</p> <p>The <code>mode</code> argument is ignored when simply wrapping an object.</p> <p>If a data source such as a path is passed, will instantiate the object first, using the default H5File driver or the passed <code>driver</code> keyword argument.</p> Source code in <code>src/metador_core/container/wrappers.py</code> <pre><code>def __init__(\n    self,\n    name_or_obj: Union[MetadorDriver, Any],\n    mode: Optional[OpenMode] = \"r\",\n    *,\n    # NOTE: driver takes class instead of enum to also allow subclasses\n    driver: Optional[Type[MetadorDriver]] = None,\n):\n\"\"\"Initialize a MetadorContainer instance from file(s) or a supported object.\n\n    The `mode` argument is ignored when simply wrapping an object.\n\n    If a data source such as a path is passed, will instantiate the object first,\n    using the default H5File driver or the passed `driver` keyword argument.\n    \"\"\"\n    # wrap the h5file-like object (will set self.__wrapped__)\n    super().__init__(self, to_h5filelike(name_or_obj, mode, driver=driver))\n    # initialize metador-specific stuff\n    self._self_toc = MetadorContainerTOC(self)\n</code></pre>"},{"location":"reference/metador_core/harvester/","title":"harvester","text":"<p>Plugin group for metadata harvesters.</p>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.HarvesterPlugin","title":"HarvesterPlugin","text":"<p>             Bases: <code>PluginBase</code></p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>class HarvesterPlugin(pg.PluginBase):\n    returns: SchemaPluginRef\n\"\"\"Schema returned by this harvester.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.HarvesterPlugin.returns","title":"returns  <code>instance-attribute</code>","text":"<pre><code>returns: SchemaPluginRef\n</code></pre> <p>Schema returned by this harvester.</p>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.HarvesterArgs","title":"HarvesterArgs","text":"<p>             Bases: <code>BaseModelPlus</code></p> <p>Base class for harvester arguments.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>class HarvesterArgs(BaseModelPlus):\n\"\"\"Base class for harvester arguments.\"\"\"\n\n    class Config(BaseModelPlus.Config):\n        extra = Extra.forbid\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.HarvesterArgsPartial","title":"HarvesterArgsPartial","text":"<p>             Bases: <code>PartialFactory</code></p> <p>Base class for partial harvester arguments.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>class HarvesterArgsPartial(PartialFactory):\n\"\"\"Base class for partial harvester arguments.\"\"\"\n\n    base_model = HarvesterArgs\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.HarvesterMetaMixin","title":"HarvesterMetaMixin","text":"<p>             Bases: <code>type</code></p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>class HarvesterMetaMixin(type):\n    _args_partials: Dict[Type, Type] = {}\n\n    @property\n    # @cache not needed, partials take care of that themselves\n    def PartialArgs(self):\n\"\"\"Access the partial schema based on the current schema.\"\"\"\n        return HarvesterArgsPartial.get_partial(self.Args)\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.HarvesterMetaMixin.PartialArgs","title":"PartialArgs  <code>property</code>","text":"<pre><code>PartialArgs\n</code></pre> <p>Access the partial schema based on the current schema.</p>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.Harvester","title":"Harvester","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for metadata harvesters.</p> <p>A harvester is a class that can be instantiated to extract metadata according to some schema plugin (the returned schema must be defined as the <code>Plugin.returns</code> attribute)</p> <p>Override the inner <code>Harvester.Args</code> class with a subclass to add arguments. This works exactly like schema definition and is simply another pydantic model, so you can use both field and root validators to check whether the arguments are making sense.</p> <p>If all you need is getting a file path, consider using <code>FileHarvester</code>, which already defines a <code>filepath</code> argument.</p> <p>Override the <code>run()</code> method to implement the metadata harvesting based on a validated configuration located at <code>self.args</code>.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>class Harvester(ABC, metaclass=HarvesterMeta):\n\"\"\"Base class for metadata harvesters.\n\n    A harvester is a class that can be instantiated to extract\n    metadata according to some schema plugin (the returned schema\n    must be defined as the `Plugin.returns` attribute)\n\n    Override the inner `Harvester.Args` class with a subclass to add arguments.\n    This works exactly like schema definition and is simply\n    another pydantic model, so you can use both field and root\n    validators to check whether the arguments are making sense.\n\n    If all you need is getting a file path, consider using\n    `FileHarvester`, which already defines a `filepath` argument.\n\n    Override the `run()` method to implement the metadata harvesting\n    based on a validated configuration located at `self.args`.\n    \"\"\"\n\n    Plugin: ClassVar[HarvesterPlugin]\n\n    Args: TypeAlias = HarvesterArgs\n\"\"\"Arguments to be passed to the harvester.\"\"\"\n\n    args: Union[Harvester.Args, HarvesterArgsPartial]\n\n    @property\n    def schema(self):\n\"\"\"Partial schema class returned by this harvester.\n\n        Provided for implementation convenience.\n        \"\"\"\n        return schemas[self.Plugin.returns.name].Partial\n\n    # ----\n    # configuring and checking harvester instances:\n\n    def __init__(self, **kwargs):\n\"\"\"Initialize harvester with (partial) configuration.\"\"\"\n        self.args = type(self).PartialArgs.parse_obj(kwargs)\n\n    def __call__(self, **kwargs):\n\"\"\"Return copy of harvester with updated configuration.\"\"\"\n        args = type(self).PartialArgs.parse_obj(kwargs)\n        merged = self.args.merge_with(args)\n        return type(self)(**merged.dict())\n\n    def __repr__(self):\n        return f\"{type(self).__name__}(args={repr(self.args)})\"\n\n    def harvest(self):\n\"\"\"Check provided arguments and run the harvester.\n\n        Call this when a harvester is configured and it should be executed.\n        \"\"\"\n        try:\n            self.args = self.args.from_partial()\n        except ValidationError as e:\n            hname = type(self).__name__\n            msg = f\"{hname} configuration incomplete or invalid:\\n{str(e)}\"\n            raise ValueError(msg)\n        return self.run()\n\n    # ----\n    # to be overridden\n\n    @abstractmethod\n    def run(self) -&gt; MetadataSchema:\n\"\"\"Do the harvesting according to instance configuration and return metadata.\n\n        Override this method with your custom metadata harvesting logic, based\n        on configuration provided in `self.args`.\n\n        All actual, possibly time-intensive harvesting computations (accessing\n        resources, running external code, etc.) MUST be performed in this method.\n\n        Ensure that your harvester does not interfere with anything else, such\n        as other harvesters running on possibly the same data - open resources\n        such as files in read-only mode. You SHOULD avoid creating tempfiles, or\n        ensure that these are managed, fresh and cleaned up when you are done\n        (e.g. if you need to run external processes that dump outputs to a\n        file).\n\n        Returns:\n            A fresh instance of type `self.schema` containing harvested metadata.\n        \"\"\"\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.Harvester.Args","title":"Args  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>Args: TypeAlias = HarvesterArgs\n</code></pre> <p>Arguments to be passed to the harvester.</p>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.Harvester.schema","title":"schema  <code>property</code>","text":"<pre><code>schema\n</code></pre> <p>Partial schema class returned by this harvester.</p> <p>Provided for implementation convenience.</p>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.Harvester.__init__","title":"__init__","text":"<pre><code>__init__(**kwargs)\n</code></pre> <p>Initialize harvester with (partial) configuration.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>def __init__(self, **kwargs):\n\"\"\"Initialize harvester with (partial) configuration.\"\"\"\n    self.args = type(self).PartialArgs.parse_obj(kwargs)\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.Harvester.__call__","title":"__call__","text":"<pre><code>__call__(**kwargs)\n</code></pre> <p>Return copy of harvester with updated configuration.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>def __call__(self, **kwargs):\n\"\"\"Return copy of harvester with updated configuration.\"\"\"\n    args = type(self).PartialArgs.parse_obj(kwargs)\n    merged = self.args.merge_with(args)\n    return type(self)(**merged.dict())\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.Harvester.harvest","title":"harvest","text":"<pre><code>harvest()\n</code></pre> <p>Check provided arguments and run the harvester.</p> <p>Call this when a harvester is configured and it should be executed.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>def harvest(self):\n\"\"\"Check provided arguments and run the harvester.\n\n    Call this when a harvester is configured and it should be executed.\n    \"\"\"\n    try:\n        self.args = self.args.from_partial()\n    except ValidationError as e:\n        hname = type(self).__name__\n        msg = f\"{hname} configuration incomplete or invalid:\\n{str(e)}\"\n        raise ValueError(msg)\n    return self.run()\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.Harvester.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run() -&gt; MetadataSchema\n</code></pre> <p>Do the harvesting according to instance configuration and return metadata.</p> <p>Override this method with your custom metadata harvesting logic, based on configuration provided in <code>self.args</code>.</p> <p>All actual, possibly time-intensive harvesting computations (accessing resources, running external code, etc.) MUST be performed in this method.</p> <p>Ensure that your harvester does not interfere with anything else, such as other harvesters running on possibly the same data - open resources such as files in read-only mode. You SHOULD avoid creating tempfiles, or ensure that these are managed, fresh and cleaned up when you are done (e.g. if you need to run external processes that dump outputs to a file).</p> <p>Returns:</p> Type Description <code>MetadataSchema</code> <p>A fresh instance of type <code>self.schema</code> containing harvested metadata.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>@abstractmethod\ndef run(self) -&gt; MetadataSchema:\n\"\"\"Do the harvesting according to instance configuration and return metadata.\n\n    Override this method with your custom metadata harvesting logic, based\n    on configuration provided in `self.args`.\n\n    All actual, possibly time-intensive harvesting computations (accessing\n    resources, running external code, etc.) MUST be performed in this method.\n\n    Ensure that your harvester does not interfere with anything else, such\n    as other harvesters running on possibly the same data - open resources\n    such as files in read-only mode. You SHOULD avoid creating tempfiles, or\n    ensure that these are managed, fresh and cleaned up when you are done\n    (e.g. if you need to run external processes that dump outputs to a\n    file).\n\n    Returns:\n        A fresh instance of type `self.schema` containing harvested metadata.\n    \"\"\"\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.FileHarvester","title":"FileHarvester","text":"<p>             Bases: <code>Harvester</code></p> <p>Harvester for processing a single file path.</p> <p>The file path is not provided or set during init, but instead is passed during harvest_file.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>class FileHarvester(Harvester):\n\"\"\"Harvester for processing a single file path.\n\n    The file path is not provided or set during __init__,\n    but instead is passed during harvest_file.\n    \"\"\"\n\n    class Args(Harvester.Args):\n        filepath: Path\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.PGHarvester","title":"PGHarvester","text":"<p>             Bases: <code>PluginGroup[Harvester]</code></p> <p>Harvester plugin group interface.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>class PGHarvester(pg.PluginGroup[Harvester]):\n\"\"\"Harvester plugin group interface.\"\"\"\n\n    class Plugin:\n        name = HARVESTER_GROUP_NAME\n        version = (0, 1, 0)\n        requires = [PluginRef(group=\"plugingroup\", name=\"schema\", version=(0, 1, 0))]\n        plugin_class = Harvester\n        plugin_info_class = HarvesterPlugin\n\n    def __post_init__(self):\n        self._harvesters_for: Dict[str, Set[str]] = {}\n\n    def plugin_deps(self, plugin):\n        if p := plugin.Plugin.returns:\n            return {p}\n\n    @overrides\n    def check_plugin(self, ep_name: str, plugin: Type[Harvester]):\n        hv_ref = plugin.Plugin.returns\n\n        schema_name = hv_ref.name\n        schema = schemas[schema_name]\n        if not schema:\n            raise TypeError(f\"{ep_name}: Schema '{schema_name}' not installed!\")\n\n        inst_ref = schema.Plugin.ref()\n        if not inst_ref.supports(hv_ref):\n            msg = f\"{ep_name}: Installed schema {inst_ref} incompatible with harvester schema {hv_ref}!\"\n            raise TypeError(msg)\n\n    @overrides\n    def init_plugin(self, plugin):\n\"\"\"Add harvester to harvester lookup table.\"\"\"\n        h_name = plugin.Plugin.name\n        schema = plugin.Plugin.returns\n        if schema.name not in self._harvesters_for:\n            self._harvesters_for[schema.name] = set()\n        self._harvesters_for[schema.name].add(h_name)\n\n    def for_schema(\n        self,\n        schema: Union[str, MetadataSchema],\n        *,\n        include_children: bool = False,\n        include_parents: bool = False,\n    ) -&gt; Set[str]:\n\"\"\"List installed harvesters for the given metadata schema.\n\n        To extend the query to parent or child schemas, set `include_children`\n        and/or `include_parents` accordingly.\n\n        Harvesters for child schemas are always compatible with the schema.\n        (assuming correct implementation of the child schemas),\n\n        Harvesters for parent schemas can be incompatible in some circumstances\n        (specifically, if the parent accepts values for some field that are\n        forbidden in the more specialized schema).\n\n        Args:\n            schema: schema name or class for which to return harvesters\n            include_children: Also include results for installed children\n            include_parents: Also include results for parent schemas\n\n        Returns:\n            Set of harvesters.\n        \"\"\"\n        schema_name = _schema_arg(schema).Plugin.name\n        ret = set(self._harvesters_for[schema_name])\n        if include_children:\n            for child in schemas.children(schema_name):\n                ret = ret.union(self.for_schema(child))\n        if include_parents:\n            for parent in schemas.parent_path(schema_name)[:-1]:\n                ret = ret.union(self.for_schema(parent))\n        return ret\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.PGHarvester.init_plugin","title":"init_plugin","text":"<pre><code>init_plugin(plugin)\n</code></pre> <p>Add harvester to harvester lookup table.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>@overrides\ndef init_plugin(self, plugin):\n\"\"\"Add harvester to harvester lookup table.\"\"\"\n    h_name = plugin.Plugin.name\n    schema = plugin.Plugin.returns\n    if schema.name not in self._harvesters_for:\n        self._harvesters_for[schema.name] = set()\n    self._harvesters_for[schema.name].add(h_name)\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.PGHarvester.for_schema","title":"for_schema","text":"<pre><code>for_schema(\n    schema: Union[str, MetadataSchema],\n    *,\n    include_children: bool = False,\n    include_parents: bool = False\n) -&gt; Set[str]\n</code></pre> <p>List installed harvesters for the given metadata schema.</p> <p>To extend the query to parent or child schemas, set <code>include_children</code> and/or <code>include_parents</code> accordingly.</p> <p>Harvesters for child schemas are always compatible with the schema. (assuming correct implementation of the child schemas),</p> <p>Harvesters for parent schemas can be incompatible in some circumstances (specifically, if the parent accepts values for some field that are forbidden in the more specialized schema).</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Union[str, MetadataSchema]</code> <p>schema name or class for which to return harvesters</p> required <code>include_children</code> <code>bool</code> <p>Also include results for installed children</p> <code>False</code> <code>include_parents</code> <code>bool</code> <p>Also include results for parent schemas</p> <code>False</code> <p>Returns:</p> Type Description <code>Set[str]</code> <p>Set of harvesters.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>def for_schema(\n    self,\n    schema: Union[str, MetadataSchema],\n    *,\n    include_children: bool = False,\n    include_parents: bool = False,\n) -&gt; Set[str]:\n\"\"\"List installed harvesters for the given metadata schema.\n\n    To extend the query to parent or child schemas, set `include_children`\n    and/or `include_parents` accordingly.\n\n    Harvesters for child schemas are always compatible with the schema.\n    (assuming correct implementation of the child schemas),\n\n    Harvesters for parent schemas can be incompatible in some circumstances\n    (specifically, if the parent accepts values for some field that are\n    forbidden in the more specialized schema).\n\n    Args:\n        schema: schema name or class for which to return harvesters\n        include_children: Also include results for installed children\n        include_parents: Also include results for parent schemas\n\n    Returns:\n        Set of harvesters.\n    \"\"\"\n    schema_name = _schema_arg(schema).Plugin.name\n    ret = set(self._harvesters_for[schema_name])\n    if include_children:\n        for child in schemas.children(schema_name):\n            ret = ret.union(self.for_schema(child))\n    if include_parents:\n        for parent in schemas.parent_path(schema_name)[:-1]:\n            ret = ret.union(self.for_schema(parent))\n    return ret\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.def_sidecar_func","title":"def_sidecar_func","text":"<pre><code>def_sidecar_func(path: Path)\n</code></pre> <p>Return sidecar filename for a given file path using the default convention.</p> <p>This means, the path gets a '_meta.yaml' suffix attached.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>def def_sidecar_func(path: Path):\n\"\"\"Return sidecar filename for a given file path using the default convention.\n\n    This means, the path gets a '_meta.yaml' suffix attached.\n    \"\"\"\n    return Path(f\"{str(path)}_meta.yaml\")\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.metadata_loader","title":"metadata_loader","text":"<pre><code>metadata_loader(\n    schema: Type[MetadataSchema],\n    *,\n    use_sidecar: bool = False,\n    sidecar_func: bool = None\n) -&gt; Type[MetadataLoader]\n</code></pre> <p>Return harvester for partial metadata files of specified schema.</p> <p>Will return an empty partial schema in case the file does not exist.</p> <p>The fields provided in the file must be valid, or an exception will be raised. This is to avoid the situation where a user intends to add or override harvester metadata, but the fields are silently ignored, leading to possible surprise and confusion.</p> <p>By default, the returned harvester will attempt to parse the provided file.</p> <p>Set <code>use_sidecar</code> if you intend to pass it filenames of data files, but want it to instead load metadata from a sidecar file.</p> <p>Provide a <code>sidecar_func</code> if you want to change the used sidecar file convention. By default, <code>def_sidecar_func</code> is used to transform the path.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>def metadata_loader(\n    schema: Type[MetadataSchema], *, use_sidecar: bool = False, sidecar_func=None\n) -&gt; Type[MetadataLoader]:\n\"\"\"Return harvester for partial metadata files of specified schema.\n\n    Will return an empty partial schema in case the file does not exist.\n\n    The fields provided in the file must be valid, or an exception will be raised.\n    This is to avoid the situation where a user intends to add or override\n    harvester metadata, but the fields are silently ignored, leading to possible\n    surprise and confusion.\n\n    By default, the returned harvester will attempt to parse the provided file.\n\n    Set `use_sidecar` if you intend to pass it filenames of data files, but\n    want it to instead load metadata from a sidecar file.\n\n    Provide a `sidecar_func` if you want to change the used sidecar file convention.\n    By default, `def_sidecar_func` is used to transform the path.\n    \"\"\"\n    used_sidecar_f = _identity\n    if use_sidecar:\n        used_sidecar_f = sidecar_func if sidecar_func else def_sidecar_func\n\n    class XLoader(MetadataLoader):\n        _partial_schema = schema.Partial\n        _sidecar_func = used_sidecar_f\n\n    name = f\"{schema.__name__}Loader\"\n    XLoader.__qualname__ = XLoader.__name__ = name\n    return XLoader\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.configure","title":"configure","text":"<pre><code>configure(\n    *harvesters: Union[Harvester, Type[Harvester]],\n    **kwargs: Union[Harvester, Type[Harvester]]\n)\n</code></pre> <p>Given a sequence of harvesters, configure them all at once.</p> <p>Can be used to set the same parameter in all of them easily.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>def configure(*harvesters: Union[Harvester, Type[Harvester]], **kwargs):\n\"\"\"Given a sequence of harvesters, configure them all at once.\n\n    Can be used to set the same parameter in all of them easily.\n    \"\"\"\n    return (h(**kwargs) for h in harvesters)\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.file_harvester_pipeline","title":"file_harvester_pipeline","text":"<pre><code>file_harvester_pipeline(\n    *hvs: Union[FileHarvester, Type[FileHarvester]]\n) -&gt; Callable[[Path], Any]\n</code></pre> <p>Generate a harvesting pipeline for a file.</p> <p>Parameters:</p> Name Type Description Default <code>hvs</code> <code>Union[FileHarvester, Type[FileHarvester]]</code> <p>FileHarvester classes or pre-configured instances to use. The passed objects must be preconfigured as needed, except for fixing a filepath (it will be overwritten).</p> <code>()</code> <p>Returns:</p> Type Description <code>Callable[[Path], Any]</code> <p>Function that takes a file path and will return</p> <code>Callable[[Path], Any]</code> <p>the harvesters configured for the passed path.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>def file_harvester_pipeline(\n    *hvs: Union[FileHarvester, Type[FileHarvester]]\n) -&gt; Callable[[Path], Any]:\n\"\"\"Generate a harvesting pipeline for a file.\n\n    Args:\n        hvs: FileHarvester classes or pre-configured instances to use.\n            The passed objects must be preconfigured as needed,\n            except for fixing a filepath (it will be overwritten).\n\n    Returns:\n        Function that takes a file path and will return\n        the harvesters configured for the passed path.\n    \"\"\"\n    return lambda path: configure(*hvs, filepath=path)\n</code></pre>"},{"location":"reference/metador_core/harvester/#metador_core.harvester.harvest","title":"harvest","text":"<pre><code>harvest(\n    schema: Type[S],\n    sources: Iterable[Union[Path, Harvester]],\n    *,\n    ignore_invalid: bool = False,\n    return_partial: bool = False\n) -&gt; S\n</code></pre> <p>Run a harvesting pipeline and return combined results.</p> <p>Will run the harvesters in the passed order, combining results.</p> <p>In general you can expect that if two harvesters provide the same field, the newer value by a later harvester will overwrite an existing one from an earlier harvester, or the values are combined in a suitable way.</p> <p>If converting some Harvester result to the desired schema fails, a <code>ValidationError</code> will be raised.  To change that behaviour, set <code>ignore_invalid</code>, will make sure that suitable fields are still used, even if the given object as a whole is not fully parsable.</p> <p>Note that <code>ignore_invalid</code> only affects the conversion AFTER running the harvesters, it does NOT affect the way the harvesters treat invalid metadata.</p> <p>By default, it is assumed that the result of the whole pipeline can be converted into a fully valid non-partial schema instance. If this final conversion fails, an exception will be raised. To prevent this conversion, set <code>return_partial</code>. In that case, you will get the partial instance as-is and can manually call <code>from_partial()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>Type[S]</code> <p>Class of schema to be returned.</p> required <code>sources</code> <code>Iterable[Union[Path, Harvester]]</code> <p>List of sources (Paths or Harvester instances).</p> required <code>ignore_invalid</code> <code>bool</code> <p>Whether to ignore invalid fields in Harvester outputs.</p> <code>False</code> <code>return_partial</code> <code>bool</code> <p>Whether to return raw partial instance (default: False).</p> <code>False</code> <p>Returns:</p> Type Description <code>S</code> <p>Metadata object with combined results.</p> Source code in <code>src/metador_core/harvester/__init__.py</code> <pre><code>def harvest(\n    schema: Type[S],\n    sources: Iterable[Union[Path, Harvester]],\n    *,\n    ignore_invalid: bool = False,\n    return_partial: bool = False,\n) -&gt; S:\n\"\"\"Run a harvesting pipeline and return combined results.\n\n    Will run the harvesters in the passed order, combining results.\n\n    In general you can expect that if two harvesters provide the same field,\n    the newer value by a later harvester will overwrite an existing one from an\n    earlier harvester, or the values are combined in a suitable way.\n\n    If converting some Harvester result to the desired schema fails,\n    a `ValidationError` will be raised.  To change that behaviour,\n    set `ignore_invalid`, will make sure that suitable fields are still used,\n    even if the given object as a whole is not fully parsable.\n\n    Note that `ignore_invalid` only affects the conversion AFTER running the\n    harvesters, it does NOT affect the way the harvesters treat invalid metadata.\n\n    By default, it is assumed that the result of the whole pipeline can be\n    converted into a fully valid non-partial schema instance.\n    If this final conversion fails, an exception will be raised.\n    To prevent this conversion, set `return_partial`. In that case, you will\n    get the partial instance as-is and can manually call `from_partial()`.\n\n    Args:\n        schema: Class of schema to be returned.\n        sources: List of sources (Paths or Harvester instances).\n        ignore_invalid: Whether to ignore invalid fields in Harvester outputs.\n        return_partial: Whether to return raw partial instance (default: False).\n\n    Returns:\n        Metadata object with combined results.\n    \"\"\"\n\n    def cast(meta):\n        # configure cast to ignore or not ignore invalid fields\n        return schema.Partial.cast(meta, ignore_invalid=ignore_invalid)\n\n    # collect partial metadata (NOTE: in principle, this could be parallelized)\n    results = map(lambda s: cast(_harvest_source(schema, s)), sources)\n\n    # accumulate collected and casted results in provided order\n    merged = schema.Partial.merge(*results)\n\n    # retrieve (completed) metadata model\n    return merged if return_partial else merged.from_partial()\n</code></pre>"},{"location":"reference/metador_core/harvester/common/","title":"common","text":""},{"location":"reference/metador_core/harvester/common/#metador_core.harvester.common.FileMetaHarvester","title":"FileMetaHarvester","text":"<p>             Bases: <code>FileHarvester</code></p> <p>Default harvester for basic common.file metadata.</p> <p>Harvests file name, file size, mimetype and hashsum of the file.</p> Source code in <code>src/metador_core/harvester/common.py</code> <pre><code>class FileMetaHarvester(FileHarvester):\n\"\"\"Default harvester for basic common.file metadata.\n\n    Harvests file name, file size, mimetype and hashsum of the file.\n    \"\"\"\n\n    class Plugin:\n        name = \"core.file.generic\"\n        version = (0, 1, 0)\n        returns = schemas.PluginRef(name=\"core.file\", version=(0, 1, 0))\n\n    def run(self):\n        path = self.args.filepath\n\n        sz = path.stat().st_size\n        hs = hashsum(open(path, \"rb\"), \"sha256\")\n        mt = magic.from_file(path, mime=True)\n        return self.schema(\n            filename=path.name, contentSize=sz, sha256=hs, encodingFormat=mt\n        )\n</code></pre>"},{"location":"reference/metador_core/harvester/common/#metador_core.harvester.common.ImageFileMetaHarvester","title":"ImageFileMetaHarvester","text":"<p>             Bases: <code>FileHarvester</code></p> <p>Harvester to obtain dimensions (width and height) of an image file.</p> Source code in <code>src/metador_core/harvester/common.py</code> <pre><code>class ImageFileMetaHarvester(FileHarvester):\n\"\"\"Harvester to obtain dimensions (width and height) of an image file.\"\"\"\n\n    class Plugin:\n        name = \"core.imagefile.dim\"\n        version = (0, 1, 0)\n        returns = schemas.PluginRef(name=\"core.imagefile\", version=(0, 1, 0))\n\n    def run(self):\n        path = self.args.filepath\n\n        with Image.open(path) as img:\n            width, height = img.size\n        return self.schema(width=width, height=height)\n</code></pre>"},{"location":"reference/metador_core/ih5/","title":"ih5","text":"<p>Immutable HDF5-based multi-container records.</p> <p>This API supports a subset of h5py, namely reading, writing and deleting groups, values and attributes. It does not support hard, symbolic or external links, so the data must be self-contained and strictly hierarchical.</p> <p>Correspondence of IH5 overlay classes and raw h5py classes:</p> IH5 API h5py <code>IH5Record</code> h5py.File <code>IH5Group</code> h5py.Group <code>IH5Dataset</code> h5py.Dataset <code>IH5AttributeManager</code> h5py.AttributeManager <p>Anything that can be done to an IH5 Group, Dataset or AttributeManager can also be done to the h5py counterparts.</p> <p>If you are missing some functionality from h5py in the overlay classes, please contact us or open an issue and we will see whether and how the missing methods can be added in a coherent manner matching the IH5 semantics.</p>"},{"location":"reference/metador_core/ih5/#metador_core.ih5--getting-started","title":"Getting Started","text":"<p>A quite minimal working example:</p> <pre><code># just use IH5Record instead of h5py.File:\nfrom metador_core.ih5.container import IH5Record\n\n\n# initial creation:\nwith IH5Record(\"record_name\", \"w\") as ds:\n    # A new record is automatically in writable mode,\n    # so let us write some data, just like with h5py:\n    ds[\"foo/bar\"] = \"something\"\n    ds[\"foo\"].attrs[\"attribute\"] = 123\n\n# updating the record we created later on:\nwith IH5Record(\"record_name\", \"r\") as ds:\n    # A record is opened in read-only mode, so\n    # before we can add, modify or delete anything, we need to call:\n    ds.create_patch()\n\n    del ds[\"foo/bar\"]\n    ds[\"/foo/bar/baz\"] = [1, 2, 3]\n</code></pre> <p>You SHOULD use <code>IH5Record</code> as a context manager, like in the example above. If you for some reason do not, be aware that you MUST manually call <code>commit()</code> to finilize your changes. You can also just use <code>close()</code>, which will also call <code>commit()</code> for you. Consider it good practice calling <code>commit()</code> manually after completing your updates of the record.</p>"},{"location":"reference/metador_core/ih5/container/","title":"container","text":"<p>Re-exports for users of the IH5 variants.</p>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Record","title":"IH5Record","text":"<p>             Bases: <code>IH5Group</code></p> <p>Class representing a record, which consists of a collection of immutable files.</p> <p>One file is a base container (with no linked predecessor state), the remaining files are a linear sequence of patch containers.</p> <p>Runtime invariants to be upheld before/after each method call (after init):</p> <ul> <li>all files of an instance are open for reading (until <code>close()</code> is called)</li> <li>all files in <code>__files__</code> are in patch index order</li> <li>at most one file is open in writable mode (if any, it is the last one)</li> <li>modifications are possible only after <code>create_patch</code> was called     and until <code>commit_patch</code> or <code>discard_patch</code> was called, and at no other time</li> </ul> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>class IH5Record(IH5Group):\n\"\"\"Class representing a record, which consists of a collection of immutable files.\n\n    One file is a base container (with no linked predecessor state),\n    the remaining files are a linear sequence of patch containers.\n\n    Runtime invariants to be upheld before/after each method call (after __init__):\n\n    * all files of an instance are open for reading (until `close()` is called)\n    * all files in `__files__` are in patch index order\n    * at most one file is open in writable mode (if any, it is the last one)\n    * modifications are possible only after `create_patch` was called\n        and until `commit_patch` or `discard_patch` was called, and at no other time\n    \"\"\"\n\n    # Characters that may appear in a record name.\n    # (to be put into regex [..] symbol braces)\n    _ALLOWED_NAME_CHARS = r\"A-Za-z0-9\\-\"\n\n    # filenames for a record named NAME are of the shape:\n    # NAME[&lt;PATCH_INFIX&gt;.*]?&lt;FILE_EXT&gt;\n    # NOTE: the first symbol of these must be one NOT in ALLOWED_NAME_CHARS!\n    # This constraint is needed for correctly filtering filenames\n    _PATCH_INFIX = \".p\"\n    _FILE_EXT = \".ih5\"\n\n    # core \"wrapped\" objects\n    __files__: List[h5py.File]\n\n    # attributes\n    _closed: bool  # True after close()\n    _allow_patching: bool  # false iff opened with \"r\"\n    _ublocks: Dict[Path, IH5UserBlock]  # in-memory copy of HDF5 user blocks\n\n    def __new__(cls, *args, **kwargs):\n        ret = super().__new__(cls)\n        ret._allow_patching = True\n        ret.__files__ = []\n        return ret\n\n    def __eq__(self, o) -&gt; bool:\n        if not isinstance(o, IH5Group):\n            return False\n        return self._record._files == o._record._files\n\n    @property\n    def _has_writable(self):\n\"\"\"Return True iff an uncommitted patch exists.\"\"\"\n        if not self.__files__:\n            return False\n        f = self.__files__[-1]\n        return bool(f) and f.mode == \"r+\"\n\n    @classmethod\n    def _is_valid_record_name(cls, name: str) -&gt; bool:\n\"\"\"Return whether a record name is valid.\"\"\"\n        return re.match(f\"^[{cls._ALLOWED_NAME_CHARS}]+$\", name) is not None\n\n    @classmethod\n    def _base_filename(cls, record_path: Path) -&gt; Path:\n\"\"\"Given a record path, return path to canonical base container name.\"\"\"\n        return Path(f\"{record_path}{cls._FILE_EXT}\")\n\n    @classmethod\n    def _infer_name(cls, record_path: Path) -&gt; str:\n        return record_path.name.split(cls._FILE_EXT)[0].split(cls._PATCH_INFIX)[0]\n\n    def _next_patch_filepath(self) -&gt; Path:\n\"\"\"Compute filepath for the next patch based on the previous one.\"\"\"\n        path = Path(self.__files__[0].filename)\n        parent = path.parent\n        patch_index = self._ublock(-1).patch_index + 1\n        res = f\"{parent}/{self._infer_name(path)}{self._PATCH_INFIX}{patch_index}{self._FILE_EXT}\"\n        return Path(res)\n\n    def _ublock(self, obj: Union[h5py.File, int]) -&gt; IH5UserBlock:\n\"\"\"Return the parsed user block of a container file.\"\"\"\n        f: h5py.File = obj if isinstance(obj, h5py.File) else self.__files__[obj]\n        return self._ublocks[Path(f.filename)]\n\n    def _set_ublock(self, obj: Union[h5py.File, int], ub: IH5UserBlock):\n        f: h5py.File = obj if isinstance(obj, h5py.File) else self.__files__[obj]\n        self._ublocks[Path(f.filename)] = ub\n\n    @classmethod\n    def _new_container(cls, path: Path, ub: IH5UserBlock) -&gt; h5py.File:\n\"\"\"Initialize a fresh container file with reserved user block.\"\"\"\n        # create if does not exist, fail if it does\n        f = h5py.File(path, mode=\"x\", userblock_size=USER_BLOCK_SIZE)\n        # close to pre-fill userblock\n        f.close()\n        ub.save(path)\n        # reopen the container file\n        return h5py.File(path, \"r+\")\n\n    def _check_ublock(\n        self,\n        filename: Union[str, Path],\n        ub: IH5UserBlock,\n        prev: Optional[IH5UserBlock] = None,\n        check_hashsum: bool = True,\n    ):\n\"\"\"Check given container file.\n\n        If `prev` block is given, assumes that `ub` is from a patch container,\n        otherwise from base container.\n        \"\"\"\n        filename = Path(filename)\n        # check presence+validity of record uuid (should be the same for all)\n        if ub.record_uuid != self.ih5_uuid:\n            msg = \"'record_uuid' inconsistent! Mixed up records?\"\n            raise ValueError(f\"{filename}: {msg}\")\n\n        # hash must match with HDF5 content (i.e. integrity check)\n        if check_hashsum and ub.hdf5_hashsum is None:\n            msg = \"hdf5_checksum is missing!\"\n            raise ValueError(f\"{filename}: {msg}\")\n        if ub.hdf5_hashsum is not None:\n            chksum = hashsum_file(filename, skip_bytes=USER_BLOCK_SIZE)\n            if ub.hdf5_hashsum != chksum:\n                msg = \"file has been modified, stored and computed checksum are different!\"\n                raise ValueError(f\"{filename}: {msg}\")\n\n        # check patch chain structure\n        if prev is not None:\n            if ub.patch_index &lt;= prev.patch_index:\n                msg = \"patch container must have greater index than predecessor!\"\n                raise ValueError(f\"{filename}: {msg}\")\n            if ub.prev_patch is None:\n                msg = \"patch must have an attribute 'prev_patch'!\"\n                raise ValueError(f\"{filename}: {msg}\")\n            # claimed predecessor uuid must match with the predecessor by index\n            # (can compare as strings directly, as we checked those already)\n            if ub.prev_patch != prev.patch_uuid:\n                msg = f\"patch for {ub.prev_patch}, but predecessor is {prev.patch_uuid}\"\n                raise ValueError(f\"{filename}: {msg}\")\n\n    def _expect_open(self):\n        if self._closed:\n            raise ValueError(\"Record is not open!\")\n\n    def _clear(self):\n\"\"\"Clear all contents of the record.\"\"\"\n        for k in self.attrs.keys():\n            del self.attrs[k]\n        for k in self.keys():\n            del self[k]\n\n    def _is_empty(self) -&gt; bool:\n\"\"\"Return whether this record currently contains any data.\"\"\"\n        return not self.attrs.keys() and not self.keys()\n\n    @classmethod\n    def _create(cls: Type[T], record: Union[Path, str], truncate: bool = False) -&gt; T:\n\"\"\"Create a new record consisting of a base container.\n\n        The base container is exposed as the `writable` container.\n        \"\"\"\n        record = Path(record)  # in case it was a str\n        if not cls._is_valid_record_name(record.name):\n            raise ValueError(f\"Invalid record name: '{record.name}'\")\n        path = cls._base_filename(record)\n\n        # if overwrite flag is set, check and remove old record if present\n        if truncate and path.is_file():\n            cls.delete_files(record)\n\n        # create new container\n        ret = cls.__new__(cls)\n        super().__init__(ret, ret)\n        ret._closed = False\n\n        ub = IH5UserBlock.create(prev=None)\n        ret._ublocks = {path: ub}\n\n        ret.__files__ = [cls._new_container(path, ub)]\n        return ret\n\n    @classmethod\n    def _open(cls: Type[T], paths: List[Path], **kwargs) -&gt; T:\n\"\"\"Open a record consisting of a base container + possible set of patches.\n\n        Expects a set of full file paths forming a valid record.\n        Will throw an exception in case of a detected inconsistency.\n\n        Will open latest patch in writable mode if it lacks a hdf5 checksum.\n        \"\"\"\n        if not paths:\n            raise ValueError(\"Cannot open empty list of containers!\")\n        allow_baseless: bool = kwargs.pop(\"allow_baseless\", False)\n\n        ret = cls.__new__(cls)\n        super().__init__(ret, ret)\n        ret._closed = False\n\n        ret._ublocks = {Path(path): IH5UserBlock.load(path) for path in paths}\n        # files, sorted  by patch index order (important!)\n        # if something is wrong with the indices, this will throw an exception.\n        ret.__files__ = [h5py.File(path, \"r\") for path in paths]\n        ret.__files__.sort(key=lambda f: ret._ublock(f).patch_index)\n        # ----\n        has_patches: bool = len(ret.__files__) &gt; 1\n\n        # check containers and relationship to each other:\n\n        # check first container (it could be a base container and it has no predecessor)\n        if not allow_baseless and ret._ublock(0).prev_patch is not None:\n            msg = \"base container must not have attribute 'prev_patch'!\"\n            raise ValueError(f\"{ret.__files__[0].filename}: {msg}\")\n        ret._check_ublock(ret.__files__[0].filename, ret._ublock(0), None, has_patches)\n\n        # check patches except last one (with checking the hashsum)\n        for i in range(1, len(ret.__files__) - 1):\n            filename = ret.__files__[i].filename\n            ret._check_ublock(filename, ret._ublock(i), ret._ublock(i - 1), True)\n        if has_patches:  # check latest patch (without checking hashsum)\n            ret._check_ublock(\n                ret.__files__[-1].filename, ret._ublock(-1), ret._ublock(-2), False\n            )\n\n        # now check whether the last container (patch or base or whatever) has a checksum\n        if ret._ublock(-1).hdf5_hashsum is None:\n            if kwargs.pop(\"reopen_incomplete_patch\", False):\n                # if opening in writable mode, allow to complete the patch\n                f = ret.__files__[-1]\n                path = ret.__files__[-1].filename\n                f.close()\n                ret.__files__[-1] = h5py.File(Path(path), \"r+\")\n\n        # additional sanity check: container uuids must be all distinct\n        cn_uuids = {ret._ublock(f).patch_uuid for f in ret.__files__}\n        if len(cn_uuids) != len(ret.__files__):\n            raise ValueError(\"Some patch_uuid is not unique, invalid file set!\")\n        # all looks good\n        return ret\n\n    # ---- public attributes and interface ----\n\n    @property\n    def ih5_uuid(self) -&gt; UUID:\n\"\"\"Return the common record UUID of the set of containers.\"\"\"\n        return self._ublock(0).record_uuid\n\n    @property\n    def ih5_files(self) -&gt; List[Path]:\n\"\"\"List of container filenames this record consists of.\"\"\"\n        return [Path(f.filename) for f in self.__files__]\n\n    @property\n    def ih5_meta(self) -&gt; List[IH5UserBlock]:\n\"\"\"Return user block metadata, in container patch order.\"\"\"\n        return [self._ublock(i).copy() for i in range(len(self.__files__))]\n\n    @classmethod\n    def find_files(cls, record: Path) -&gt; List[Path]:\n\"\"\"Return file names that look like they belong to the same record.\n\n        This operation is based on purely syntactic pattern matching on file names\n        that follow the default naming convention.\n\n        Given a path `/foo/bar`, will find all containers in directory\n        `/foo` whose name starts with `bar` followed by the correct file extension(s),\n        such as `/foo/bar.ih5` and `/foo/bar.p1.ih5`.\n        \"\"\"\n        record = Path(record)  # in case it was a str\n        if not cls._is_valid_record_name(record.name):\n            raise ValueError(f\"Invalid record name: '{record.name}'\")\n\n        globstr = f\"{record.name}*{cls._FILE_EXT}\"  # rough wildcard pattern\n        # filter out possible false positives (i.e. foobar* matching foo* as well)\n        return [\n            p\n            for p in record.parent.glob(globstr)\n            if re.match(f\"^{record.name}[^{cls._ALLOWED_NAME_CHARS}]\", p.name)\n        ]\n\n    @classmethod\n    def list_records(cls, dir: Path) -&gt; List[Path]:\n\"\"\"Return paths of records found in the given directory.\n\n        Will NOT recurse into subdirectories.\n\n        This operation is based on purely syntactic pattern matching on file names\n        that follow the default naming convention (i.e. just as `find_files`).\n\n        Returned paths can be used as-is for opening the (supposed) record.\n        \"\"\"\n        dir = Path(dir)  # in case it was a str\n        if not dir.is_dir():\n            raise ValueError(f\"'{dir}' is not a directory\")\n\n        ret = []\n        namepat = f\"[{cls._ALLOWED_NAME_CHARS}]+(?=[^{cls._ALLOWED_NAME_CHARS}])\"\n        for p in dir.glob(f\"*{cls._FILE_EXT}\"):\n            if m := re.match(namepat, p.name):\n                ret.append(m[0])\n        return list(map(lambda name: dir / name, set(ret)))\n\n    def __init__(\n        self, record: Union[str, Path, List[Path]], mode: OpenMode = \"r\", **kwargs\n    ):\n\"\"\"Open or create a record.\n\n        This method uses `find_files` to infer the correct set of files syntactically.\n\n        The open mode semantics are the same as for h5py.File.\n\n        If the mode is 'r', then creating, committing or discarding patches is disabled.\n\n        If the mode is 'a' or 'r+', then a new patch will be created in case the latest\n        patch has already been committed.\n        \"\"\"\n        super().__init__(self)\n\n        if isinstance(record, list):\n            if mode[0] == \"w\" or mode == \"x\":\n                raise ValueError(\"Pass a prefix path for creating or overwriting!\")\n            paths = record\n        else:\n            paths = None\n            path: Path = Path(record)\n\n        if mode not in _OPEN_MODES:\n            raise ValueError(f\"Unknown file open mode: {mode}\")\n\n        if mode[0] == \"w\" or mode == \"x\":\n            # create new or overwrite to get new\n            ret = self._create(path, truncate=(mode == \"w\"))\n            self.__dict__.update(ret.__dict__)\n            return\n\n        if mode == \"a\" or mode[0] == \"r\":\n            if not paths:  # user passed a path prefix -&gt; find files\n                paths = self.find_files(path)  # type: ignore\n\n            if not paths:  # no files were found\n                if mode != \"a\":  # r/r+ need existing containers\n                    raise FileNotFoundError(f\"No files found for record: {path}\")\n                else:  # 'a' means create new if not existing (will be writable)\n                    ret = self._create(path, truncate=False)\n                    self.__dict__.update(ret.__dict__)\n                    return\n\n            # open existing (will be ro if everything is fine, writable if latest patch was uncommitted)\n            want_rw = mode != \"r\"\n            ret = self._open(paths, reopen_incomplete_patch=want_rw, **kwargs)\n            self.__dict__.update(ret.__dict__)\n            self._allow_patching = want_rw\n\n            if want_rw and not self._has_writable:\n                # latest patch was completed correctly -&gt; make writable by creating new patch\n                self.create_patch()\n\n    @property\n    def mode(self) -&gt; Literal[\"r\", \"r+\"]:\n        return \"r+\" if self._allow_patching else \"r\"\n\n    def close(self, commit: bool = True) -&gt; None:\n\"\"\"Close all files that belong to this record.\n\n        If there exists an uncommited patch, it will be committed\n        (unless `commit` is set to false).\n\n        After this, the object may not be used anymore.\n        \"\"\"\n        if self._closed:\n            return  # nothing to do\n\n        if self._has_writable and commit:\n            self.commit_patch()\n        for f in self.__files__:\n            f.close()\n        self.__files__ = []\n        self._closed = True\n\n    def _expect_not_ro(self):\n        if self.mode == \"r\":\n            raise ValueError(\"The container is opened as read-only!\")\n\n    def create_patch(self) -&gt; None:\n\"\"\"Create a new patch in order to update the record.\"\"\"\n        self._expect_open()\n        self._expect_not_ro()\n        if self._has_writable:\n            raise ValueError(\"There already exists a writable container, commit first!\")\n\n        path = self._next_patch_filepath()\n        ub = IH5UserBlock.create(prev=self._ublock(-1))\n        self.__files__.append(self._new_container(path, ub))\n        self._ublocks[path] = ub\n\n    def _delete_latest_container(self) -&gt; None:\n\"\"\"Discard the current writable container (patch or base).\"\"\"\n        cfile = self.__files__.pop()\n        fn = cfile.filename\n        del self._ublocks[Path(fn)]\n        cfile.close()\n        Path(fn).unlink()\n\n    def discard_patch(self) -&gt; None:\n\"\"\"Discard the current incomplete patch container.\"\"\"\n        self._expect_open()\n        self._expect_not_ro()\n        if not self._has_writable:\n            raise ValueError(\"No patch to discard!\")\n        if len(self.__files__) == 1:\n            raise ValueError(\"Cannot discard base container! Just delete the file!\")\n            # reason: the base container provides record_uuid,\n            # destroying it makes this object inconsistent / breaks invariants\n            # so if this is done, it should not be used anymore.\n        return self._delete_latest_container()\n\n    def commit_patch(self, **kwargs) -&gt; None:\n\"\"\"Complete the current writable container (base or patch) for the record.\n\n        Will perform checks on the new container and throw an exception on failure.\n\n        After committing the patch is completed and cannot be edited anymore, so\n        any further modifications must go into a new patch.\n        \"\"\"\n        if kwargs:\n            raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n\n        self._expect_open()\n        self._expect_not_ro()\n        if not self._has_writable:\n            raise ValueError(\"No patch to commit!\")\n        cfile = self.__files__[-1]\n        filepath = Path(cfile.filename)\n        cfile.close()  # must close it now, as we will write outside of HDF5 next\n\n        # compute checksum, write user block\n        chksum = hashsum_file(filepath, skip_bytes=USER_BLOCK_SIZE)\n        self._ublocks[filepath].hdf5_hashsum = QualHashsumStr(chksum)\n        self._ublocks[filepath].save(filepath)\n\n        # reopen the container file now as read-only\n        self.__files__[-1] = h5py.File(filepath, \"r\")\n\n    def _fixes_after_merge(self, merged_file, ub):\n\"\"\"Run hook for subclasses into merge process.\n\n        The method is called after creating the merged container, but before\n        updating its user block on disk.\n\n        The passed userblock is a prepared userblock with updated HDF5 hashsum for the\n        merged container and adapted prev_patch field, as will it be written to the file.\n        Additional changes done to it in-place will be included.\n\n        The passed filename can be used to perform additional necessary actions.\n        \"\"\"\n\n    def merge_files(self, target: Path) -&gt; Path:\n\"\"\"Given a path with a record name, merge current record into new container.\n\n        Returns new resulting container.\n        \"\"\"\n        self._expect_open()\n        if self._has_writable:\n            raise ValueError(\"Cannot merge, please commit or discard your changes!\")\n\n        with type(self)(target, \"x\") as ds:\n            source_node = self[\"/\"]\n            target_node = ds[\"/\"]\n            for k, v in source_node.attrs.items():  # copy root attributes\n                target_node.attrs[k] = v\n            for name in source_node.keys():  # copy each entity (will recurse)\n                h5_copy_from_to(source_node[name], target_node, name)\n\n            cfile = ds.ih5_files[0]  # store filename to override userblock afterwards\n\n        # compute new merged userblock\n        ub = self._ublock(-1).copy(update={\"prev_patch\": self._ublock(0).prev_patch})\n        # update hashsum with saved new merged hdf5 payload\n        chksum = hashsum_file(cfile, skip_bytes=USER_BLOCK_SIZE)\n        ub.hdf5_hashsum = QualHashsumStr(chksum)\n\n        self._fixes_after_merge(cfile, ub)  # for subclass hooks\n\n        self._set_ublock(-1, ub)\n        ub.save(cfile)\n        return cfile\n\n    @classmethod\n    def delete_files(cls, record: Path):\n\"\"\"Irreversibly(!) delete all containers matching the record path.\n\n        This object is invalid after this operation.\n        \"\"\"\n        for file in cls.find_files(record):\n            file.unlink()\n\n    def __repr__(self):\n        return f\"&lt;IH5 record (mode {self.mode}) {self.__files__}&gt;\"\n\n    # ---- context manager support (i.e. to use `with`) ----\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, ex_type, ex_value, ex_traceback):\n        # this will ensure that commit_patch() is called and the files are closed\n        self.close()\n</code></pre>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Record.ih5_uuid","title":"ih5_uuid  <code>property</code>","text":"<pre><code>ih5_uuid: UUID\n</code></pre> <p>Return the common record UUID of the set of containers.</p>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Record.ih5_files","title":"ih5_files  <code>property</code>","text":"<pre><code>ih5_files: List[Path]\n</code></pre> <p>List of container filenames this record consists of.</p>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Record.ih5_meta","title":"ih5_meta  <code>property</code>","text":"<pre><code>ih5_meta: List[IH5UserBlock]\n</code></pre> <p>Return user block metadata, in container patch order.</p>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Record.find_files","title":"find_files  <code>classmethod</code>","text":"<pre><code>find_files(record: Path) -&gt; List[Path]\n</code></pre> <p>Return file names that look like they belong to the same record.</p> <p>This operation is based on purely syntactic pattern matching on file names that follow the default naming convention.</p> <p>Given a path <code>/foo/bar</code>, will find all containers in directory <code>/foo</code> whose name starts with <code>bar</code> followed by the correct file extension(s), such as <code>/foo/bar.ih5</code> and <code>/foo/bar.p1.ih5</code>.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>@classmethod\ndef find_files(cls, record: Path) -&gt; List[Path]:\n\"\"\"Return file names that look like they belong to the same record.\n\n    This operation is based on purely syntactic pattern matching on file names\n    that follow the default naming convention.\n\n    Given a path `/foo/bar`, will find all containers in directory\n    `/foo` whose name starts with `bar` followed by the correct file extension(s),\n    such as `/foo/bar.ih5` and `/foo/bar.p1.ih5`.\n    \"\"\"\n    record = Path(record)  # in case it was a str\n    if not cls._is_valid_record_name(record.name):\n        raise ValueError(f\"Invalid record name: '{record.name}'\")\n\n    globstr = f\"{record.name}*{cls._FILE_EXT}\"  # rough wildcard pattern\n    # filter out possible false positives (i.e. foobar* matching foo* as well)\n    return [\n        p\n        for p in record.parent.glob(globstr)\n        if re.match(f\"^{record.name}[^{cls._ALLOWED_NAME_CHARS}]\", p.name)\n    ]\n</code></pre>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Record.list_records","title":"list_records  <code>classmethod</code>","text":"<pre><code>list_records(dir: Path) -&gt; List[Path]\n</code></pre> <p>Return paths of records found in the given directory.</p> <p>Will NOT recurse into subdirectories.</p> <p>This operation is based on purely syntactic pattern matching on file names that follow the default naming convention (i.e. just as <code>find_files</code>).</p> <p>Returned paths can be used as-is for opening the (supposed) record.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>@classmethod\ndef list_records(cls, dir: Path) -&gt; List[Path]:\n\"\"\"Return paths of records found in the given directory.\n\n    Will NOT recurse into subdirectories.\n\n    This operation is based on purely syntactic pattern matching on file names\n    that follow the default naming convention (i.e. just as `find_files`).\n\n    Returned paths can be used as-is for opening the (supposed) record.\n    \"\"\"\n    dir = Path(dir)  # in case it was a str\n    if not dir.is_dir():\n        raise ValueError(f\"'{dir}' is not a directory\")\n\n    ret = []\n    namepat = f\"[{cls._ALLOWED_NAME_CHARS}]+(?=[^{cls._ALLOWED_NAME_CHARS}])\"\n    for p in dir.glob(f\"*{cls._FILE_EXT}\"):\n        if m := re.match(namepat, p.name):\n            ret.append(m[0])\n    return list(map(lambda name: dir / name, set(ret)))\n</code></pre>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Record.__init__","title":"__init__","text":"<pre><code>__init__(\n    record: Union[str, Path, List[Path]],\n    mode: OpenMode = \"r\",\n    **kwargs: OpenMode\n)\n</code></pre> <p>Open or create a record.</p> <p>This method uses <code>find_files</code> to infer the correct set of files syntactically.</p> <p>The open mode semantics are the same as for h5py.File.</p> <p>If the mode is 'r', then creating, committing or discarding patches is disabled.</p> <p>If the mode is 'a' or 'r+', then a new patch will be created in case the latest patch has already been committed.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def __init__(\n    self, record: Union[str, Path, List[Path]], mode: OpenMode = \"r\", **kwargs\n):\n\"\"\"Open or create a record.\n\n    This method uses `find_files` to infer the correct set of files syntactically.\n\n    The open mode semantics are the same as for h5py.File.\n\n    If the mode is 'r', then creating, committing or discarding patches is disabled.\n\n    If the mode is 'a' or 'r+', then a new patch will be created in case the latest\n    patch has already been committed.\n    \"\"\"\n    super().__init__(self)\n\n    if isinstance(record, list):\n        if mode[0] == \"w\" or mode == \"x\":\n            raise ValueError(\"Pass a prefix path for creating or overwriting!\")\n        paths = record\n    else:\n        paths = None\n        path: Path = Path(record)\n\n    if mode not in _OPEN_MODES:\n        raise ValueError(f\"Unknown file open mode: {mode}\")\n\n    if mode[0] == \"w\" or mode == \"x\":\n        # create new or overwrite to get new\n        ret = self._create(path, truncate=(mode == \"w\"))\n        self.__dict__.update(ret.__dict__)\n        return\n\n    if mode == \"a\" or mode[0] == \"r\":\n        if not paths:  # user passed a path prefix -&gt; find files\n            paths = self.find_files(path)  # type: ignore\n\n        if not paths:  # no files were found\n            if mode != \"a\":  # r/r+ need existing containers\n                raise FileNotFoundError(f\"No files found for record: {path}\")\n            else:  # 'a' means create new if not existing (will be writable)\n                ret = self._create(path, truncate=False)\n                self.__dict__.update(ret.__dict__)\n                return\n\n        # open existing (will be ro if everything is fine, writable if latest patch was uncommitted)\n        want_rw = mode != \"r\"\n        ret = self._open(paths, reopen_incomplete_patch=want_rw, **kwargs)\n        self.__dict__.update(ret.__dict__)\n        self._allow_patching = want_rw\n\n        if want_rw and not self._has_writable:\n            # latest patch was completed correctly -&gt; make writable by creating new patch\n            self.create_patch()\n</code></pre>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Record.close","title":"close","text":"<pre><code>close(commit: bool = True) -&gt; None\n</code></pre> <p>Close all files that belong to this record.</p> <p>If there exists an uncommited patch, it will be committed (unless <code>commit</code> is set to false).</p> <p>After this, the object may not be used anymore.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def close(self, commit: bool = True) -&gt; None:\n\"\"\"Close all files that belong to this record.\n\n    If there exists an uncommited patch, it will be committed\n    (unless `commit` is set to false).\n\n    After this, the object may not be used anymore.\n    \"\"\"\n    if self._closed:\n        return  # nothing to do\n\n    if self._has_writable and commit:\n        self.commit_patch()\n    for f in self.__files__:\n        f.close()\n    self.__files__ = []\n    self._closed = True\n</code></pre>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Record.create_patch","title":"create_patch","text":"<pre><code>create_patch() -&gt; None\n</code></pre> <p>Create a new patch in order to update the record.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def create_patch(self) -&gt; None:\n\"\"\"Create a new patch in order to update the record.\"\"\"\n    self._expect_open()\n    self._expect_not_ro()\n    if self._has_writable:\n        raise ValueError(\"There already exists a writable container, commit first!\")\n\n    path = self._next_patch_filepath()\n    ub = IH5UserBlock.create(prev=self._ublock(-1))\n    self.__files__.append(self._new_container(path, ub))\n    self._ublocks[path] = ub\n</code></pre>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Record.discard_patch","title":"discard_patch","text":"<pre><code>discard_patch() -&gt; None\n</code></pre> <p>Discard the current incomplete patch container.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def discard_patch(self) -&gt; None:\n\"\"\"Discard the current incomplete patch container.\"\"\"\n    self._expect_open()\n    self._expect_not_ro()\n    if not self._has_writable:\n        raise ValueError(\"No patch to discard!\")\n    if len(self.__files__) == 1:\n        raise ValueError(\"Cannot discard base container! Just delete the file!\")\n        # reason: the base container provides record_uuid,\n        # destroying it makes this object inconsistent / breaks invariants\n        # so if this is done, it should not be used anymore.\n    return self._delete_latest_container()\n</code></pre>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Record.commit_patch","title":"commit_patch","text":"<pre><code>commit_patch(**kwargs) -&gt; None\n</code></pre> <p>Complete the current writable container (base or patch) for the record.</p> <p>Will perform checks on the new container and throw an exception on failure.</p> <p>After committing the patch is completed and cannot be edited anymore, so any further modifications must go into a new patch.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def commit_patch(self, **kwargs) -&gt; None:\n\"\"\"Complete the current writable container (base or patch) for the record.\n\n    Will perform checks on the new container and throw an exception on failure.\n\n    After committing the patch is completed and cannot be edited anymore, so\n    any further modifications must go into a new patch.\n    \"\"\"\n    if kwargs:\n        raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n\n    self._expect_open()\n    self._expect_not_ro()\n    if not self._has_writable:\n        raise ValueError(\"No patch to commit!\")\n    cfile = self.__files__[-1]\n    filepath = Path(cfile.filename)\n    cfile.close()  # must close it now, as we will write outside of HDF5 next\n\n    # compute checksum, write user block\n    chksum = hashsum_file(filepath, skip_bytes=USER_BLOCK_SIZE)\n    self._ublocks[filepath].hdf5_hashsum = QualHashsumStr(chksum)\n    self._ublocks[filepath].save(filepath)\n\n    # reopen the container file now as read-only\n    self.__files__[-1] = h5py.File(filepath, \"r\")\n</code></pre>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Record.merge_files","title":"merge_files","text":"<pre><code>merge_files(target: Path) -&gt; Path\n</code></pre> <p>Given a path with a record name, merge current record into new container.</p> <p>Returns new resulting container.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def merge_files(self, target: Path) -&gt; Path:\n\"\"\"Given a path with a record name, merge current record into new container.\n\n    Returns new resulting container.\n    \"\"\"\n    self._expect_open()\n    if self._has_writable:\n        raise ValueError(\"Cannot merge, please commit or discard your changes!\")\n\n    with type(self)(target, \"x\") as ds:\n        source_node = self[\"/\"]\n        target_node = ds[\"/\"]\n        for k, v in source_node.attrs.items():  # copy root attributes\n            target_node.attrs[k] = v\n        for name in source_node.keys():  # copy each entity (will recurse)\n            h5_copy_from_to(source_node[name], target_node, name)\n\n        cfile = ds.ih5_files[0]  # store filename to override userblock afterwards\n\n    # compute new merged userblock\n    ub = self._ublock(-1).copy(update={\"prev_patch\": self._ublock(0).prev_patch})\n    # update hashsum with saved new merged hdf5 payload\n    chksum = hashsum_file(cfile, skip_bytes=USER_BLOCK_SIZE)\n    ub.hdf5_hashsum = QualHashsumStr(chksum)\n\n    self._fixes_after_merge(cfile, ub)  # for subclass hooks\n\n    self._set_ublock(-1, ub)\n    ub.save(cfile)\n    return cfile\n</code></pre>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Record.delete_files","title":"delete_files  <code>classmethod</code>","text":"<pre><code>delete_files(record: Path)\n</code></pre> <p>Irreversibly(!) delete all containers matching the record path.</p> <p>This object is invalid after this operation.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>@classmethod\ndef delete_files(cls, record: Path):\n\"\"\"Irreversibly(!) delete all containers matching the record path.\n\n    This object is invalid after this operation.\n    \"\"\"\n    for file in cls.find_files(record):\n        file.unlink()\n</code></pre>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Dataset","title":"IH5Dataset","text":"<p>             Bases: <code>IH5Node</code></p> <p><code>IH5Node</code> representing a <code>h5py.Dataset</code>, i.e. a leaf of the tree.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>class IH5Dataset(IH5Node):\n\"\"\"`IH5Node` representing a `h5py.Dataset`, i.e. a leaf of the tree.\"\"\"\n\n    def __init__(self, files, gpath, creation_idx):\n        super().__init__(files, gpath, creation_idx)\n\n    def copy_into_patch(self):\n\"\"\"Copy the most recent value at this path into the current patch.\n\n        This is useful e.g. for editing inside a complex value, such as an array.\n        \"\"\"\n        self._guard_open()\n        self._guard_read_only()\n        if self._cidx == self._last_idx:\n            raise ValueError(\"Cannot copy, this node is already from latest patch!\")\n        # copy value from older container to current patch\n        self._files[-1][self._gpath] = self[()]\n\n    # h5py-like interface\n    @property\n    def name(self) -&gt; str:\n        return self._gpath\n\n    @property\n    def file(self) -&gt; IH5Record:\n        return self._record\n\n    @property\n    def parent(self) -&gt; IH5Group:\n        return self._record[self._parent_path()]\n\n    @property\n    def attrs(self) -&gt; IH5AttributeManager:\n        self._guard_open()\n        return IH5AttributeManager(self._record, self._gpath, self._cidx)\n\n    # this one is also needed to work with H5DatasetLike\n    @property\n    def ndim(self) -&gt; int:\n        return self._files[self._cidx][self._gpath].ndim  # type: ignore\n\n    # for a dataset, instead of paths the numpy data is indexed. at this level\n    # the patching mechanism ends, so it's just passing through to h5py\n\n    def __getitem__(self, key):\n        # just pass through dataset indexing to underlying dataset\n        self._guard_open()\n        return self._files[self._cidx][self._gpath][key]  # type: ignore\n\n    def __setitem__(self, key, val):\n        self._guard_open()\n        self._guard_read_only()\n        if self._cidx != self._last_idx:\n            raise ValueError(f\"Cannot set '{key}', node is not from the latest patch!\")\n        # if we're in the latest patch, allow writing as usual (pass through)\n        self._files[-1][self._gpath][key] = val  # type: ignore\n</code></pre>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Dataset.copy_into_patch","title":"copy_into_patch","text":"<pre><code>copy_into_patch()\n</code></pre> <p>Copy the most recent value at this path into the current patch.</p> <p>This is useful e.g. for editing inside a complex value, such as an array.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>def copy_into_patch(self):\n\"\"\"Copy the most recent value at this path into the current patch.\n\n    This is useful e.g. for editing inside a complex value, such as an array.\n    \"\"\"\n    self._guard_open()\n    self._guard_read_only()\n    if self._cidx == self._last_idx:\n        raise ValueError(\"Cannot copy, this node is already from latest patch!\")\n    # copy value from older container to current patch\n    self._files[-1][self._gpath] = self[()]\n</code></pre>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5AttributeManager","title":"IH5AttributeManager","text":"<p>             Bases: <code>IH5InnerNode</code></p> <p><code>IH5Node</code> representing an <code>h5py.AttributeManager</code>.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>class IH5AttributeManager(IH5InnerNode):\n\"\"\"`IH5Node` representing an `h5py.AttributeManager`.\"\"\"\n\n    def __init__(self, files, gpath, creation_idx):\n        super().__init__(files, gpath, creation_idx, True)\n\n    def __setitem__(self, key: str, val):\n        self._guard_open()\n        self._guard_read_only()\n        self._guard_key(key)\n        self._guard_value(val)\n\n        # if path does not exist in current patch, just create \"virtual node\"\n        if self._gpath not in self._files[-1]:\n            self._files[-1].create_group(self._gpath)\n        # deletion marker at `key` (if set) is overwritten automatically here\n        # so no need to worry about removing it before assigning `val`\n        self._files[-1][self._gpath].attrs[key] = val\n\n    def __delitem__(self, key: str):\n        self._guard_open()\n        self._guard_read_only()\n        self._guard_key(key)\n        # remove the entity if it is found in newest container,\n        # mark the path as deleted if doing a patch and not working on base container\n        if self._expect_real_item_idx(key) == self._last_idx:\n            del self._files[-1][self._gpath].attrs[key]\n        if len(self._files) &gt; 1:  # is a patch?\n            if self._gpath not in self._files[-1]:  # no node at path in latest?\n                self._files[-1].create_group(self._gpath)  # create \"virtual\" node\n            self._files[-1][self._gpath].attrs[key] = DEL_VALUE  # mark deleted\n</code></pre>"},{"location":"reference/metador_core/ih5/container/#metador_core.ih5.container.IH5Group","title":"IH5Group","text":"<p>             Bases: <code>IH5InnerNode</code></p> <p><code>IH5Node</code> representing a <code>h5py.Group</code>.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>class IH5Group(IH5InnerNode):\n\"\"\"`IH5Node` representing a `h5py.Group`.\"\"\"\n\n    def _require_node(self, name: str, node_type: Type[T]) -&gt; Optional[T]:\n        # helper for require_{group|dataset}\n        grp = self.get(name)\n        if isinstance(grp, node_type):\n            return grp\n        if grp is not None:\n            msg = f\"Incompatible object ({type(grp).__name__}) already exists\"\n            raise TypeError(msg)\n        return None\n\n    def __init__(self, record, gpath: str = \"/\", creation_idx: Optional[int] = None):\n        if gpath == \"/\":\n            creation_idx = 0\n        if creation_idx is None:\n            raise ValueError(\"Need creation_idx for path != '/'!\")\n        super().__init__(record, gpath, creation_idx, False)\n\n    def _create_virtual(self, path: str) -&gt; bool:\n        nodes = self._node_seq(path)\n        path = self._abs_path(path)\n        if (\n            nodes[-1]._gpath == path\n            and nodes[-1]._cidx == self._last_idx\n            and not _node_is_del_mark(nodes[-1])\n        ):\n            return False  # something at that path in most recent container exists\n\n        # most recent entity is a deletion marker or not existing?\n        if nodes[-1]._gpath != path or _node_is_del_mark(nodes[-1]):\n            suf_segs = nodes[-1]._rel_path(path).split(\"/\")\n            # create \"overwrite\" group in most recent patch...\n            self.create_group(f\"{nodes[-1]._gpath}/{suf_segs[0]}\")\n            # ... and create (nested) virtual group node(s), if needed\n            if len(suf_segs) &gt; 1:\n                self._files[-1].create_group(path)\n\n        return True\n\n    # h5py-like interface\n\n    def __setitem__(self, path: str, value):\n        return self.create_dataset(path, data=value)\n\n    def __delitem__(self, key: str):\n        self._guard_open()\n        self._guard_read_only()\n        self._guard_key(key)\n        self._expect_real_item_idx(key)\n        # remove the entity if it is found in newest container,\n        # mark the path as deleted if doing a patch and not working on base container\n        path = self._abs_path(key)\n        if path in self._files[-1]:\n            del self._files[-1][path]\n        if len(self._files) &gt; 1:  # has patches? mark deleted (instead of real delete)\n            self._files[-1][path] = DEL_VALUE\n\n    @property\n    def name(self) -&gt; str:\n        return self._gpath\n\n    @property\n    def file(self):  # -&gt; IH5Record\n        return self._record\n\n    @property\n    def parent(self) -&gt; IH5Group:\n        return self._record[self._parent_path()]\n\n    @property\n    def attrs(self) -&gt; IH5AttributeManager:\n        self._guard_open()\n        return IH5AttributeManager(self._record, self._gpath, self._cidx)\n\n    def create_group(self, name: str) -&gt; IH5Group:\n        self._guard_open()\n        self._guard_read_only()\n\n        path = self._abs_path(name)\n        nodes = self._node_seq(path)\n        if not isinstance(nodes[-1], IH5Group):\n            raise ValueError(f\"Cannot create group, {nodes[-1]._gpath} is a dataset!\")\n        if nodes[-1]._gpath == path:\n            raise ValueError(\"Cannot create group, it already exists!\")\n\n        # remove \"deleted\" marker, if set at current path in current patch container\n        if path in self._files[-1] and _node_is_del_mark(self._files[-1][path]):\n            del self._files[-1][path]\n        # create group (or fail if something else exists there already)\n        self._files[-1].create_group(path)\n        # if this is a patch: mark as non-virtual, i.e. \"overwrite\" with empty group\n        # because the intent here is to \"create\", not update something.\n        if len(self._files) &gt; 1:\n            self._files[-1][path].attrs[SUBST_KEY] = h5py.Empty(None)\n\n        return IH5Group(self._record, path, self._last_idx)\n\n    def create_dataset(\n        self, path: str, shape=None, dtype=None, data=None, **kwargs\n    ) -&gt; IH5Dataset:\n        self._guard_open()\n        self._guard_read_only()\n        self._guard_key(path)\n        self._guard_value(data)\n\n        if unknown_kwargs := set(kwargs.keys()) - {\"compression\", \"compression_opts\"}:\n            raise ValueError(f\"Unkown kwargs: {unknown_kwargs}\")\n\n        path = self._abs_path(path)\n        fidx = self._find(path)\n        if fidx is not None:\n            prev_val = self._get_child(path, fidx)\n            if isinstance(prev_val, (IH5Group, IH5Dataset)):\n                raise ValueError(\"Path exists, in order to replace - delete first!\")\n\n        if path in self._files[-1] and _node_is_del_mark(\n            self._get_child_raw(path, self._last_idx)\n        ):\n            # remove deletion marker in latest patch, if set\n            del self._files[-1][path]\n        elif path not in self._files[-1]:\n            # create path and overwrite-group in latest patch\n            self._create_virtual(path)\n            assert path in self._files[-1]\n            del self._files[-1][path]\n\n        self._files[-1].create_dataset(  # actually create it, finally\n            path, shape=shape, dtype=dtype, data=data, **kwargs\n        )\n        return IH5Dataset(self._record, path, self._last_idx)\n\n    def require_group(self, name: str) -&gt; IH5Group:\n        if (n := self._require_node(name, IH5Group)) is not None:\n            return n  # existing group\n        return self.create_group(name)\n\n    def require_dataset(self, name: str, *args, **kwds) -&gt; IH5Dataset:\n        if (n := self._require_node(name, IH5Dataset)) is not None:\n            # TODO: check dimensions etc, copy into patch if it fits\n            return n\n        return self.create_dataset(name, *args, **kwds)\n\n    def copy(self, source: CopySource, dest: CopyDest, **kwargs):\n        src_node = self[source] if isinstance(source, str) else source\n        name: str = kwargs.pop(\"name\", src_node.name.split(\"/\")[-1])\n        dst_name: str\n        if isinstance(dest, str):\n            # if dest is a path, ignore inferred/passed name\n            segs = self._abs_path(dest).split(\"/\")\n            dst_group = self.require_group(\"/\".join(segs[:-1]) or \"/\")\n            dst_name = segs[-1]\n        else:\n            # given dest is a group node, use inferred/passed name\n\n            dst_group = dest if dest.name != \"/\" else dest[\"/\"]  # *\n            # * ugly workaround for treating files as groups in the copy method\n\n            dst_name = name\n        return h5_copy_from_to(src_node, cast(Any, dst_group), dst_name, **kwargs)\n\n    def move(self, source: str, dest: str):\n        self.copy(source, dest)\n        del self[source]\n\n    def visititems(self, func: Callable[[str, object], Optional[Any]]) -&gt; Any:\n        self._guard_open()\n        stack = list(reversed(self._get_children()))\n        while stack:\n            curr = stack.pop()\n            val = func(self._rel_path(curr._gpath), curr)\n            if val is not None:\n                return val\n            if isinstance(curr, IH5Group):\n                stack += reversed(curr._get_children())\n\n    def visit(self, func: Callable[[str], Optional[Any]]) -&gt; Any:\n        return self.visititems(lambda x, _: func(x))\n</code></pre>"},{"location":"reference/metador_core/ih5/manifest/","title":"manifest","text":"<p>Sidecar JSON file storing a skeleton to create stubs and patch containers.</p>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5Manifest","title":"IH5Manifest","text":"<p>             Bases: <code>BaseModel</code></p> <p>A metadata sidecar file for a collection of IH5 record files.</p> <p>It contains the skeleton of the container, in order to be able to create a patch for an IH5Record the data is not locally available for (but manifest is).</p> Source code in <code>src/metador_core/ih5/manifest.py</code> <pre><code>class IH5Manifest(BaseModel):\n\"\"\"A metadata sidecar file for a collection of IH5 record files.\n\n    It contains the skeleton of the container, in order to be able to create a\n    patch for an IH5Record the data is not locally available for (but manifest is).\n    \"\"\"\n\n    # uuid for the manifest file itself (so the filename does not matter)\n    manifest_uuid: UUID\n\n    user_block: IH5UserBlock  # copy of user block (without the manifest extension part)\n\n    skeleton: IH5Skeleton  # computed with IH5Skeleton, used to create a stub\n\n    manifest_exts: Dict[str, Any]  # Arbitrary extensions, similar to IH5UserBlock\n\n    @classmethod\n    def from_userblock(cls, ub: IH5UserBlock, skeleton=None, exts=None) -&gt; IH5Manifest:\n\"\"\"Create a manifest file based on a user block.\"\"\"\n        skeleton = skeleton or {}\n        exts = exts or {}\n        ub_copy = ub.copy()\n        # only keep other extensions (otherwise its circular)\n        ub_copy.ub_exts = {\n            k: v for k, v in ub.ub_exts.items() if k != IH5UBExtManifest.ext_name()\n        }\n        return cls(\n            manifest_uuid=uuid1(),\n            user_block=ub_copy,\n            skeleton=skeleton,\n            manifest_exts=exts,\n        )\n\n    def __bytes__(self) -&gt; bytes:\n\"\"\"Serialize to JSON and return UTF-8 encoded bytes to be written in a file.\"\"\"\n        # add a newline, as otherwise behaviour with text editors will be confusing\n        # (e.g. vim automatically adds a trailing newline that it hides)\n        # https://stackoverflow.com/questions/729692/why-should-text-files-end-with-a-newline\n        return (self.json(indent=2) + \"\\n\").encode(encoding=\"utf-8\")\n\n    def save(self, path: Path):\n\"\"\"Save manifest (as returned by bytes()) into a file.\"\"\"\n        with open(path, \"wb\") as f:\n            f.write(bytes(self))\n            f.flush()\n</code></pre>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5Manifest.from_userblock","title":"from_userblock  <code>classmethod</code>","text":"<pre><code>from_userblock(\n    ub: IH5UserBlock,\n    skeleton: IH5UserBlock = None,\n    exts: IH5UserBlock = None,\n) -&gt; IH5Manifest\n</code></pre> <p>Create a manifest file based on a user block.</p> Source code in <code>src/metador_core/ih5/manifest.py</code> <pre><code>@classmethod\ndef from_userblock(cls, ub: IH5UserBlock, skeleton=None, exts=None) -&gt; IH5Manifest:\n\"\"\"Create a manifest file based on a user block.\"\"\"\n    skeleton = skeleton or {}\n    exts = exts or {}\n    ub_copy = ub.copy()\n    # only keep other extensions (otherwise its circular)\n    ub_copy.ub_exts = {\n        k: v for k, v in ub.ub_exts.items() if k != IH5UBExtManifest.ext_name()\n    }\n    return cls(\n        manifest_uuid=uuid1(),\n        user_block=ub_copy,\n        skeleton=skeleton,\n        manifest_exts=exts,\n    )\n</code></pre>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5Manifest.__bytes__","title":"__bytes__","text":"<pre><code>__bytes__() -&gt; bytes\n</code></pre> <p>Serialize to JSON and return UTF-8 encoded bytes to be written in a file.</p> Source code in <code>src/metador_core/ih5/manifest.py</code> <pre><code>def __bytes__(self) -&gt; bytes:\n\"\"\"Serialize to JSON and return UTF-8 encoded bytes to be written in a file.\"\"\"\n    # add a newline, as otherwise behaviour with text editors will be confusing\n    # (e.g. vim automatically adds a trailing newline that it hides)\n    # https://stackoverflow.com/questions/729692/why-should-text-files-end-with-a-newline\n    return (self.json(indent=2) + \"\\n\").encode(encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5Manifest.save","title":"save","text":"<pre><code>save(path: Path)\n</code></pre> <p>Save manifest (as returned by bytes()) into a file.</p> Source code in <code>src/metador_core/ih5/manifest.py</code> <pre><code>def save(self, path: Path):\n\"\"\"Save manifest (as returned by bytes()) into a file.\"\"\"\n    with open(path, \"wb\") as f:\n        f.write(bytes(self))\n        f.flush()\n</code></pre>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5UBExtManifest","title":"IH5UBExtManifest","text":"<p>             Bases: <code>BaseModel</code></p> <p>IH5 user block extension for stub and manifest support.</p> Source code in <code>src/metador_core/ih5/manifest.py</code> <pre><code>class IH5UBExtManifest(BaseModel):\n\"\"\"IH5 user block extension for stub and manifest support.\"\"\"\n\n    is_stub_container: bool\n\"\"\"True if file has the structure of another container, without actual data inside.\"\"\"\n\n    manifest_uuid: UUID\n\"\"\"UUID of the manifest file that belongs to this IH5 file.\"\"\"\n\n    manifest_hashsum: QualHashsumStr\n\"\"\"Hashsum of the manifest file that belongs to this IH5 file.\"\"\"\n\n    @classmethod\n    def ext_name(cls) -&gt; str:\n\"\"\"Name of user block extension section for stub and manifest info.\"\"\"\n        return \"ih5mf_v01\"\n\n    @classmethod\n    def get(cls, ub: IH5UserBlock) -&gt; Optional[IH5UBExtManifest]:\n\"\"\"Parse extension metadata from userblock, if it is available.\"\"\"\n        if cls.ext_name() not in ub.ub_exts:\n            return None\n        return cls.parse_obj(ub.ub_exts[cls.ext_name()])\n\n    def update(self, ub: IH5UserBlock):\n\"\"\"Create or overwrite extension metadata in given userblock.\"\"\"\n        ub.ub_exts[self.ext_name()] = self.dict()\n</code></pre>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5UBExtManifest.is_stub_container","title":"is_stub_container  <code>instance-attribute</code>","text":"<pre><code>is_stub_container: bool\n</code></pre> <p>True if file has the structure of another container, without actual data inside.</p>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5UBExtManifest.manifest_uuid","title":"manifest_uuid  <code>instance-attribute</code>","text":"<pre><code>manifest_uuid: UUID\n</code></pre> <p>UUID of the manifest file that belongs to this IH5 file.</p>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5UBExtManifest.manifest_hashsum","title":"manifest_hashsum  <code>instance-attribute</code>","text":"<pre><code>manifest_hashsum: QualHashsumStr\n</code></pre> <p>Hashsum of the manifest file that belongs to this IH5 file.</p>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5UBExtManifest.ext_name","title":"ext_name  <code>classmethod</code>","text":"<pre><code>ext_name() -&gt; str\n</code></pre> <p>Name of user block extension section for stub and manifest info.</p> Source code in <code>src/metador_core/ih5/manifest.py</code> <pre><code>@classmethod\ndef ext_name(cls) -&gt; str:\n\"\"\"Name of user block extension section for stub and manifest info.\"\"\"\n    return \"ih5mf_v01\"\n</code></pre>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5UBExtManifest.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(ub: IH5UserBlock) -&gt; Optional[IH5UBExtManifest]\n</code></pre> <p>Parse extension metadata from userblock, if it is available.</p> Source code in <code>src/metador_core/ih5/manifest.py</code> <pre><code>@classmethod\ndef get(cls, ub: IH5UserBlock) -&gt; Optional[IH5UBExtManifest]:\n\"\"\"Parse extension metadata from userblock, if it is available.\"\"\"\n    if cls.ext_name() not in ub.ub_exts:\n        return None\n    return cls.parse_obj(ub.ub_exts[cls.ext_name()])\n</code></pre>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5UBExtManifest.update","title":"update","text":"<pre><code>update(ub: IH5UserBlock)\n</code></pre> <p>Create or overwrite extension metadata in given userblock.</p> Source code in <code>src/metador_core/ih5/manifest.py</code> <pre><code>def update(self, ub: IH5UserBlock):\n\"\"\"Create or overwrite extension metadata in given userblock.\"\"\"\n    ub.ub_exts[self.ext_name()] = self.dict()\n</code></pre>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5MFRecord","title":"IH5MFRecord","text":"<p>             Bases: <code>IH5Record</code></p> <p>IH5Record extended by a manifest file.</p> <p>The manifest file is a sidcar JSON file that contains enough information to support the creation of a stub container and patching a dataset without having the actual container locally available.</p> <p>In a chain of container files, only the base container may be a stub. All files without the manifest extension in the userblock are considered not stubs.</p> <p>An IH5MFRecord is a valid IH5Record (the manifest file then is simply ignored). Also, it is possible to open an IH5Record as IH5MFRecord and turn it into a valid IH5MFRecord by committing a patch (this will create the missing manifest).</p> <p>In addition to the ability to create stubs, the manifest file can be used to carry information that should be attached to a container, but is too large or inappropriate for storage in the userblock (e.g. should be available separately).</p> <p>The manifest should store information that applies semantically to the whole fileset at the current patch level, it MUST NOT be required to have manifest files for each ih5 patch. Additional information stored in the manifest is inherited to the manifest of successive patches until overridden.</p> <p>The main use case of this extension is to be used by automated packers from suitably prepared source directories, to be uploaded to remote locations, and for these packers being able to create patches for the record without access to all the data containers (based only on the most recent manifest file).</p> Source code in <code>src/metador_core/ih5/manifest.py</code> <pre><code>class IH5MFRecord(IH5Record):\n\"\"\"IH5Record extended by a manifest file.\n\n    The manifest file is a sidcar JSON file that contains enough information to support\n    the creation of a stub container and patching a dataset without having the actual\n    container locally available.\n\n    In a chain of container files, only the base container may be a stub.\n    All files without the manifest extension in the userblock are considered not stubs.\n\n    An IH5MFRecord is a valid IH5Record (the manifest file then is simply ignored).\n    Also, it is possible to open an IH5Record as IH5MFRecord and turn it into a\n    valid IH5MFRecord by committing a patch (this will create the missing manifest).\n\n    In addition to the ability to create stubs, the manifest file can be used to carry\n    information that should be attached to a container, but is too large or inappropriate\n    for storage in the userblock (e.g. should be available separately).\n\n    The manifest should store information that applies semantically to the whole fileset\n    at the current patch level, it MUST NOT be required to have manifest files for each\n    ih5 patch. Additional information stored in the manifest is inherited to the\n    manifest of successive patches until overridden.\n\n    The main use case of this extension is to be used by automated packers\n    from suitably prepared source directories, to be uploaded to remote locations,\n    and for these packers being able to create patches for the record without\n    access to all the data containers (based only on the most recent manifest file).\n    \"\"\"\n\n    MANIFEST_EXT: str = \"mf.json\"\n\n    _manifest: Optional[IH5Manifest] = None\n\"\"\"Manifest of newest loaded container file (only None for new uncommited records).\"\"\"\n\n    @property\n    def manifest(self) -&gt; IH5Manifest:\n\"\"\"Return loaded manifest object of latest committed record patch.\"\"\"\n        if self._manifest is None:  # should only happen with fresh create()d records\n            raise ValueError(\"No manifest exists yet! Did you forget to commit?\")\n        return self._manifest\n\n    def _fresh_manifest(self) -&gt; IH5Manifest:\n\"\"\"Return new manifest based on current state of the record.\"\"\"\n        ub = self._ublock(-1)\n        skel = IH5Skeleton.for_record(self)\n        return IH5Manifest.from_userblock(ub, skeleton=skel, exts={})\n\n    @classmethod\n    def _manifest_filepath(cls, record: str) -&gt; Path:\n\"\"\"Return canonical filename of manifest based on path of a container file.\"\"\"\n        return Path(f\"{str(record)}{cls.MANIFEST_EXT}\")\n\n    # Override to also load and check latest manifest\n    @classmethod\n    def _open(cls, paths: List[Path], **kwargs):\n        manifest_file: Optional[Path] = kwargs.pop(\"manifest_file\", None)\n        ret: IH5MFRecord = super()._open(paths, **kwargs)\n\n        # if not given explicitly, infer correct manifest filename\n        # based on logically latest container (they are sorted after parent init)\n        if manifest_file is None:\n            manifest_file = cls._manifest_filepath(ret._files[-1].filename)\n\n        # for latest container, check linked manifest (if any) against given/inferred one\n        ub = ret._ublock(-1)\n        ubext = IH5UBExtManifest.get(ub)\n        if ubext is not None:\n            if not manifest_file.is_file():\n                msg = f\"Manifest file {manifest_file} does not exist, cannot open!\"\n                raise ValueError(f\"{ret._files[-1].filename}: {msg}\")\n\n            chksum = hashsum_file(manifest_file)\n            if ubext.manifest_hashsum != chksum:\n                msg = \"Manifest has been modified, unexpected hashsum!\"\n                raise ValueError(f\"{ret._files[-1].filename}: {msg}\")\n\n            ret._manifest = IH5Manifest.parse_file(manifest_file)\n            # NOTE: as long as we enforce checksum of manifest, this failure can't happen:\n            # if ubext.manifest_uuid != self._manifest.manifest_uuid:\n            #     raise ValueError(f\"{ub._filename}: Manifest file has wrong UUID!\")\n        # all looks good\n        return ret\n\n    # Override to also check user block extension\n    def _check_ublock(\n        self,\n        filename: Union[str, Path],\n        ub: IH5UserBlock,\n        prev: Optional[IH5UserBlock] = None,\n        check_hashsum: bool = True,\n    ):\n        super()._check_ublock(filename, ub, prev, check_hashsum)\n        # Try getting manifest info in the userblock.\n        # If it is missing, probably we're opening a \"raw\" IH5Record or a messed up mix\n        ubext = IH5UBExtManifest.get(ub)\n        # we only allow to write patches on top of stubs,\n        # but not have stubs on top of something else.\n        # If something creates a patch that is (marked as) a stub, its a developer error.\n        # If the ub ext is missing, then we must assume that it is not a stub.\n        assert prev is None or ubext is None or not ubext.is_stub_container\n\n    def _fixes_after_merge(self, file, ub):\n        # if a manifest exists for the current dataset,\n        # copy its manifest to overwrite the fresh one of the merged container\n        # and fix its user block\n        if self._manifest is not None:\n            # check that new userblock inherited the original linked manifest\n            ext = IH5UBExtManifest.get(ub)\n            assert ext is not None and ext.manifest_uuid == self.manifest.manifest_uuid\n            # overwrite the \"fresh\" manifest from merge with the original one\n            self.manifest.save(self._manifest_filepath(file))\n\n    # Override to prevent merge if a stub is present\n    def merge_files(self, target: Path):\n        def is_stub(x):\n            ext = IH5UBExtManifest.get(x)\n            # missing ext -&gt; not a stub (valid stub has ext + is marked as stub)\n            return ext is not None and ext.is_stub_container\n\n        if any(map(is_stub, self.ih5_meta)):\n            raise ValueError(\"Cannot merge, files contain a stub!\")\n\n        return super().merge_files(target)\n\n    # Override to create skeleton and dir hashsums, write manifest and add to user block\n    # Will inherit old manifest extensions, unless overridden by passed argument\n    def commit_patch(self, **kwargs) -&gt; None:\n        # is_stub == True only if called from create_stub!!! (NOT for the \"end-user\"!)\n        is_stub = kwargs.pop(\"__is_stub__\", False)\n        exts = kwargs.pop(\"manifest_exts\", None)\n\n        # create manifest for the new patch\n        mf = self._fresh_manifest()\n        if self._manifest is not None:  # inherit attached data, if manifest exists\n            mf.manifest_exts = self.manifest.manifest_exts\n        if exts is not None:  # override, if extensions provided\n            mf.manifest_exts = exts\n\n        old_ub = self._ublock(-1)  # keep ref in case anything goes wrong\n        # prepare new user block that links to the prospective manifest\n        new_ub = old_ub.copy()\n        IH5UBExtManifest(\n            is_stub_container=is_stub,\n            manifest_uuid=mf.manifest_uuid,\n            manifest_hashsum=qualified_hashsum(bytes(mf)),\n        ).update(new_ub)\n\n        # try writing new container\n        self._set_ublock(-1, new_ub)\n        try:\n            super().commit_patch(**kwargs)\n        except ValueError as e:  # some checks failed\n            self._set_ublock(-1, old_ub)  # reset current user block\n            raise e\n\n        # as everything is fine, finally (over)write manifest here and on disk\n        self._manifest = mf\n        mf.save(self._manifest_filepath(self._files[-1].filename))\n\n    @classmethod\n    def create_stub(\n        cls,\n        record: Union[Path, str],\n        manifest_file: Path,\n    ) -&gt; IH5MFRecord:\n\"\"\"Create a stub base container for patching an existing but unavailable record.\n\n        The stub is based on the user block of a real IH5 record container line\n        and the skeleton of the overlay structure (as returned by `IH5Skeleton`),\n        which are taken from a provided manifest file.\n\n        Patches created on top of the stub are compatible with the original record\n        whose metadata the stub is based on.\n\n        The returned container is read-only and only serves as base for patches.\n        \"\"\"\n        manifest = IH5Manifest.parse_file(manifest_file)\n\n        skeleton: IH5Skeleton = manifest.skeleton\n        user_block: IH5UserBlock = manifest.user_block.copy()\n\n        # the manifest-stored user block has no manifest extension itself - create new\n        # based on passed manifest.\n        # mark it as stub in extra metadata now! important to avoid accidents!\n        # must pass it in like that, because the container will be auto-commited.\n        ubext = IH5UBExtManifest(\n            is_stub_container=True,  # &lt;- the ONLY place where this is allowed!\n            manifest_uuid=manifest.manifest_uuid,\n            manifest_hashsum=hashsum_file(manifest_file),\n        )\n        ubext.update(user_block)\n\n        # create and finalize the stub (override userblock and create skeleton structure)\n        ds = IH5MFRecord._create(Path(record))\n        init_stub_base(ds, user_block, skeleton)  # prepares structure and user block\n        # commit_patch() completes stub + fixes the hashsum\n        ds.commit_patch(__is_stub__=True)\n        assert not ds._has_writable\n\n        return ds\n</code></pre>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5MFRecord.manifest","title":"manifest  <code>property</code>","text":"<pre><code>manifest: IH5Manifest\n</code></pre> <p>Return loaded manifest object of latest committed record patch.</p>"},{"location":"reference/metador_core/ih5/manifest/#metador_core.ih5.manifest.IH5MFRecord.create_stub","title":"create_stub  <code>classmethod</code>","text":"<pre><code>create_stub(\n    record: Union[Path, str], manifest_file: Path\n) -&gt; IH5MFRecord\n</code></pre> <p>Create a stub base container for patching an existing but unavailable record.</p> <p>The stub is based on the user block of a real IH5 record container line and the skeleton of the overlay structure (as returned by <code>IH5Skeleton</code>), which are taken from a provided manifest file.</p> <p>Patches created on top of the stub are compatible with the original record whose metadata the stub is based on.</p> <p>The returned container is read-only and only serves as base for patches.</p> Source code in <code>src/metador_core/ih5/manifest.py</code> <pre><code>@classmethod\ndef create_stub(\n    cls,\n    record: Union[Path, str],\n    manifest_file: Path,\n) -&gt; IH5MFRecord:\n\"\"\"Create a stub base container for patching an existing but unavailable record.\n\n    The stub is based on the user block of a real IH5 record container line\n    and the skeleton of the overlay structure (as returned by `IH5Skeleton`),\n    which are taken from a provided manifest file.\n\n    Patches created on top of the stub are compatible with the original record\n    whose metadata the stub is based on.\n\n    The returned container is read-only and only serves as base for patches.\n    \"\"\"\n    manifest = IH5Manifest.parse_file(manifest_file)\n\n    skeleton: IH5Skeleton = manifest.skeleton\n    user_block: IH5UserBlock = manifest.user_block.copy()\n\n    # the manifest-stored user block has no manifest extension itself - create new\n    # based on passed manifest.\n    # mark it as stub in extra metadata now! important to avoid accidents!\n    # must pass it in like that, because the container will be auto-commited.\n    ubext = IH5UBExtManifest(\n        is_stub_container=True,  # &lt;- the ONLY place where this is allowed!\n        manifest_uuid=manifest.manifest_uuid,\n        manifest_hashsum=hashsum_file(manifest_file),\n    )\n    ubext.update(user_block)\n\n    # create and finalize the stub (override userblock and create skeleton structure)\n    ds = IH5MFRecord._create(Path(record))\n    init_stub_base(ds, user_block, skeleton)  # prepares structure and user block\n    # commit_patch() completes stub + fixes the hashsum\n    ds.commit_patch(__is_stub__=True)\n    assert not ds._has_writable\n\n    return ds\n</code></pre>"},{"location":"reference/metador_core/ih5/overlay/","title":"overlay","text":"<p>Overlay wrappers to access a virtual record consisting of a base container + patches.</p> <p>The wrappers take care of dispatching requests to records, groups and attributes to the correct path.</p>"},{"location":"reference/metador_core/ih5/overlay/#metador_core.ih5.overlay.IH5Node","title":"IH5Node  <code>dataclass</code>","text":"<p>An overlay node wraps a group, dataset or attribute manager.</p> <p>It takes care of finding the correct container to look for the data and helps with patching data in a new patch container.</p> <p>It essentially lifts the interface of h5py from a single file to an IH5 record that may consist of a base container file and a number of patch containers.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>@dataclass(frozen=True)\nclass IH5Node:\n\"\"\"An overlay node wraps a group, dataset or attribute manager.\n\n    It takes care of finding the correct container to look for the data\n    and helps with patching data in a new patch container.\n\n    It essentially lifts the interface of h5py from a single file to an IH5 record\n    that may consist of a base container file and a number of patch containers.\n    \"\"\"\n\n    _record: IH5Record\n\"\"\"Record this node belongs to (needed to access the actual data).\"\"\"\n\n    _gpath: str\n\"\"\"Path in record that this node represents (absolute wrt. root of record).\"\"\"\n\n    _cidx: int\n\"\"\"Left boundary index for lookups in order of loaded containers, i.e.\n    this node will not consider containers with smaller index than that.\n    \"\"\"\n\n    def __post_init__(self):\n\"\"\"Instantiate an overlay node.\"\"\"\n        if not self._gpath or self._gpath[0] != \"/\":\n            raise ValueError(\"Path must be absolute!\")\n        if self._cidx &lt; 0:\n            raise ValueError(\"Creation index must be non-negative!\")\n\n    @property\n    def _files(self) -&gt; List[h5py.File]:\n        return self._record.__files__\n\n    def __hash__(self):\n\"\"\"Hash an overlay node.\n\n        Two nodes are equivalent if they are linked to the same\n        open record and address the same entity.\n        \"\"\"\n        return hash((id(self._record), self._gpath, self._cidx))\n\n    def __bool__(self) -&gt; bool:\n        return bool(self._files) and all(map(bool, self._files))\n\n    @property\n    def _last_idx(self):\n\"\"\"Index of the latest container.\"\"\"\n        return len(self._files) - 1\n\n    @property\n    def _is_read_only(self) -&gt; bool:\n\"\"\"Return true if the newest container is read-only and nothing can be written.\"\"\"\n        return not self._record._has_writable\n\n    def _guard_open(self):\n\"\"\"Check that the record is open (if it was closed, the files are gone).\"\"\"\n        if not self:\n            raise ValueError(\"Record is not open or accessible!\")\n\n    def _guard_read_only(self):\n        if self._is_read_only:\n            raise ValueError(\"Create a patch in order to change the container!\")\n\n    def _guard_value(self, data):\n        if _is_del_mark(data):\n            raise ValueError(f\"Value '{data}' is forbidden, cannot assign!\")\n        if isinstance(data, IH5Node):\n            raise ValueError(\"Hard links are not supported, cannot assign!\")\n        if isinstance(data, h5py.SoftLink) or isinstance(data, h5py.ExternalLink):\n            raise ValueError(\"SymLink and ExternalLink not supported, cannot assign!\")\n\n    @classmethod\n    def _latest_idx(cls, files, path: str) -&gt; Optional[int]:\n\"\"\"Return index of newest file where the group/dataset was overwritten/created.\n\n        Returns None if not found or most recent value is a deletion mark.\n        \"\"\"\n        idx = None\n        for i in reversed(range(len(files))):\n            if _node_is_del_mark(files[i][path]):\n                return None\n            elif _node_is_virtual(files[i][path]):\n                idx = i\n            else:\n                return i  # some patch overrides the group\n        return idx\n\n    # path transformations\n\n    def _parent_path(self) -&gt; str:\n\"\"\"Return path of the parent node (the root is its own parent).\"\"\"\n        if self._gpath == \"/\":\n            return \"/\"\n        segs = self._gpath.split(\"/\")[:-1]\n        return \"/\" if segs == [\"\"] else \"/\".join(segs)\n\n    def _rel_path(self, path: str) -&gt; str:\n\"\"\"Return relative path based on node location, if passed path is absolute.\n\n        If relative, returns the path back unchanged.\n        \"\"\"\n        if path[0] != \"/\":\n            return path\n        if path.find(self._gpath) != 0:\n            raise RuntimeError(\"Invalid usage, cannot strip non-matching prefix!\")\n        start_idx = len(self._gpath) + int(self._gpath != \"/\")\n        return path[start_idx:]\n\n    def _abs_path(self, path: str) -&gt; str:\n\"\"\"Return absolute path based on node location, if given path is relative.\n\n        If absolute, returns the path back unchanged.\n        \"\"\"\n        pref = self._gpath if self._gpath != \"/\" else \"\"\n        return path if path and path[0] == \"/\" else f\"{pref}/{path}\"\n\n    def _inspect_path(self, path):  # pragma: no cover\n\"\"\"Print the path node of all containers where the path is contained in.\"\"\"\n        print(f\"Path {path}:\")\n        for j in range(len(self._files)):\n            if path in self._files[j]:\n                node = self._files[j][path]\n                print(f\"  idx={j}: {type(node).__name__}\")\n                if isinstance(node, h5py.Dataset):\n                    print(\"    \", node[()])\n</code></pre>"},{"location":"reference/metador_core/ih5/overlay/#metador_core.ih5.overlay.IH5Node.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Instantiate an overlay node.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>def __post_init__(self):\n\"\"\"Instantiate an overlay node.\"\"\"\n    if not self._gpath or self._gpath[0] != \"/\":\n        raise ValueError(\"Path must be absolute!\")\n    if self._cidx &lt; 0:\n        raise ValueError(\"Creation index must be non-negative!\")\n</code></pre>"},{"location":"reference/metador_core/ih5/overlay/#metador_core.ih5.overlay.IH5Node.__hash__","title":"__hash__","text":"<pre><code>__hash__()\n</code></pre> <p>Hash an overlay node.</p> <p>Two nodes are equivalent if they are linked to the same open record and address the same entity.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>def __hash__(self):\n\"\"\"Hash an overlay node.\n\n    Two nodes are equivalent if they are linked to the same\n    open record and address the same entity.\n    \"\"\"\n    return hash((id(self._record), self._gpath, self._cidx))\n</code></pre>"},{"location":"reference/metador_core/ih5/overlay/#metador_core.ih5.overlay.IH5InnerNode","title":"IH5InnerNode","text":"<p>             Bases: <code>IH5Node</code></p> <p>Common functionality for Group and AttributeManager.</p> <p>Will grant either access to child records/subgroups, or to the attributes attached to the group/dataset at a path in a record.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>class IH5InnerNode(IH5Node):\n\"\"\"Common functionality for Group and AttributeManager.\n\n    Will grant either access to child records/subgroups,\n    or to the attributes attached to the group/dataset at a path in a record.\n    \"\"\"\n\n    @property\n    def _is_attrs(self) -&gt; bool:\n        return self.__is_attrs__\n\n    def __init__(\n        self,\n        record: IH5Record,\n        gpath: str,\n        creation_idx: int,\n        attrs: bool = False,\n    ):\n\"\"\"See `IH5Node` constructor.\n\n        This variant represents an \"overlay container\", of which there are two types -\n        a group (h5py.Group) and a set of attributes (h5py.AttributeManager).\n\n        This class takes care of both (in order to avoid lots of code duplication),\n        distinguishing them through the additional `attrs` flag.\n        \"\"\"\n        super().__init__(record, gpath, creation_idx)\n        # if attrs set, represents AttributeManager, otherwise its a group\n        self.__is_attrs__: bool = attrs\n\n    def _guard_key(self, key: str):\n\"\"\"Check a key used with bracket accessor notation.\n\n        (e.g. used for `__getitem__, __setitem__, __delitem__`)\n        \"\"\"\n        if key == \"\":\n            raise ValueError(\"Invalid empty path!\")\n        if key.find(\"@\") &gt;= 0:  # used as attribute separator in the skeleton! TODO\n            raise ValueError(f\"Invalid symbol '@' in key: '{key}'!\")\n        if re.match(r\"^[!-~]+$\", key) is None:\n            raise ValueError(\"Invalid key: Only printable ASCII is allowed!\")\n        if self._is_attrs and (key.find(\"/\") &gt;= 0 or key == SUBST_KEY):\n            raise ValueError(f\"Invalid attribute key: '{key}'!\")\n\n    def _get_child_raw(self, key: str, cidx: int) -&gt; Any:\n\"\"\"Return given child (dataset, group, attribute) from given container.\"\"\"\n        if self._is_attrs:\n            return self._files[cidx][self._gpath].attrs[key]\n        else:\n            return self._files[cidx][self._abs_path(key)]\n\n    def _get_child(self, key: str, cidx: int) -&gt; Any:\n\"\"\"Like _get_child_raw, but wraps the result with an overlay class if needed.\"\"\"\n        val = self._get_child_raw(key, cidx)\n        path = self._abs_path(key)\n        if isinstance(val, h5py.Group):\n            return IH5Group(self._record, path, cidx)\n        elif isinstance(val, h5py.Dataset):\n            return IH5Dataset(self._record, path, cidx)\n        else:\n            return val\n\n    def _children(self) -&gt; Dict[str, int]:\n\"\"\"Return dict mapping from a child name to the most recent overriding patch idx.\n\n        For datasets, dereferencing the child path in that container will give the data.\n        For groups, the returned number is to be treated as the lower bound, i.e.\n        the child creation_idx to recursively get the descendents.\n        \"\"\"\n        self._guard_open()\n\n        children: Dict[str, int] = {}\n        is_virtual: Dict[str, bool] = {}\n        for i in reversed(range(self._cidx, len(self._files))):\n            if self._gpath not in self._files[i]:\n                continue\n\n            obj = self._files[i][self._gpath]\n            if self._is_attrs:\n                obj = obj.attrs\n            assert isinstance(obj, (h5py.Group, h5py.AttributeManager))\n\n            # keep most recent version of child node / attribute\n            for k in obj.keys():\n                if k not in children:\n                    is_virtual[k] = _node_is_virtual(self._get_child_raw(k, i))\n                    children[k] = i\n                elif is_virtual[k]:  # .. and k in children!\n                    # decrease lower bound\n                    children[k] = min(children[k], i)\n\n        # return resulting child nodes / attributes (without the deleted ones)\n        # in alphabetical order,\n        # in case of attributes, also excludes special SUBST marker attribute\n        return {\n            k: idx\n            for k, idx in sorted(children.items(), key=lambda x: x[0])\n            if (not self._is_attrs or k != SUBST_KEY)\n            and not _node_is_del_mark(self._get_child_raw(k, idx))\n        }\n\n    def _get_children(self) -&gt; List[Any]:\n\"\"\"Get alphabetically ordered list of child nodes.\"\"\"\n        return [\n            self._get_child(self._abs_path(k), idx)\n            for k, idx in self._children().items()\n        ]\n\n    def _node_seq(self, path: str) -&gt; List[IH5Node]:\n\"\"\"Return node sequence (one node per path prefix) to given path.\n\n        Returns:\n            Sequence starting with the current node (if path is relative)\n            or the root node (if absolute) followed by all successive\n            children along the requested path that exist.\n        \"\"\"\n        curr: IH5InnerNode = IH5Group(self._record) if path[0] == \"/\" else self\n\n        ret: List[IH5Node] = [curr]\n        if path == \"/\" or path == \".\":  # special case\n            return ret\n\n        # access entity through child group sequence\n        segs = path.strip(\"/\").split(\"/\")\n        nxt_cidx = 0\n        for i in range(len(segs)):\n            seg, is_last_seg = segs[i], i == len(segs) - 1\n            # find most recent container with that child\n            nxt_cidx = curr._children().get(seg, -1)\n            if nxt_cidx == -1:\n                return ret  # not found -&gt; return current prefix\n            curr = curr._get_child(seg, nxt_cidx)  # proceed to child\n            ret.append(curr)\n            # catch invalid access, e.g. /foo is record, user accesses /foo/bar:\n            if not is_last_seg and isinstance(curr, IH5Dataset):\n                raise ValueError(f\"Cannot access path inside a value: {curr._gpath}\")\n        # return path index sequence\n        return ret\n\n    def _find(self, key: str) -&gt; Optional[int]:\n\"\"\"Return index of container holding that key (attribute or path), if any.\n\n        Args:\n            key: nonempty string (attribute, or relative/absolute path)\n\n        Returns:\n            Index &gt;= 0 of most recent container patching that path if found, else None.\n        \"\"\"\n        if self._is_attrs:  # access an attribute by key (always \"relative\")\n            return self._children().get(key, None)\n        # access a path (absolute or relative)\n        nodes = self._node_seq(key)\n        return nodes[-1]._cidx if nodes[-1]._gpath == self._abs_path(key) else None\n\n    # h5py-like interface\n\n    def get(self, key: str, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __getitem__(self, key: str):\n        self._guard_open()\n        self._guard_key(key)\n        found_cidx = self._find(key)\n        if found_cidx is None:\n            raise KeyError(key)\n        return self._get_child(key, found_cidx)\n\n    def _expect_real_item_idx(self, key: str) -&gt; int:\n        found_cidx = self._find(key)\n        if found_cidx is None or _node_is_del_mark(self._get_child(key, found_cidx)):\n            raise KeyError(f\"Cannot delete '{key}', it does not exist!\")\n        return found_cidx\n\n    def __contains__(self, key: str):\n        self._guard_key(key)\n        return self._find(key) is not None\n\n    def __iter__(self):\n        return iter(self._children().keys())\n\n    def __len__(self):\n        return len(self.keys())\n\n    def keys(self):\n        return self._children().keys()\n\n    def _dict(self):\n        return {k: self._get_child(k, idx) for k, idx in self._children().items()}\n\n    def values(self):\n        return self._dict().values()\n\n    def items(self):\n        return self._dict().items()\n</code></pre>"},{"location":"reference/metador_core/ih5/overlay/#metador_core.ih5.overlay.IH5InnerNode.__init__","title":"__init__","text":"<pre><code>__init__(\n    record: IH5Record,\n    gpath: str,\n    creation_idx: int,\n    attrs: bool = False,\n)\n</code></pre> <p>See <code>IH5Node</code> constructor.</p> <p>This variant represents an \"overlay container\", of which there are two types - a group (h5py.Group) and a set of attributes (h5py.AttributeManager).</p> <p>This class takes care of both (in order to avoid lots of code duplication), distinguishing them through the additional <code>attrs</code> flag.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>def __init__(\n    self,\n    record: IH5Record,\n    gpath: str,\n    creation_idx: int,\n    attrs: bool = False,\n):\n\"\"\"See `IH5Node` constructor.\n\n    This variant represents an \"overlay container\", of which there are two types -\n    a group (h5py.Group) and a set of attributes (h5py.AttributeManager).\n\n    This class takes care of both (in order to avoid lots of code duplication),\n    distinguishing them through the additional `attrs` flag.\n    \"\"\"\n    super().__init__(record, gpath, creation_idx)\n    # if attrs set, represents AttributeManager, otherwise its a group\n    self.__is_attrs__: bool = attrs\n</code></pre>"},{"location":"reference/metador_core/ih5/overlay/#metador_core.ih5.overlay.IH5Dataset","title":"IH5Dataset","text":"<p>             Bases: <code>IH5Node</code></p> <p><code>IH5Node</code> representing a <code>h5py.Dataset</code>, i.e. a leaf of the tree.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>class IH5Dataset(IH5Node):\n\"\"\"`IH5Node` representing a `h5py.Dataset`, i.e. a leaf of the tree.\"\"\"\n\n    def __init__(self, files, gpath, creation_idx):\n        super().__init__(files, gpath, creation_idx)\n\n    def copy_into_patch(self):\n\"\"\"Copy the most recent value at this path into the current patch.\n\n        This is useful e.g. for editing inside a complex value, such as an array.\n        \"\"\"\n        self._guard_open()\n        self._guard_read_only()\n        if self._cidx == self._last_idx:\n            raise ValueError(\"Cannot copy, this node is already from latest patch!\")\n        # copy value from older container to current patch\n        self._files[-1][self._gpath] = self[()]\n\n    # h5py-like interface\n    @property\n    def name(self) -&gt; str:\n        return self._gpath\n\n    @property\n    def file(self) -&gt; IH5Record:\n        return self._record\n\n    @property\n    def parent(self) -&gt; IH5Group:\n        return self._record[self._parent_path()]\n\n    @property\n    def attrs(self) -&gt; IH5AttributeManager:\n        self._guard_open()\n        return IH5AttributeManager(self._record, self._gpath, self._cidx)\n\n    # this one is also needed to work with H5DatasetLike\n    @property\n    def ndim(self) -&gt; int:\n        return self._files[self._cidx][self._gpath].ndim  # type: ignore\n\n    # for a dataset, instead of paths the numpy data is indexed. at this level\n    # the patching mechanism ends, so it's just passing through to h5py\n\n    def __getitem__(self, key):\n        # just pass through dataset indexing to underlying dataset\n        self._guard_open()\n        return self._files[self._cidx][self._gpath][key]  # type: ignore\n\n    def __setitem__(self, key, val):\n        self._guard_open()\n        self._guard_read_only()\n        if self._cidx != self._last_idx:\n            raise ValueError(f\"Cannot set '{key}', node is not from the latest patch!\")\n        # if we're in the latest patch, allow writing as usual (pass through)\n        self._files[-1][self._gpath][key] = val  # type: ignore\n</code></pre>"},{"location":"reference/metador_core/ih5/overlay/#metador_core.ih5.overlay.IH5Dataset.copy_into_patch","title":"copy_into_patch","text":"<pre><code>copy_into_patch()\n</code></pre> <p>Copy the most recent value at this path into the current patch.</p> <p>This is useful e.g. for editing inside a complex value, such as an array.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>def copy_into_patch(self):\n\"\"\"Copy the most recent value at this path into the current patch.\n\n    This is useful e.g. for editing inside a complex value, such as an array.\n    \"\"\"\n    self._guard_open()\n    self._guard_read_only()\n    if self._cidx == self._last_idx:\n        raise ValueError(\"Cannot copy, this node is already from latest patch!\")\n    # copy value from older container to current patch\n    self._files[-1][self._gpath] = self[()]\n</code></pre>"},{"location":"reference/metador_core/ih5/overlay/#metador_core.ih5.overlay.IH5AttributeManager","title":"IH5AttributeManager","text":"<p>             Bases: <code>IH5InnerNode</code></p> <p><code>IH5Node</code> representing an <code>h5py.AttributeManager</code>.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>class IH5AttributeManager(IH5InnerNode):\n\"\"\"`IH5Node` representing an `h5py.AttributeManager`.\"\"\"\n\n    def __init__(self, files, gpath, creation_idx):\n        super().__init__(files, gpath, creation_idx, True)\n\n    def __setitem__(self, key: str, val):\n        self._guard_open()\n        self._guard_read_only()\n        self._guard_key(key)\n        self._guard_value(val)\n\n        # if path does not exist in current patch, just create \"virtual node\"\n        if self._gpath not in self._files[-1]:\n            self._files[-1].create_group(self._gpath)\n        # deletion marker at `key` (if set) is overwritten automatically here\n        # so no need to worry about removing it before assigning `val`\n        self._files[-1][self._gpath].attrs[key] = val\n\n    def __delitem__(self, key: str):\n        self._guard_open()\n        self._guard_read_only()\n        self._guard_key(key)\n        # remove the entity if it is found in newest container,\n        # mark the path as deleted if doing a patch and not working on base container\n        if self._expect_real_item_idx(key) == self._last_idx:\n            del self._files[-1][self._gpath].attrs[key]\n        if len(self._files) &gt; 1:  # is a patch?\n            if self._gpath not in self._files[-1]:  # no node at path in latest?\n                self._files[-1].create_group(self._gpath)  # create \"virtual\" node\n            self._files[-1][self._gpath].attrs[key] = DEL_VALUE  # mark deleted\n</code></pre>"},{"location":"reference/metador_core/ih5/overlay/#metador_core.ih5.overlay.IH5Group","title":"IH5Group","text":"<p>             Bases: <code>IH5InnerNode</code></p> <p><code>IH5Node</code> representing a <code>h5py.Group</code>.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>class IH5Group(IH5InnerNode):\n\"\"\"`IH5Node` representing a `h5py.Group`.\"\"\"\n\n    def _require_node(self, name: str, node_type: Type[T]) -&gt; Optional[T]:\n        # helper for require_{group|dataset}\n        grp = self.get(name)\n        if isinstance(grp, node_type):\n            return grp\n        if grp is not None:\n            msg = f\"Incompatible object ({type(grp).__name__}) already exists\"\n            raise TypeError(msg)\n        return None\n\n    def __init__(self, record, gpath: str = \"/\", creation_idx: Optional[int] = None):\n        if gpath == \"/\":\n            creation_idx = 0\n        if creation_idx is None:\n            raise ValueError(\"Need creation_idx for path != '/'!\")\n        super().__init__(record, gpath, creation_idx, False)\n\n    def _create_virtual(self, path: str) -&gt; bool:\n        nodes = self._node_seq(path)\n        path = self._abs_path(path)\n        if (\n            nodes[-1]._gpath == path\n            and nodes[-1]._cidx == self._last_idx\n            and not _node_is_del_mark(nodes[-1])\n        ):\n            return False  # something at that path in most recent container exists\n\n        # most recent entity is a deletion marker or not existing?\n        if nodes[-1]._gpath != path or _node_is_del_mark(nodes[-1]):\n            suf_segs = nodes[-1]._rel_path(path).split(\"/\")\n            # create \"overwrite\" group in most recent patch...\n            self.create_group(f\"{nodes[-1]._gpath}/{suf_segs[0]}\")\n            # ... and create (nested) virtual group node(s), if needed\n            if len(suf_segs) &gt; 1:\n                self._files[-1].create_group(path)\n\n        return True\n\n    # h5py-like interface\n\n    def __setitem__(self, path: str, value):\n        return self.create_dataset(path, data=value)\n\n    def __delitem__(self, key: str):\n        self._guard_open()\n        self._guard_read_only()\n        self._guard_key(key)\n        self._expect_real_item_idx(key)\n        # remove the entity if it is found in newest container,\n        # mark the path as deleted if doing a patch and not working on base container\n        path = self._abs_path(key)\n        if path in self._files[-1]:\n            del self._files[-1][path]\n        if len(self._files) &gt; 1:  # has patches? mark deleted (instead of real delete)\n            self._files[-1][path] = DEL_VALUE\n\n    @property\n    def name(self) -&gt; str:\n        return self._gpath\n\n    @property\n    def file(self):  # -&gt; IH5Record\n        return self._record\n\n    @property\n    def parent(self) -&gt; IH5Group:\n        return self._record[self._parent_path()]\n\n    @property\n    def attrs(self) -&gt; IH5AttributeManager:\n        self._guard_open()\n        return IH5AttributeManager(self._record, self._gpath, self._cidx)\n\n    def create_group(self, name: str) -&gt; IH5Group:\n        self._guard_open()\n        self._guard_read_only()\n\n        path = self._abs_path(name)\n        nodes = self._node_seq(path)\n        if not isinstance(nodes[-1], IH5Group):\n            raise ValueError(f\"Cannot create group, {nodes[-1]._gpath} is a dataset!\")\n        if nodes[-1]._gpath == path:\n            raise ValueError(\"Cannot create group, it already exists!\")\n\n        # remove \"deleted\" marker, if set at current path in current patch container\n        if path in self._files[-1] and _node_is_del_mark(self._files[-1][path]):\n            del self._files[-1][path]\n        # create group (or fail if something else exists there already)\n        self._files[-1].create_group(path)\n        # if this is a patch: mark as non-virtual, i.e. \"overwrite\" with empty group\n        # because the intent here is to \"create\", not update something.\n        if len(self._files) &gt; 1:\n            self._files[-1][path].attrs[SUBST_KEY] = h5py.Empty(None)\n\n        return IH5Group(self._record, path, self._last_idx)\n\n    def create_dataset(\n        self, path: str, shape=None, dtype=None, data=None, **kwargs\n    ) -&gt; IH5Dataset:\n        self._guard_open()\n        self._guard_read_only()\n        self._guard_key(path)\n        self._guard_value(data)\n\n        if unknown_kwargs := set(kwargs.keys()) - {\"compression\", \"compression_opts\"}:\n            raise ValueError(f\"Unkown kwargs: {unknown_kwargs}\")\n\n        path = self._abs_path(path)\n        fidx = self._find(path)\n        if fidx is not None:\n            prev_val = self._get_child(path, fidx)\n            if isinstance(prev_val, (IH5Group, IH5Dataset)):\n                raise ValueError(\"Path exists, in order to replace - delete first!\")\n\n        if path in self._files[-1] and _node_is_del_mark(\n            self._get_child_raw(path, self._last_idx)\n        ):\n            # remove deletion marker in latest patch, if set\n            del self._files[-1][path]\n        elif path not in self._files[-1]:\n            # create path and overwrite-group in latest patch\n            self._create_virtual(path)\n            assert path in self._files[-1]\n            del self._files[-1][path]\n\n        self._files[-1].create_dataset(  # actually create it, finally\n            path, shape=shape, dtype=dtype, data=data, **kwargs\n        )\n        return IH5Dataset(self._record, path, self._last_idx)\n\n    def require_group(self, name: str) -&gt; IH5Group:\n        if (n := self._require_node(name, IH5Group)) is not None:\n            return n  # existing group\n        return self.create_group(name)\n\n    def require_dataset(self, name: str, *args, **kwds) -&gt; IH5Dataset:\n        if (n := self._require_node(name, IH5Dataset)) is not None:\n            # TODO: check dimensions etc, copy into patch if it fits\n            return n\n        return self.create_dataset(name, *args, **kwds)\n\n    def copy(self, source: CopySource, dest: CopyDest, **kwargs):\n        src_node = self[source] if isinstance(source, str) else source\n        name: str = kwargs.pop(\"name\", src_node.name.split(\"/\")[-1])\n        dst_name: str\n        if isinstance(dest, str):\n            # if dest is a path, ignore inferred/passed name\n            segs = self._abs_path(dest).split(\"/\")\n            dst_group = self.require_group(\"/\".join(segs[:-1]) or \"/\")\n            dst_name = segs[-1]\n        else:\n            # given dest is a group node, use inferred/passed name\n\n            dst_group = dest if dest.name != \"/\" else dest[\"/\"]  # *\n            # * ugly workaround for treating files as groups in the copy method\n\n            dst_name = name\n        return h5_copy_from_to(src_node, cast(Any, dst_group), dst_name, **kwargs)\n\n    def move(self, source: str, dest: str):\n        self.copy(source, dest)\n        del self[source]\n\n    def visititems(self, func: Callable[[str, object], Optional[Any]]) -&gt; Any:\n        self._guard_open()\n        stack = list(reversed(self._get_children()))\n        while stack:\n            curr = stack.pop()\n            val = func(self._rel_path(curr._gpath), curr)\n            if val is not None:\n                return val\n            if isinstance(curr, IH5Group):\n                stack += reversed(curr._get_children())\n\n    def visit(self, func: Callable[[str], Optional[Any]]) -&gt; Any:\n        return self.visititems(lambda x, _: func(x))\n</code></pre>"},{"location":"reference/metador_core/ih5/overlay/#metador_core.ih5.overlay.H5Type","title":"H5Type","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Type of an entity in a HDF5-like container.</p> <p>We list only those we care about, ignoring various link types etc.</p> <p>This will be used in wrappers around HDF5-like objects instead of using isinstance/subclass checks to implement duck-typing based decorator functionality that can work with (at least) raw HDF5, IH5 and IH5+Manifest.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>class H5Type(str, Enum):\n\"\"\"Type of an entity in a HDF5-like container.\n\n    We list only those we care about, ignoring various\n    link types etc.\n\n    This will be used in wrappers around HDF5-like objects\n    instead of using isinstance/subclass checks to implement\n    duck-typing based decorator functionality that can\n    work with (at least) raw HDF5, IH5 and IH5+Manifest.\n    \"\"\"\n\n    group = \"group\"  # possibly nested, dict-like\n    dataset = \"dataset\"  # = wrapped, indexable data\n    attribute_set = \"attribute-set\"  # = not further nested, dict-like\n    attribute = \"attribute\"  # = unwrapped data\n\n    def __repr__(self) -&gt; str:\n        return f\"{type(self).__name__}.{self.value}\"\n</code></pre>"},{"location":"reference/metador_core/ih5/overlay/#metador_core.ih5.overlay.h5_copy_from_to","title":"h5_copy_from_to","text":"<pre><code>h5_copy_from_to(\n    source_node: Union[H5DatasetLike, H5GroupLike],\n    target_group: H5GroupLike,\n    target_path: str,\n    **kwargs: str\n)\n</code></pre> <p>Copy a dataset or group from one container to a fresh location.</p> <p>This works also between HDF5 and IH5.</p> <p>Source node must be group or dataset object. Target node must be an existing group object. Target path must be fresh path relative to target node.</p> Source code in <code>src/metador_core/ih5/overlay.py</code> <pre><code>def h5_copy_from_to(\n    source_node: Union[H5DatasetLike, H5GroupLike],\n    target_group: H5GroupLike,\n    target_path: str,\n    **kwargs,\n):\n\"\"\"Copy a dataset or group from one container to a fresh location.\n\n    This works also between HDF5 and IH5.\n\n    Source node must be group or dataset object.\n    Target node must be an existing group object.\n    Target path must be fresh path relative to target node.\n    \"\"\"\n    without_attrs: bool = kwargs.pop(\"without_attrs\", False)\n    shallow: bool = kwargs.pop(\"shallow\", False)\n    for arg in [\"expand_soft\", \"expand_external\", \"expand_refs\"]:\n        if not kwargs.pop(arg, True):\n            raise ValueError(\"IH5 does not support keeping references!\")\n    if kwargs:\n        raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n\n    if not target_path or target_path[0] == \"/\":\n        raise ValueError(\"Target path must be non-empty and relative!\")\n    if target_path in target_group:\n        raise ValueError(f\"Target path {target_path} already exists in target group!\")\n\n    def copy_attrs(src_node, trg_node):\n        if not without_attrs:\n            trg_atrs = trg_node.attrs\n            for k, v in src_node.attrs.items():\n                trg_atrs[k] = v\n\n    if isinstance(source_node, H5DatasetLike):\n        node = target_group.create_dataset(target_path, data=source_node[()])\n        copy_attrs(source_node, node)  # copy dataset attributes\n    else:\n        trg_root = target_group.create_group(target_path)\n        copy_attrs(source_node, trg_root)  # copy source node attributes\n\n        def copy_children(name, src_child):\n            # name is relative to source root -&gt; can use it\n            if isinstance(src_child, H5DatasetLike):\n                trg_root[name] = src_child[()]\n            else:  # must be grouplike\n                trg_root.create_group(name)\n            copy_attrs(src_child, trg_root[name])\n\n        if shallow:  # only immediate children\n            for name, src_child in source_node.items():\n                copy_children(name, src_child)\n        else:  # recursive copy\n            source_node.visititems(copy_children)\n</code></pre>"},{"location":"reference/metador_core/ih5/record/","title":"record","text":"<p>Immutable HDF5 container records.</p> <p>A record consists of a base container and a number of patch containers. This allows a record to work in settings where files are immutable, but still provide a structured way of updating data stored inside.</p> <p>Both base containers and patches are HDF5 files that are linked together by some special attributes in the container root. <code>IH5Record</code> is a class that wraps such a set of files. It features * support for record creation and updating * automatic handling of the patch mechanism (i.e., creating/finding corresponding files) * transparent access to data in the record (possibly spanning multiple files)</p>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.FORMAT_MAGIC_STR","title":"FORMAT_MAGIC_STR  <code>module-attribute</code>","text":"<pre><code>FORMAT_MAGIC_STR: Final[str] = 'ih5_v01'\n</code></pre> <p>Magic value at the beginning of the file to detect that an HDF5 file is valid IH5.</p>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.USER_BLOCK_SIZE","title":"USER_BLOCK_SIZE  <code>module-attribute</code>","text":"<pre><code>USER_BLOCK_SIZE: Final[int] = 1024\n</code></pre> <p>Space to reserve at beginning of each HDF5 file in bytes. Must be a power of 2 and at least 512 (required by HDF5).</p>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5UserBlock","title":"IH5UserBlock","text":"<p>             Bases: <code>BaseModel</code></p> <p>IH5 metadata object parser and writer for the HDF5 user block.</p> <p>The user block stores technical administrative information linking together multiple HDF5 files that form a base container + patch sequence.</p> <p>The userblock begins with the magic format string, followed by a newline, followed by a string encoding the length of the user block, followed by a newline, followed by a serialized JSON object without newlines containing the metadata, followed by a NUL byte.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>class IH5UserBlock(BaseModel):\n\"\"\"IH5 metadata object parser and writer for the HDF5 user block.\n\n    The user block stores technical administrative information linking together\n    multiple HDF5 files that form a base container + patch sequence.\n\n    The userblock begins with the magic format string, followed by a newline,\n    followed by a string encoding the length of the user block, followed by a newline,\n    followed by a serialized JSON object without newlines containing the metadata,\n    followed by a NUL byte.\n    \"\"\"\n\n    _userblock_size: int = PrivateAttr(default=USER_BLOCK_SIZE)\n\"\"\"User block size claimed in the block itself (second line).\"\"\"\n\n    record_uuid: UUID\n\"\"\"UUID linking together multiple HDF5 files that form a (patched) record.\"\"\"\n\n    patch_index: Annotated[int, Field(ge=0)]\n\"\"\"Index with the current revision number, i.e. the file is the n-th patch.\"\"\"\n\n    patch_uuid: UUID\n\"\"\"UUID representing a certain state of the data in the record.\"\"\"\n\n    prev_patch: Optional[UUID]\n\"\"\"UUID of the previous patch UUID (unless it is a base container, i.e. first one).\"\"\"\n\n    hdf5_hashsum: Optional[QualHashsumStr] = None\n\"\"\"Hashsum to verity integrity of the HDF5 data after the user block.\"\"\"\n\n    ub_exts: Dict[str, Any]\n\"\"\"Any extra metadata to be stored in the user block, unvalidated in dicts.\n\n    Subclasses must ensure that desired extra metadata is stored and loaded correctly.\n\n    NOTE: In a merge of userblocks only newer extension section will be preserved!\n    \"\"\"\n\n    @classmethod\n    def create(cls, prev: Optional[IH5UserBlock] = None) -&gt; IH5UserBlock:\n\"\"\"Create a new user block for a base or patch container.\n\n        If `prev` is None, will return a new base container block.\n        Otherwise, will return a block linking back to the passed `prev` block.\n        \"\"\"\n        ret = cls(\n            patch_uuid=uuid1(),\n            record_uuid=uuid1() if prev is None else prev.record_uuid,\n            patch_index=0 if prev is None else prev.patch_index + 1,\n            prev_patch=None if prev is None else prev.patch_uuid,\n            ub_exts={},\n        )\n        return ret\n\n    @classmethod\n    def _read_head_raw(cls, stream, ub_size: int) -&gt; Optional[Tuple[int, str]]:\n\"\"\"Try reading user block.\n\n        Args:\n            stream: the open binary file stream\n            ub_size: number of bytes to read\n\n        Returns:\n            (user block size claimed in block, embedded data until first NUL byte)\n            or None, if block does not look right.\n        \"\"\"\n        stream.seek(0)\n        probe = stream.read(ub_size)\n        dat = probe.decode(\"utf-8\").split(\"\\n\")\n        if len(dat) != 3 or dat[0] != FORMAT_MAGIC_STR:\n            return None\n        return (int(dat[1]), dat[2][: dat[2].find(\"\\x00\")])  # read until first NUL byte\n\n    @classmethod\n    def load(cls, filename: Path) -&gt; IH5UserBlock:\n\"\"\"Load a user block of the given HDF5 file.\"\"\"\n        with open(filename, \"rb\") as f:\n            # try smallest valid UB size first\n            head = cls._read_head_raw(f, 512)\n            if head is None:\n                raise ValueError(f\"{filename}: it doesn't look like a valid IH5 file!\")\n            if head[0] &gt; 512:  # if stored user block size is bigger, re-read\n                head = cls._read_head_raw(f, head[0])\n                assert head is not None\n        ret = IH5UserBlock.parse_obj(json.loads(head[1]))\n        ret._userblock_size = head[0]\n        return ret\n\n    def save(self, filename: Union[Path, str]):\n\"\"\"Save this object in the user block of the given HDF5 file.\"\"\"\n        filename = Path(filename)\n        dat_str = f\"{FORMAT_MAGIC_STR}\\n{self._userblock_size}\\n{self.json()}\"\n        data = dat_str.encode(\"utf-8\")\n        assert len(data) &lt; USER_BLOCK_SIZE\n        with open(filename, \"r+b\") as f:\n            # check that this HDF file actually has a user block\n            check = f.read(4)\n            if check == b\"\\x89HDF\":\n                raise ValueError(f\"{filename}: no user block reserved, can't write!\")\n            f.seek(0)\n            f.write(data)\n            f.write(b\"\\x00\")  # mark end of the data\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5UserBlock.record_uuid","title":"record_uuid  <code>instance-attribute</code>","text":"<pre><code>record_uuid: UUID\n</code></pre> <p>UUID linking together multiple HDF5 files that form a (patched) record.</p>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5UserBlock.patch_index","title":"patch_index  <code>instance-attribute</code>","text":"<pre><code>patch_index: Annotated[int, Field(ge=0)]\n</code></pre> <p>Index with the current revision number, i.e. the file is the n-th patch.</p>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5UserBlock.patch_uuid","title":"patch_uuid  <code>instance-attribute</code>","text":"<pre><code>patch_uuid: UUID\n</code></pre> <p>UUID representing a certain state of the data in the record.</p>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5UserBlock.prev_patch","title":"prev_patch  <code>instance-attribute</code>","text":"<pre><code>prev_patch: Optional[UUID]\n</code></pre> <p>UUID of the previous patch UUID (unless it is a base container, i.e. first one).</p>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5UserBlock.hdf5_hashsum","title":"hdf5_hashsum  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hdf5_hashsum: Optional[QualHashsumStr] = None\n</code></pre> <p>Hashsum to verity integrity of the HDF5 data after the user block.</p>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5UserBlock.ub_exts","title":"ub_exts  <code>instance-attribute</code>","text":"<pre><code>ub_exts: Dict[str, Any]\n</code></pre> <p>Any extra metadata to be stored in the user block, unvalidated in dicts.</p> <p>Subclasses must ensure that desired extra metadata is stored and loaded correctly.</p> <p>NOTE: In a merge of userblocks only newer extension section will be preserved!</p>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5UserBlock.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(prev: Optional[IH5UserBlock] = None) -&gt; IH5UserBlock\n</code></pre> <p>Create a new user block for a base or patch container.</p> <p>If <code>prev</code> is None, will return a new base container block. Otherwise, will return a block linking back to the passed <code>prev</code> block.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>@classmethod\ndef create(cls, prev: Optional[IH5UserBlock] = None) -&gt; IH5UserBlock:\n\"\"\"Create a new user block for a base or patch container.\n\n    If `prev` is None, will return a new base container block.\n    Otherwise, will return a block linking back to the passed `prev` block.\n    \"\"\"\n    ret = cls(\n        patch_uuid=uuid1(),\n        record_uuid=uuid1() if prev is None else prev.record_uuid,\n        patch_index=0 if prev is None else prev.patch_index + 1,\n        prev_patch=None if prev is None else prev.patch_uuid,\n        ub_exts={},\n    )\n    return ret\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5UserBlock.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(filename: Path) -&gt; IH5UserBlock\n</code></pre> <p>Load a user block of the given HDF5 file.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>@classmethod\ndef load(cls, filename: Path) -&gt; IH5UserBlock:\n\"\"\"Load a user block of the given HDF5 file.\"\"\"\n    with open(filename, \"rb\") as f:\n        # try smallest valid UB size first\n        head = cls._read_head_raw(f, 512)\n        if head is None:\n            raise ValueError(f\"{filename}: it doesn't look like a valid IH5 file!\")\n        if head[0] &gt; 512:  # if stored user block size is bigger, re-read\n            head = cls._read_head_raw(f, head[0])\n            assert head is not None\n    ret = IH5UserBlock.parse_obj(json.loads(head[1]))\n    ret._userblock_size = head[0]\n    return ret\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5UserBlock.save","title":"save","text":"<pre><code>save(filename: Union[Path, str])\n</code></pre> <p>Save this object in the user block of the given HDF5 file.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def save(self, filename: Union[Path, str]):\n\"\"\"Save this object in the user block of the given HDF5 file.\"\"\"\n    filename = Path(filename)\n    dat_str = f\"{FORMAT_MAGIC_STR}\\n{self._userblock_size}\\n{self.json()}\"\n    data = dat_str.encode(\"utf-8\")\n    assert len(data) &lt; USER_BLOCK_SIZE\n    with open(filename, \"r+b\") as f:\n        # check that this HDF file actually has a user block\n        check = f.read(4)\n        if check == b\"\\x89HDF\":\n            raise ValueError(f\"{filename}: no user block reserved, can't write!\")\n        f.seek(0)\n        f.write(data)\n        f.write(b\"\\x00\")  # mark end of the data\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5Record","title":"IH5Record","text":"<p>             Bases: <code>IH5Group</code></p> <p>Class representing a record, which consists of a collection of immutable files.</p> <p>One file is a base container (with no linked predecessor state), the remaining files are a linear sequence of patch containers.</p> <p>Runtime invariants to be upheld before/after each method call (after init):</p> <ul> <li>all files of an instance are open for reading (until <code>close()</code> is called)</li> <li>all files in <code>__files__</code> are in patch index order</li> <li>at most one file is open in writable mode (if any, it is the last one)</li> <li>modifications are possible only after <code>create_patch</code> was called     and until <code>commit_patch</code> or <code>discard_patch</code> was called, and at no other time</li> </ul> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>class IH5Record(IH5Group):\n\"\"\"Class representing a record, which consists of a collection of immutable files.\n\n    One file is a base container (with no linked predecessor state),\n    the remaining files are a linear sequence of patch containers.\n\n    Runtime invariants to be upheld before/after each method call (after __init__):\n\n    * all files of an instance are open for reading (until `close()` is called)\n    * all files in `__files__` are in patch index order\n    * at most one file is open in writable mode (if any, it is the last one)\n    * modifications are possible only after `create_patch` was called\n        and until `commit_patch` or `discard_patch` was called, and at no other time\n    \"\"\"\n\n    # Characters that may appear in a record name.\n    # (to be put into regex [..] symbol braces)\n    _ALLOWED_NAME_CHARS = r\"A-Za-z0-9\\-\"\n\n    # filenames for a record named NAME are of the shape:\n    # NAME[&lt;PATCH_INFIX&gt;.*]?&lt;FILE_EXT&gt;\n    # NOTE: the first symbol of these must be one NOT in ALLOWED_NAME_CHARS!\n    # This constraint is needed for correctly filtering filenames\n    _PATCH_INFIX = \".p\"\n    _FILE_EXT = \".ih5\"\n\n    # core \"wrapped\" objects\n    __files__: List[h5py.File]\n\n    # attributes\n    _closed: bool  # True after close()\n    _allow_patching: bool  # false iff opened with \"r\"\n    _ublocks: Dict[Path, IH5UserBlock]  # in-memory copy of HDF5 user blocks\n\n    def __new__(cls, *args, **kwargs):\n        ret = super().__new__(cls)\n        ret._allow_patching = True\n        ret.__files__ = []\n        return ret\n\n    def __eq__(self, o) -&gt; bool:\n        if not isinstance(o, IH5Group):\n            return False\n        return self._record._files == o._record._files\n\n    @property\n    def _has_writable(self):\n\"\"\"Return True iff an uncommitted patch exists.\"\"\"\n        if not self.__files__:\n            return False\n        f = self.__files__[-1]\n        return bool(f) and f.mode == \"r+\"\n\n    @classmethod\n    def _is_valid_record_name(cls, name: str) -&gt; bool:\n\"\"\"Return whether a record name is valid.\"\"\"\n        return re.match(f\"^[{cls._ALLOWED_NAME_CHARS}]+$\", name) is not None\n\n    @classmethod\n    def _base_filename(cls, record_path: Path) -&gt; Path:\n\"\"\"Given a record path, return path to canonical base container name.\"\"\"\n        return Path(f\"{record_path}{cls._FILE_EXT}\")\n\n    @classmethod\n    def _infer_name(cls, record_path: Path) -&gt; str:\n        return record_path.name.split(cls._FILE_EXT)[0].split(cls._PATCH_INFIX)[0]\n\n    def _next_patch_filepath(self) -&gt; Path:\n\"\"\"Compute filepath for the next patch based on the previous one.\"\"\"\n        path = Path(self.__files__[0].filename)\n        parent = path.parent\n        patch_index = self._ublock(-1).patch_index + 1\n        res = f\"{parent}/{self._infer_name(path)}{self._PATCH_INFIX}{patch_index}{self._FILE_EXT}\"\n        return Path(res)\n\n    def _ublock(self, obj: Union[h5py.File, int]) -&gt; IH5UserBlock:\n\"\"\"Return the parsed user block of a container file.\"\"\"\n        f: h5py.File = obj if isinstance(obj, h5py.File) else self.__files__[obj]\n        return self._ublocks[Path(f.filename)]\n\n    def _set_ublock(self, obj: Union[h5py.File, int], ub: IH5UserBlock):\n        f: h5py.File = obj if isinstance(obj, h5py.File) else self.__files__[obj]\n        self._ublocks[Path(f.filename)] = ub\n\n    @classmethod\n    def _new_container(cls, path: Path, ub: IH5UserBlock) -&gt; h5py.File:\n\"\"\"Initialize a fresh container file with reserved user block.\"\"\"\n        # create if does not exist, fail if it does\n        f = h5py.File(path, mode=\"x\", userblock_size=USER_BLOCK_SIZE)\n        # close to pre-fill userblock\n        f.close()\n        ub.save(path)\n        # reopen the container file\n        return h5py.File(path, \"r+\")\n\n    def _check_ublock(\n        self,\n        filename: Union[str, Path],\n        ub: IH5UserBlock,\n        prev: Optional[IH5UserBlock] = None,\n        check_hashsum: bool = True,\n    ):\n\"\"\"Check given container file.\n\n        If `prev` block is given, assumes that `ub` is from a patch container,\n        otherwise from base container.\n        \"\"\"\n        filename = Path(filename)\n        # check presence+validity of record uuid (should be the same for all)\n        if ub.record_uuid != self.ih5_uuid:\n            msg = \"'record_uuid' inconsistent! Mixed up records?\"\n            raise ValueError(f\"{filename}: {msg}\")\n\n        # hash must match with HDF5 content (i.e. integrity check)\n        if check_hashsum and ub.hdf5_hashsum is None:\n            msg = \"hdf5_checksum is missing!\"\n            raise ValueError(f\"{filename}: {msg}\")\n        if ub.hdf5_hashsum is not None:\n            chksum = hashsum_file(filename, skip_bytes=USER_BLOCK_SIZE)\n            if ub.hdf5_hashsum != chksum:\n                msg = \"file has been modified, stored and computed checksum are different!\"\n                raise ValueError(f\"{filename}: {msg}\")\n\n        # check patch chain structure\n        if prev is not None:\n            if ub.patch_index &lt;= prev.patch_index:\n                msg = \"patch container must have greater index than predecessor!\"\n                raise ValueError(f\"{filename}: {msg}\")\n            if ub.prev_patch is None:\n                msg = \"patch must have an attribute 'prev_patch'!\"\n                raise ValueError(f\"{filename}: {msg}\")\n            # claimed predecessor uuid must match with the predecessor by index\n            # (can compare as strings directly, as we checked those already)\n            if ub.prev_patch != prev.patch_uuid:\n                msg = f\"patch for {ub.prev_patch}, but predecessor is {prev.patch_uuid}\"\n                raise ValueError(f\"{filename}: {msg}\")\n\n    def _expect_open(self):\n        if self._closed:\n            raise ValueError(\"Record is not open!\")\n\n    def _clear(self):\n\"\"\"Clear all contents of the record.\"\"\"\n        for k in self.attrs.keys():\n            del self.attrs[k]\n        for k in self.keys():\n            del self[k]\n\n    def _is_empty(self) -&gt; bool:\n\"\"\"Return whether this record currently contains any data.\"\"\"\n        return not self.attrs.keys() and not self.keys()\n\n    @classmethod\n    def _create(cls: Type[T], record: Union[Path, str], truncate: bool = False) -&gt; T:\n\"\"\"Create a new record consisting of a base container.\n\n        The base container is exposed as the `writable` container.\n        \"\"\"\n        record = Path(record)  # in case it was a str\n        if not cls._is_valid_record_name(record.name):\n            raise ValueError(f\"Invalid record name: '{record.name}'\")\n        path = cls._base_filename(record)\n\n        # if overwrite flag is set, check and remove old record if present\n        if truncate and path.is_file():\n            cls.delete_files(record)\n\n        # create new container\n        ret = cls.__new__(cls)\n        super().__init__(ret, ret)\n        ret._closed = False\n\n        ub = IH5UserBlock.create(prev=None)\n        ret._ublocks = {path: ub}\n\n        ret.__files__ = [cls._new_container(path, ub)]\n        return ret\n\n    @classmethod\n    def _open(cls: Type[T], paths: List[Path], **kwargs) -&gt; T:\n\"\"\"Open a record consisting of a base container + possible set of patches.\n\n        Expects a set of full file paths forming a valid record.\n        Will throw an exception in case of a detected inconsistency.\n\n        Will open latest patch in writable mode if it lacks a hdf5 checksum.\n        \"\"\"\n        if not paths:\n            raise ValueError(\"Cannot open empty list of containers!\")\n        allow_baseless: bool = kwargs.pop(\"allow_baseless\", False)\n\n        ret = cls.__new__(cls)\n        super().__init__(ret, ret)\n        ret._closed = False\n\n        ret._ublocks = {Path(path): IH5UserBlock.load(path) for path in paths}\n        # files, sorted  by patch index order (important!)\n        # if something is wrong with the indices, this will throw an exception.\n        ret.__files__ = [h5py.File(path, \"r\") for path in paths]\n        ret.__files__.sort(key=lambda f: ret._ublock(f).patch_index)\n        # ----\n        has_patches: bool = len(ret.__files__) &gt; 1\n\n        # check containers and relationship to each other:\n\n        # check first container (it could be a base container and it has no predecessor)\n        if not allow_baseless and ret._ublock(0).prev_patch is not None:\n            msg = \"base container must not have attribute 'prev_patch'!\"\n            raise ValueError(f\"{ret.__files__[0].filename}: {msg}\")\n        ret._check_ublock(ret.__files__[0].filename, ret._ublock(0), None, has_patches)\n\n        # check patches except last one (with checking the hashsum)\n        for i in range(1, len(ret.__files__) - 1):\n            filename = ret.__files__[i].filename\n            ret._check_ublock(filename, ret._ublock(i), ret._ublock(i - 1), True)\n        if has_patches:  # check latest patch (without checking hashsum)\n            ret._check_ublock(\n                ret.__files__[-1].filename, ret._ublock(-1), ret._ublock(-2), False\n            )\n\n        # now check whether the last container (patch or base or whatever) has a checksum\n        if ret._ublock(-1).hdf5_hashsum is None:\n            if kwargs.pop(\"reopen_incomplete_patch\", False):\n                # if opening in writable mode, allow to complete the patch\n                f = ret.__files__[-1]\n                path = ret.__files__[-1].filename\n                f.close()\n                ret.__files__[-1] = h5py.File(Path(path), \"r+\")\n\n        # additional sanity check: container uuids must be all distinct\n        cn_uuids = {ret._ublock(f).patch_uuid for f in ret.__files__}\n        if len(cn_uuids) != len(ret.__files__):\n            raise ValueError(\"Some patch_uuid is not unique, invalid file set!\")\n        # all looks good\n        return ret\n\n    # ---- public attributes and interface ----\n\n    @property\n    def ih5_uuid(self) -&gt; UUID:\n\"\"\"Return the common record UUID of the set of containers.\"\"\"\n        return self._ublock(0).record_uuid\n\n    @property\n    def ih5_files(self) -&gt; List[Path]:\n\"\"\"List of container filenames this record consists of.\"\"\"\n        return [Path(f.filename) for f in self.__files__]\n\n    @property\n    def ih5_meta(self) -&gt; List[IH5UserBlock]:\n\"\"\"Return user block metadata, in container patch order.\"\"\"\n        return [self._ublock(i).copy() for i in range(len(self.__files__))]\n\n    @classmethod\n    def find_files(cls, record: Path) -&gt; List[Path]:\n\"\"\"Return file names that look like they belong to the same record.\n\n        This operation is based on purely syntactic pattern matching on file names\n        that follow the default naming convention.\n\n        Given a path `/foo/bar`, will find all containers in directory\n        `/foo` whose name starts with `bar` followed by the correct file extension(s),\n        such as `/foo/bar.ih5` and `/foo/bar.p1.ih5`.\n        \"\"\"\n        record = Path(record)  # in case it was a str\n        if not cls._is_valid_record_name(record.name):\n            raise ValueError(f\"Invalid record name: '{record.name}'\")\n\n        globstr = f\"{record.name}*{cls._FILE_EXT}\"  # rough wildcard pattern\n        # filter out possible false positives (i.e. foobar* matching foo* as well)\n        return [\n            p\n            for p in record.parent.glob(globstr)\n            if re.match(f\"^{record.name}[^{cls._ALLOWED_NAME_CHARS}]\", p.name)\n        ]\n\n    @classmethod\n    def list_records(cls, dir: Path) -&gt; List[Path]:\n\"\"\"Return paths of records found in the given directory.\n\n        Will NOT recurse into subdirectories.\n\n        This operation is based on purely syntactic pattern matching on file names\n        that follow the default naming convention (i.e. just as `find_files`).\n\n        Returned paths can be used as-is for opening the (supposed) record.\n        \"\"\"\n        dir = Path(dir)  # in case it was a str\n        if not dir.is_dir():\n            raise ValueError(f\"'{dir}' is not a directory\")\n\n        ret = []\n        namepat = f\"[{cls._ALLOWED_NAME_CHARS}]+(?=[^{cls._ALLOWED_NAME_CHARS}])\"\n        for p in dir.glob(f\"*{cls._FILE_EXT}\"):\n            if m := re.match(namepat, p.name):\n                ret.append(m[0])\n        return list(map(lambda name: dir / name, set(ret)))\n\n    def __init__(\n        self, record: Union[str, Path, List[Path]], mode: OpenMode = \"r\", **kwargs\n    ):\n\"\"\"Open or create a record.\n\n        This method uses `find_files` to infer the correct set of files syntactically.\n\n        The open mode semantics are the same as for h5py.File.\n\n        If the mode is 'r', then creating, committing or discarding patches is disabled.\n\n        If the mode is 'a' or 'r+', then a new patch will be created in case the latest\n        patch has already been committed.\n        \"\"\"\n        super().__init__(self)\n\n        if isinstance(record, list):\n            if mode[0] == \"w\" or mode == \"x\":\n                raise ValueError(\"Pass a prefix path for creating or overwriting!\")\n            paths = record\n        else:\n            paths = None\n            path: Path = Path(record)\n\n        if mode not in _OPEN_MODES:\n            raise ValueError(f\"Unknown file open mode: {mode}\")\n\n        if mode[0] == \"w\" or mode == \"x\":\n            # create new or overwrite to get new\n            ret = self._create(path, truncate=(mode == \"w\"))\n            self.__dict__.update(ret.__dict__)\n            return\n\n        if mode == \"a\" or mode[0] == \"r\":\n            if not paths:  # user passed a path prefix -&gt; find files\n                paths = self.find_files(path)  # type: ignore\n\n            if not paths:  # no files were found\n                if mode != \"a\":  # r/r+ need existing containers\n                    raise FileNotFoundError(f\"No files found for record: {path}\")\n                else:  # 'a' means create new if not existing (will be writable)\n                    ret = self._create(path, truncate=False)\n                    self.__dict__.update(ret.__dict__)\n                    return\n\n            # open existing (will be ro if everything is fine, writable if latest patch was uncommitted)\n            want_rw = mode != \"r\"\n            ret = self._open(paths, reopen_incomplete_patch=want_rw, **kwargs)\n            self.__dict__.update(ret.__dict__)\n            self._allow_patching = want_rw\n\n            if want_rw and not self._has_writable:\n                # latest patch was completed correctly -&gt; make writable by creating new patch\n                self.create_patch()\n\n    @property\n    def mode(self) -&gt; Literal[\"r\", \"r+\"]:\n        return \"r+\" if self._allow_patching else \"r\"\n\n    def close(self, commit: bool = True) -&gt; None:\n\"\"\"Close all files that belong to this record.\n\n        If there exists an uncommited patch, it will be committed\n        (unless `commit` is set to false).\n\n        After this, the object may not be used anymore.\n        \"\"\"\n        if self._closed:\n            return  # nothing to do\n\n        if self._has_writable and commit:\n            self.commit_patch()\n        for f in self.__files__:\n            f.close()\n        self.__files__ = []\n        self._closed = True\n\n    def _expect_not_ro(self):\n        if self.mode == \"r\":\n            raise ValueError(\"The container is opened as read-only!\")\n\n    def create_patch(self) -&gt; None:\n\"\"\"Create a new patch in order to update the record.\"\"\"\n        self._expect_open()\n        self._expect_not_ro()\n        if self._has_writable:\n            raise ValueError(\"There already exists a writable container, commit first!\")\n\n        path = self._next_patch_filepath()\n        ub = IH5UserBlock.create(prev=self._ublock(-1))\n        self.__files__.append(self._new_container(path, ub))\n        self._ublocks[path] = ub\n\n    def _delete_latest_container(self) -&gt; None:\n\"\"\"Discard the current writable container (patch or base).\"\"\"\n        cfile = self.__files__.pop()\n        fn = cfile.filename\n        del self._ublocks[Path(fn)]\n        cfile.close()\n        Path(fn).unlink()\n\n    def discard_patch(self) -&gt; None:\n\"\"\"Discard the current incomplete patch container.\"\"\"\n        self._expect_open()\n        self._expect_not_ro()\n        if not self._has_writable:\n            raise ValueError(\"No patch to discard!\")\n        if len(self.__files__) == 1:\n            raise ValueError(\"Cannot discard base container! Just delete the file!\")\n            # reason: the base container provides record_uuid,\n            # destroying it makes this object inconsistent / breaks invariants\n            # so if this is done, it should not be used anymore.\n        return self._delete_latest_container()\n\n    def commit_patch(self, **kwargs) -&gt; None:\n\"\"\"Complete the current writable container (base or patch) for the record.\n\n        Will perform checks on the new container and throw an exception on failure.\n\n        After committing the patch is completed and cannot be edited anymore, so\n        any further modifications must go into a new patch.\n        \"\"\"\n        if kwargs:\n            raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n\n        self._expect_open()\n        self._expect_not_ro()\n        if not self._has_writable:\n            raise ValueError(\"No patch to commit!\")\n        cfile = self.__files__[-1]\n        filepath = Path(cfile.filename)\n        cfile.close()  # must close it now, as we will write outside of HDF5 next\n\n        # compute checksum, write user block\n        chksum = hashsum_file(filepath, skip_bytes=USER_BLOCK_SIZE)\n        self._ublocks[filepath].hdf5_hashsum = QualHashsumStr(chksum)\n        self._ublocks[filepath].save(filepath)\n\n        # reopen the container file now as read-only\n        self.__files__[-1] = h5py.File(filepath, \"r\")\n\n    def _fixes_after_merge(self, merged_file, ub):\n\"\"\"Run hook for subclasses into merge process.\n\n        The method is called after creating the merged container, but before\n        updating its user block on disk.\n\n        The passed userblock is a prepared userblock with updated HDF5 hashsum for the\n        merged container and adapted prev_patch field, as will it be written to the file.\n        Additional changes done to it in-place will be included.\n\n        The passed filename can be used to perform additional necessary actions.\n        \"\"\"\n\n    def merge_files(self, target: Path) -&gt; Path:\n\"\"\"Given a path with a record name, merge current record into new container.\n\n        Returns new resulting container.\n        \"\"\"\n        self._expect_open()\n        if self._has_writable:\n            raise ValueError(\"Cannot merge, please commit or discard your changes!\")\n\n        with type(self)(target, \"x\") as ds:\n            source_node = self[\"/\"]\n            target_node = ds[\"/\"]\n            for k, v in source_node.attrs.items():  # copy root attributes\n                target_node.attrs[k] = v\n            for name in source_node.keys():  # copy each entity (will recurse)\n                h5_copy_from_to(source_node[name], target_node, name)\n\n            cfile = ds.ih5_files[0]  # store filename to override userblock afterwards\n\n        # compute new merged userblock\n        ub = self._ublock(-1).copy(update={\"prev_patch\": self._ublock(0).prev_patch})\n        # update hashsum with saved new merged hdf5 payload\n        chksum = hashsum_file(cfile, skip_bytes=USER_BLOCK_SIZE)\n        ub.hdf5_hashsum = QualHashsumStr(chksum)\n\n        self._fixes_after_merge(cfile, ub)  # for subclass hooks\n\n        self._set_ublock(-1, ub)\n        ub.save(cfile)\n        return cfile\n\n    @classmethod\n    def delete_files(cls, record: Path):\n\"\"\"Irreversibly(!) delete all containers matching the record path.\n\n        This object is invalid after this operation.\n        \"\"\"\n        for file in cls.find_files(record):\n            file.unlink()\n\n    def __repr__(self):\n        return f\"&lt;IH5 record (mode {self.mode}) {self.__files__}&gt;\"\n\n    # ---- context manager support (i.e. to use `with`) ----\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, ex_type, ex_value, ex_traceback):\n        # this will ensure that commit_patch() is called and the files are closed\n        self.close()\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5Record.ih5_uuid","title":"ih5_uuid  <code>property</code>","text":"<pre><code>ih5_uuid: UUID\n</code></pre> <p>Return the common record UUID of the set of containers.</p>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5Record.ih5_files","title":"ih5_files  <code>property</code>","text":"<pre><code>ih5_files: List[Path]\n</code></pre> <p>List of container filenames this record consists of.</p>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5Record.ih5_meta","title":"ih5_meta  <code>property</code>","text":"<pre><code>ih5_meta: List[IH5UserBlock]\n</code></pre> <p>Return user block metadata, in container patch order.</p>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5Record.find_files","title":"find_files  <code>classmethod</code>","text":"<pre><code>find_files(record: Path) -&gt; List[Path]\n</code></pre> <p>Return file names that look like they belong to the same record.</p> <p>This operation is based on purely syntactic pattern matching on file names that follow the default naming convention.</p> <p>Given a path <code>/foo/bar</code>, will find all containers in directory <code>/foo</code> whose name starts with <code>bar</code> followed by the correct file extension(s), such as <code>/foo/bar.ih5</code> and <code>/foo/bar.p1.ih5</code>.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>@classmethod\ndef find_files(cls, record: Path) -&gt; List[Path]:\n\"\"\"Return file names that look like they belong to the same record.\n\n    This operation is based on purely syntactic pattern matching on file names\n    that follow the default naming convention.\n\n    Given a path `/foo/bar`, will find all containers in directory\n    `/foo` whose name starts with `bar` followed by the correct file extension(s),\n    such as `/foo/bar.ih5` and `/foo/bar.p1.ih5`.\n    \"\"\"\n    record = Path(record)  # in case it was a str\n    if not cls._is_valid_record_name(record.name):\n        raise ValueError(f\"Invalid record name: '{record.name}'\")\n\n    globstr = f\"{record.name}*{cls._FILE_EXT}\"  # rough wildcard pattern\n    # filter out possible false positives (i.e. foobar* matching foo* as well)\n    return [\n        p\n        for p in record.parent.glob(globstr)\n        if re.match(f\"^{record.name}[^{cls._ALLOWED_NAME_CHARS}]\", p.name)\n    ]\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5Record.list_records","title":"list_records  <code>classmethod</code>","text":"<pre><code>list_records(dir: Path) -&gt; List[Path]\n</code></pre> <p>Return paths of records found in the given directory.</p> <p>Will NOT recurse into subdirectories.</p> <p>This operation is based on purely syntactic pattern matching on file names that follow the default naming convention (i.e. just as <code>find_files</code>).</p> <p>Returned paths can be used as-is for opening the (supposed) record.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>@classmethod\ndef list_records(cls, dir: Path) -&gt; List[Path]:\n\"\"\"Return paths of records found in the given directory.\n\n    Will NOT recurse into subdirectories.\n\n    This operation is based on purely syntactic pattern matching on file names\n    that follow the default naming convention (i.e. just as `find_files`).\n\n    Returned paths can be used as-is for opening the (supposed) record.\n    \"\"\"\n    dir = Path(dir)  # in case it was a str\n    if not dir.is_dir():\n        raise ValueError(f\"'{dir}' is not a directory\")\n\n    ret = []\n    namepat = f\"[{cls._ALLOWED_NAME_CHARS}]+(?=[^{cls._ALLOWED_NAME_CHARS}])\"\n    for p in dir.glob(f\"*{cls._FILE_EXT}\"):\n        if m := re.match(namepat, p.name):\n            ret.append(m[0])\n    return list(map(lambda name: dir / name, set(ret)))\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5Record.__init__","title":"__init__","text":"<pre><code>__init__(\n    record: Union[str, Path, List[Path]],\n    mode: OpenMode = \"r\",\n    **kwargs: OpenMode\n)\n</code></pre> <p>Open or create a record.</p> <p>This method uses <code>find_files</code> to infer the correct set of files syntactically.</p> <p>The open mode semantics are the same as for h5py.File.</p> <p>If the mode is 'r', then creating, committing or discarding patches is disabled.</p> <p>If the mode is 'a' or 'r+', then a new patch will be created in case the latest patch has already been committed.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def __init__(\n    self, record: Union[str, Path, List[Path]], mode: OpenMode = \"r\", **kwargs\n):\n\"\"\"Open or create a record.\n\n    This method uses `find_files` to infer the correct set of files syntactically.\n\n    The open mode semantics are the same as for h5py.File.\n\n    If the mode is 'r', then creating, committing or discarding patches is disabled.\n\n    If the mode is 'a' or 'r+', then a new patch will be created in case the latest\n    patch has already been committed.\n    \"\"\"\n    super().__init__(self)\n\n    if isinstance(record, list):\n        if mode[0] == \"w\" or mode == \"x\":\n            raise ValueError(\"Pass a prefix path for creating or overwriting!\")\n        paths = record\n    else:\n        paths = None\n        path: Path = Path(record)\n\n    if mode not in _OPEN_MODES:\n        raise ValueError(f\"Unknown file open mode: {mode}\")\n\n    if mode[0] == \"w\" or mode == \"x\":\n        # create new or overwrite to get new\n        ret = self._create(path, truncate=(mode == \"w\"))\n        self.__dict__.update(ret.__dict__)\n        return\n\n    if mode == \"a\" or mode[0] == \"r\":\n        if not paths:  # user passed a path prefix -&gt; find files\n            paths = self.find_files(path)  # type: ignore\n\n        if not paths:  # no files were found\n            if mode != \"a\":  # r/r+ need existing containers\n                raise FileNotFoundError(f\"No files found for record: {path}\")\n            else:  # 'a' means create new if not existing (will be writable)\n                ret = self._create(path, truncate=False)\n                self.__dict__.update(ret.__dict__)\n                return\n\n        # open existing (will be ro if everything is fine, writable if latest patch was uncommitted)\n        want_rw = mode != \"r\"\n        ret = self._open(paths, reopen_incomplete_patch=want_rw, **kwargs)\n        self.__dict__.update(ret.__dict__)\n        self._allow_patching = want_rw\n\n        if want_rw and not self._has_writable:\n            # latest patch was completed correctly -&gt; make writable by creating new patch\n            self.create_patch()\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5Record.close","title":"close","text":"<pre><code>close(commit: bool = True) -&gt; None\n</code></pre> <p>Close all files that belong to this record.</p> <p>If there exists an uncommited patch, it will be committed (unless <code>commit</code> is set to false).</p> <p>After this, the object may not be used anymore.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def close(self, commit: bool = True) -&gt; None:\n\"\"\"Close all files that belong to this record.\n\n    If there exists an uncommited patch, it will be committed\n    (unless `commit` is set to false).\n\n    After this, the object may not be used anymore.\n    \"\"\"\n    if self._closed:\n        return  # nothing to do\n\n    if self._has_writable and commit:\n        self.commit_patch()\n    for f in self.__files__:\n        f.close()\n    self.__files__ = []\n    self._closed = True\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5Record.create_patch","title":"create_patch","text":"<pre><code>create_patch() -&gt; None\n</code></pre> <p>Create a new patch in order to update the record.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def create_patch(self) -&gt; None:\n\"\"\"Create a new patch in order to update the record.\"\"\"\n    self._expect_open()\n    self._expect_not_ro()\n    if self._has_writable:\n        raise ValueError(\"There already exists a writable container, commit first!\")\n\n    path = self._next_patch_filepath()\n    ub = IH5UserBlock.create(prev=self._ublock(-1))\n    self.__files__.append(self._new_container(path, ub))\n    self._ublocks[path] = ub\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5Record.discard_patch","title":"discard_patch","text":"<pre><code>discard_patch() -&gt; None\n</code></pre> <p>Discard the current incomplete patch container.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def discard_patch(self) -&gt; None:\n\"\"\"Discard the current incomplete patch container.\"\"\"\n    self._expect_open()\n    self._expect_not_ro()\n    if not self._has_writable:\n        raise ValueError(\"No patch to discard!\")\n    if len(self.__files__) == 1:\n        raise ValueError(\"Cannot discard base container! Just delete the file!\")\n        # reason: the base container provides record_uuid,\n        # destroying it makes this object inconsistent / breaks invariants\n        # so if this is done, it should not be used anymore.\n    return self._delete_latest_container()\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5Record.commit_patch","title":"commit_patch","text":"<pre><code>commit_patch(**kwargs) -&gt; None\n</code></pre> <p>Complete the current writable container (base or patch) for the record.</p> <p>Will perform checks on the new container and throw an exception on failure.</p> <p>After committing the patch is completed and cannot be edited anymore, so any further modifications must go into a new patch.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def commit_patch(self, **kwargs) -&gt; None:\n\"\"\"Complete the current writable container (base or patch) for the record.\n\n    Will perform checks on the new container and throw an exception on failure.\n\n    After committing the patch is completed and cannot be edited anymore, so\n    any further modifications must go into a new patch.\n    \"\"\"\n    if kwargs:\n        raise ValueError(f\"Unknown keyword arguments: {kwargs}\")\n\n    self._expect_open()\n    self._expect_not_ro()\n    if not self._has_writable:\n        raise ValueError(\"No patch to commit!\")\n    cfile = self.__files__[-1]\n    filepath = Path(cfile.filename)\n    cfile.close()  # must close it now, as we will write outside of HDF5 next\n\n    # compute checksum, write user block\n    chksum = hashsum_file(filepath, skip_bytes=USER_BLOCK_SIZE)\n    self._ublocks[filepath].hdf5_hashsum = QualHashsumStr(chksum)\n    self._ublocks[filepath].save(filepath)\n\n    # reopen the container file now as read-only\n    self.__files__[-1] = h5py.File(filepath, \"r\")\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5Record.merge_files","title":"merge_files","text":"<pre><code>merge_files(target: Path) -&gt; Path\n</code></pre> <p>Given a path with a record name, merge current record into new container.</p> <p>Returns new resulting container.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def merge_files(self, target: Path) -&gt; Path:\n\"\"\"Given a path with a record name, merge current record into new container.\n\n    Returns new resulting container.\n    \"\"\"\n    self._expect_open()\n    if self._has_writable:\n        raise ValueError(\"Cannot merge, please commit or discard your changes!\")\n\n    with type(self)(target, \"x\") as ds:\n        source_node = self[\"/\"]\n        target_node = ds[\"/\"]\n        for k, v in source_node.attrs.items():  # copy root attributes\n            target_node.attrs[k] = v\n        for name in source_node.keys():  # copy each entity (will recurse)\n            h5_copy_from_to(source_node[name], target_node, name)\n\n        cfile = ds.ih5_files[0]  # store filename to override userblock afterwards\n\n    # compute new merged userblock\n    ub = self._ublock(-1).copy(update={\"prev_patch\": self._ublock(0).prev_patch})\n    # update hashsum with saved new merged hdf5 payload\n    chksum = hashsum_file(cfile, skip_bytes=USER_BLOCK_SIZE)\n    ub.hdf5_hashsum = QualHashsumStr(chksum)\n\n    self._fixes_after_merge(cfile, ub)  # for subclass hooks\n\n    self._set_ublock(-1, ub)\n    ub.save(cfile)\n    return cfile\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.IH5Record.delete_files","title":"delete_files  <code>classmethod</code>","text":"<pre><code>delete_files(record: Path)\n</code></pre> <p>Irreversibly(!) delete all containers matching the record path.</p> <p>This object is invalid after this operation.</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>@classmethod\ndef delete_files(cls, record: Path):\n\"\"\"Irreversibly(!) delete all containers matching the record path.\n\n    This object is invalid after this operation.\n    \"\"\"\n    for file in cls.find_files(record):\n        file.unlink()\n</code></pre>"},{"location":"reference/metador_core/ih5/record/#metador_core.ih5.record.hashsum_file","title":"hashsum_file","text":"<pre><code>hashsum_file(filename: Path, skip_bytes: int = 0) -&gt; str\n</code></pre> <p>Compute hashsum of HDF5 file (ignoring the first <code>skip_bytes</code>).</p> Source code in <code>src/metador_core/ih5/record.py</code> <pre><code>def hashsum_file(filename: Path, skip_bytes: int = 0) -&gt; str:\n\"\"\"Compute hashsum of HDF5 file (ignoring the first `skip_bytes`).\"\"\"\n    with open(filename, \"rb\") as f:\n        f.seek(skip_bytes)\n        return qualified_hashsum(f)\n</code></pre>"},{"location":"reference/metador_core/ih5/skeleton/","title":"skeleton","text":"<p>IH5 skeletons and stubs (low-level structures used by IH5MFRecord).</p> <p>A skeleton is documenting the tree structure of a HDF5-like container, ignoring the actual data content (attribute values and datasets).</p> <p>This can be used to implement manifest file and support \"patching in thin air\", i.e. without having the actual container.</p>"},{"location":"reference/metador_core/ih5/skeleton/#metador_core.ih5.skeleton.IH5Skeleton","title":"IH5Skeleton","text":"<p>             Bases: <code>BaseModel</code></p> Source code in <code>src/metador_core/ih5/skeleton.py</code> <pre><code>class IH5Skeleton(BaseModel):\n    __root__: Dict[str, SkeletonNodeInfo]\n\n    def with_patch_index(self, idx: int):\n\"\"\"Copy of skeletonwith  patch index of all nodes and attributes modified.\"\"\"\n        ret = self.copy()\n        ret.__root__ = dict(self.__root__)\n        for k, v in ret.__root__.items():\n            ret.__root__[k] = v.with_patch_index(idx)\n        return ret\n\n    @classmethod\n    def for_record(cls, rec: IH5Record):\n\"\"\"Return mapping from all paths in a IH5 record to their type.\n\n        The attributes are represented as special paths with the shape `a/b/.../n@attr`,\n        pointing to the attribute named `attr` at the path `a/b/.../n`.\n\n        First component is a H5Type enum value,\n        Second component is more detailed type for attribute values and `IH5Dataset`s.\n        \"\"\"\n        skel = {\"/\": SkeletonNodeInfo.for_node(rec[\"/\"])}\n\n        def add_paths(_, node):\n            skel[node.name] = SkeletonNodeInfo.for_node(node)\n\n        rec.visititems(add_paths)\n        return cls(__root__=skel)\n</code></pre>"},{"location":"reference/metador_core/ih5/skeleton/#metador_core.ih5.skeleton.IH5Skeleton.with_patch_index","title":"with_patch_index","text":"<pre><code>with_patch_index(idx: int)\n</code></pre> <p>Copy of skeletonwith  patch index of all nodes and attributes modified.</p> Source code in <code>src/metador_core/ih5/skeleton.py</code> <pre><code>def with_patch_index(self, idx: int):\n\"\"\"Copy of skeletonwith  patch index of all nodes and attributes modified.\"\"\"\n    ret = self.copy()\n    ret.__root__ = dict(self.__root__)\n    for k, v in ret.__root__.items():\n        ret.__root__[k] = v.with_patch_index(idx)\n    return ret\n</code></pre>"},{"location":"reference/metador_core/ih5/skeleton/#metador_core.ih5.skeleton.IH5Skeleton.for_record","title":"for_record  <code>classmethod</code>","text":"<pre><code>for_record(rec: IH5Record)\n</code></pre> <p>Return mapping from all paths in a IH5 record to their type.</p> <p>The attributes are represented as special paths with the shape <code>a/b/.../n@attr</code>, pointing to the attribute named <code>attr</code> at the path <code>a/b/.../n</code>.</p> <p>First component is a H5Type enum value, Second component is more detailed type for attribute values and <code>IH5Dataset</code>s.</p> Source code in <code>src/metador_core/ih5/skeleton.py</code> <pre><code>@classmethod\ndef for_record(cls, rec: IH5Record):\n\"\"\"Return mapping from all paths in a IH5 record to their type.\n\n    The attributes are represented as special paths with the shape `a/b/.../n@attr`,\n    pointing to the attribute named `attr` at the path `a/b/.../n`.\n\n    First component is a H5Type enum value,\n    Second component is more detailed type for attribute values and `IH5Dataset`s.\n    \"\"\"\n    skel = {\"/\": SkeletonNodeInfo.for_node(rec[\"/\"])}\n\n    def add_paths(_, node):\n        skel[node.name] = SkeletonNodeInfo.for_node(node)\n\n    rec.visititems(add_paths)\n    return cls(__root__=skel)\n</code></pre>"},{"location":"reference/metador_core/ih5/skeleton/#metador_core.ih5.skeleton.init_stub_skeleton","title":"init_stub_skeleton","text":"<pre><code>init_stub_skeleton(ds: IH5Record, skel: IH5Skeleton)\n</code></pre> <p>Fill a passed fresh container with stub structure based on a skeleton.</p> Source code in <code>src/metador_core/ih5/skeleton.py</code> <pre><code>def init_stub_skeleton(ds: IH5Record, skel: IH5Skeleton):\n\"\"\"Fill a passed fresh container with stub structure based on a skeleton.\"\"\"\n    if len(ds) or len(ds.attrs):\n        raise ValueError(\"Container not empty, cannot initialize stub structure here!\")\n\n    for k, v in skel.__root__.items():\n        if v.node_type == H5Type.group:\n            if k not in ds:\n                ds.create_group(k)\n        elif v.node_type == H5Type.dataset:\n            ds[k] = h5py.Empty(None)\n\n        for a in v.attrs.keys():\n            ds[k].attrs[a] = h5py.Empty(None)\n</code></pre>"},{"location":"reference/metador_core/ih5/skeleton/#metador_core.ih5.skeleton.init_stub_base","title":"init_stub_base","text":"<pre><code>init_stub_base(\n    target: IH5Record,\n    src_ub: IH5UserBlock,\n    src_skel: IH5Skeleton,\n)\n</code></pre> <p>Prepare a stub base container, given empty target, source user block and skeleton.</p> <p>Patches on top of this container will work with the original container.</p> Source code in <code>src/metador_core/ih5/skeleton.py</code> <pre><code>def init_stub_base(target: IH5Record, src_ub: IH5UserBlock, src_skel: IH5Skeleton):\n\"\"\"Prepare a stub base container, given empty target, source user block and skeleton.\n\n    Patches on top of this container will work with the original container.\n    \"\"\"\n    init_stub_skeleton(target, src_skel)\n    # mark as base container\n    target._set_ublock(-1, src_ub.copy(update={\"prev_patch\": None}))\n</code></pre>"},{"location":"reference/metador_core/packer/","title":"packer","text":"<p>Definition of HDF5 packer plugin interface.</p>"},{"location":"reference/metador_core/packer/#metador_core.packer.Packer","title":"Packer","text":"<p>             Bases: <code>ABC</code>, <code>EnforceOverrides</code></p> <p>Interface to be implemented by Metador HDF5 packer plugins.</p> <p>These plugins is how support for wildly different domain-specific use-cases can be added to Metador in a opt-in and loosely-coupled way.</p> <p>Users can install only the packer plugins they need for their use-cases, and such plugins can be easily developed independently from the rest of the Metador tooling, as long as this interface is respected.</p> <p>Carefully read the documentation for the required attributes and methods and implement them for your use-case in a subclass. See <code>metador_core.packer.example.GenericPacker</code> for an example plugin.</p> <p>Requirements for well-behaved packers:</p> <ol> <li> <p>No closing of the container: The packer gets a writable record and is only responsible for performing the neccessary additions, deletions and modifications. It is not allowed to <code>close()</code> the container.</p> </li> <li> <p>No access to data in the container: Data in the container MUST NOT be read or be relied on for doing an update, as the nodes could be dummy stubs. One MAY rely on existence or absence of Groups, Datasets, Attributes and Metadata in the container (e.g. <code>in</code> or <code>keys</code>).</p> </li> <li> <p>Source directory is read-only: Files or directories inside of <code>data_dir</code> MUST NOT be created, deleted or modified by this method.</p> </li> <li> <p>Exceptional termination: In case that packing must be aborted, and exception MUST be raised. If the exception happened due to invalid data or metadata, it MUST be an DirValidationError object like in the other methods above, helping to find and fix the problem. Otherwise, a different appropriate exception may be used.</p> </li> <li> <p>Semantic correctness: Packing a directory into a fresh container and updating an existing container MUST lead to the same observable result.</p> </li> </ol> <p>If you cannot guarantee this in full generality, do not implement <code>update</code>. In that case, if a container is updated, it will be cleared and then <code>pack</code> is called on it, as if it was a fresh container. In this case, there is no space advantage gained over a fresh container (but it will keep its UUID).</p> <ol> <li>Semantic versioning of packers:</li> </ol> <p>A packer MUST be able to update records that were created by this packer of the same or an earlier MINOR version.</p> <p>More formally, the version MAJOR.MINOR.PATCH MUST adhere to the following contract:</p> <ol> <li> <p>increasing MAJOR means a break in backward-compatibility for older datasets (i.e. new packer cannot work with old records),</p> </li> <li> <p>increasing MINOR means a break in forward-compatibility for newer datasets (i.e. older packers will not work with newer records),</p> </li> <li> <p>increasing PATCH does not affect compatibility for datasets with the same MAJOR and MINOR version.</p> </li> </ol> <p>When the packer is updated, the Python package version MUST increase in a suitable way. As usual, whenever an earlier number is increased, the following numbers are reset to zero.</p> <p>This means, the PATCH version should increase for e.g. bugfixes that do not change the structure or metadata stored in the dataset, MINOR should increase whenever from now on older versions of the packer would not be able to produce a valid update for a dataset created with this version, but upgrading older existing datasets with this version still works. Finally, MAJOR version is increased when all compatibility guarantees are off and the resulting container cannot be migrated or updated automatically.</p> <p>You SHOULD provide tooling to migrate datasets between major versions.</p> Source code in <code>src/metador_core/packer/__init__.py</code> <pre><code>class Packer(ABC, EnforceOverrides):\n\"\"\"Interface to be implemented by Metador HDF5 packer plugins.\n\n    These plugins is how support for wildly different domain-specific\n    use-cases can be added to Metador in a opt-in and loosely-coupled way.\n\n    Users can install only the packer plugins they need for their use-cases,\n    and such plugins can be easily developed independently from the rest\n    of the Metador tooling, as long as this interface is respected.\n\n    Carefully read the documentation for the required attributes and methods\n    and implement them for your use-case in a subclass.\n    See `metador_core.packer.example.GenericPacker` for an example plugin.\n\n    Requirements for well-behaved packers:\n\n    1. No closing of the container:\n    The packer gets a writable record and is only responsible for performing\n    the neccessary additions, deletions and modifications. It is not allowed\n    to `close()` the container.\n\n    2. No access to data in the container:\n    Data in the container MUST NOT be read or be relied on for doing an update,\n    as the nodes could be dummy stubs. One MAY rely on existence or absence of\n    Groups, Datasets, Attributes and Metadata in the container (e.g. `in` or `keys`).\n\n    3. Source directory is read-only:\n    Files or directories inside of `data_dir` MUST NOT be created, deleted or\n    modified by this method.\n\n    4. Exceptional termination:\n    In case that packing must be aborted, and exception MUST be raised.\n    If the exception happened due to invalid data or metadata, it MUST be\n    an DirValidationError object like in the other methods above, helping to find\n    and fix the problem. Otherwise, a different appropriate exception may be used.\n\n    5. Semantic correctness:\n    Packing a directory into a fresh container and updating an existing container\n    MUST lead to the same observable result.\n\n    If you cannot guarantee this in full generality, do not implement `update`.\n    In that case, if a container is updated, it will be cleared and then `pack` is\n    called on it, as if it was a fresh container. In this case, there is no space\n    advantage gained over a fresh container (but it will keep its UUID).\n\n    6. Semantic versioning of packers:\n\n    A packer MUST be able to update records that were created by this packer\n    of the same or an earlier MINOR version.\n\n    More formally, the version MAJOR.MINOR.PATCH\n    MUST adhere to the following contract:\n\n    1. increasing MAJOR means a break in backward-compatibility\n    for older datasets (i.e. new packer cannot work with old records),\n\n    2. increasing MINOR means a break in forward-compatibility\n    for newer datasets (i.e. older packers will not work with newer records),\n\n    3. increasing PATCH does not affect compatibility\n    for datasets with the same MAJOR and MINOR version.\n\n    When the packer is updated, the Python package version MUST increase\n    in a suitable way. As usual, whenever an earlier number is increased,\n    the following numbers are reset to zero.\n\n    This means, the PATCH version should increase for e.g. bugfixes that do\n    not change the structure or metadata stored in the dataset,\n    MINOR should increase whenever from now on older versions of the packer\n    would not be able to produce a valid update for a dataset created with this version,\n    but upgrading older existing datasets with this version still works.\n    Finally, MAJOR version is increased when all compatibility guarantees are off\n    and the resulting container cannot be migrated or updated automatically.\n\n    You SHOULD provide tooling to migrate datasets between major versions.\n    \"\"\"\n\n    Plugin: PackerPlugin\n\n    @classmethod\n    @abstractmethod\n    def check_dir(cls, data_dir: Path) -&gt; DirValidationErrors:\n\"\"\"Check whether the given directory is suitable for packing with this plugin.\n\n        This method will be called before `pack` or `update` and MUST detect\n        all problems (such as missing or invalid data or metadata) that can be\n        expected to be fixed by the user in preparation for the packing.\n\n        More specifically, it MUST cover all metadata that is to be provided directly by\n        the user (i.e. is not inferred or extracted from generated data) for the purpose\n        of packing and SHOULD try to cover as many problems with data and metadata as\n        possible to avoid failure during the actual packing process.\n\n        Files or directories inside of `data_dir` MUST NOT be created,\n        deleted or modified by this method.\n\n        Args:\n            data_dir: Directory containing all the data to be packed.\n\n        Returns:\n            DirValidationError initialized with a dict mapping file paths\n            (relative to `dir`) to lists of detected errors.\n\n            The errors must be either a string (containing a human-readable summary of all\n            problems with that file), or another dict with more granular error messages,\n            in case that the file is e.g. a JSON-compatible file subject to validation\n            with JSON Schemas.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    @abstractmethod\n    def update(cls, mc: MetadorContainer, data_dir: Path, diff: DirDiff):\n\"\"\"Update a MetadorContainer with changes done to the data source directory.\n\n        The `container` is assumed to be writable, and is either empty\n        or was previously packed by a compatible version of the packer.\n\n        The `data_dir` is assumed to be suitable (according to `check_dir`).\n\n        The `diff` structure contains information about changed paths.\n\n        If not implemented, updates will be created by clearing the provided\n        container and using `pack` on it.\n\n        Args:\n            mc: Metador IH5 record to pack the data into or update\n            data_dir: Directory containing all the data to be packed\n            diff: Diff tree of dirs and files in data_dir compared to a previous state\n        \"\"\"\n        # default fallback implementation using pack\n        for obj in [mc, mc.attrs, mc.meta]:\n            for key in obj.keys():\n                del obj[key]\n        return cls.pack(mc, data_dir)\n\n    @classmethod\n    @abstractmethod\n    def pack(cls, mc: MetadorContainer, data_dir: Path):\n\"\"\"Pack a directory into an MetadorContainer.\n\n        The `container` is assumed to be writable and empty.\n\n        The `data_dir` is assumed to be suitable (according to `check_dir`).\n\n        If not implemented, initial packing is done using `update`\n        with an empty container and a diff containing all the files.\n\n        Args:\n            mc: Metador IH5 record to pack the data into or update\n            data_dir: Directory containing all the data to be packed\n        \"\"\"\n        # default fallback implementation using update\n        return cls.update(mc, data_dir, DirDiff.compare({}, dir_hashsums(data_dir)))\n</code></pre>"},{"location":"reference/metador_core/packer/#metador_core.packer.Packer.check_dir","title":"check_dir  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>check_dir(data_dir: Path) -&gt; DirValidationErrors\n</code></pre> <p>Check whether the given directory is suitable for packing with this plugin.</p> <p>This method will be called before <code>pack</code> or <code>update</code> and MUST detect all problems (such as missing or invalid data or metadata) that can be expected to be fixed by the user in preparation for the packing.</p> <p>More specifically, it MUST cover all metadata that is to be provided directly by the user (i.e. is not inferred or extracted from generated data) for the purpose of packing and SHOULD try to cover as many problems with data and metadata as possible to avoid failure during the actual packing process.</p> <p>Files or directories inside of <code>data_dir</code> MUST NOT be created, deleted or modified by this method.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>Path</code> <p>Directory containing all the data to be packed.</p> required <p>Returns:</p> Type Description <code>DirValidationErrors</code> <p>DirValidationError initialized with a dict mapping file paths</p> <code>DirValidationErrors</code> <p>(relative to <code>dir</code>) to lists of detected errors.</p> <code>DirValidationErrors</code> <p>The errors must be either a string (containing a human-readable summary of all</p> <code>DirValidationErrors</code> <p>problems with that file), or another dict with more granular error messages,</p> <code>DirValidationErrors</code> <p>in case that the file is e.g. a JSON-compatible file subject to validation</p> <code>DirValidationErrors</code> <p>with JSON Schemas.</p> Source code in <code>src/metador_core/packer/__init__.py</code> <pre><code>@classmethod\n@abstractmethod\ndef check_dir(cls, data_dir: Path) -&gt; DirValidationErrors:\n\"\"\"Check whether the given directory is suitable for packing with this plugin.\n\n    This method will be called before `pack` or `update` and MUST detect\n    all problems (such as missing or invalid data or metadata) that can be\n    expected to be fixed by the user in preparation for the packing.\n\n    More specifically, it MUST cover all metadata that is to be provided directly by\n    the user (i.e. is not inferred or extracted from generated data) for the purpose\n    of packing and SHOULD try to cover as many problems with data and metadata as\n    possible to avoid failure during the actual packing process.\n\n    Files or directories inside of `data_dir` MUST NOT be created,\n    deleted or modified by this method.\n\n    Args:\n        data_dir: Directory containing all the data to be packed.\n\n    Returns:\n        DirValidationError initialized with a dict mapping file paths\n        (relative to `dir`) to lists of detected errors.\n\n        The errors must be either a string (containing a human-readable summary of all\n        problems with that file), or another dict with more granular error messages,\n        in case that the file is e.g. a JSON-compatible file subject to validation\n        with JSON Schemas.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/metador_core/packer/#metador_core.packer.Packer.update","title":"update  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>update(mc: MetadorContainer, data_dir: Path, diff: DirDiff)\n</code></pre> <p>Update a MetadorContainer with changes done to the data source directory.</p> <p>The <code>container</code> is assumed to be writable, and is either empty or was previously packed by a compatible version of the packer.</p> <p>The <code>data_dir</code> is assumed to be suitable (according to <code>check_dir</code>).</p> <p>The <code>diff</code> structure contains information about changed paths.</p> <p>If not implemented, updates will be created by clearing the provided container and using <code>pack</code> on it.</p> <p>Parameters:</p> Name Type Description Default <code>mc</code> <code>MetadorContainer</code> <p>Metador IH5 record to pack the data into or update</p> required <code>data_dir</code> <code>Path</code> <p>Directory containing all the data to be packed</p> required <code>diff</code> <code>DirDiff</code> <p>Diff tree of dirs and files in data_dir compared to a previous state</p> required Source code in <code>src/metador_core/packer/__init__.py</code> <pre><code>@classmethod\n@abstractmethod\ndef update(cls, mc: MetadorContainer, data_dir: Path, diff: DirDiff):\n\"\"\"Update a MetadorContainer with changes done to the data source directory.\n\n    The `container` is assumed to be writable, and is either empty\n    or was previously packed by a compatible version of the packer.\n\n    The `data_dir` is assumed to be suitable (according to `check_dir`).\n\n    The `diff` structure contains information about changed paths.\n\n    If not implemented, updates will be created by clearing the provided\n    container and using `pack` on it.\n\n    Args:\n        mc: Metador IH5 record to pack the data into or update\n        data_dir: Directory containing all the data to be packed\n        diff: Diff tree of dirs and files in data_dir compared to a previous state\n    \"\"\"\n    # default fallback implementation using pack\n    for obj in [mc, mc.attrs, mc.meta]:\n        for key in obj.keys():\n            del obj[key]\n    return cls.pack(mc, data_dir)\n</code></pre>"},{"location":"reference/metador_core/packer/#metador_core.packer.Packer.pack","title":"pack  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>pack(mc: MetadorContainer, data_dir: Path)\n</code></pre> <p>Pack a directory into an MetadorContainer.</p> <p>The <code>container</code> is assumed to be writable and empty.</p> <p>The <code>data_dir</code> is assumed to be suitable (according to <code>check_dir</code>).</p> <p>If not implemented, initial packing is done using <code>update</code> with an empty container and a diff containing all the files.</p> <p>Parameters:</p> Name Type Description Default <code>mc</code> <code>MetadorContainer</code> <p>Metador IH5 record to pack the data into or update</p> required <code>data_dir</code> <code>Path</code> <p>Directory containing all the data to be packed</p> required Source code in <code>src/metador_core/packer/__init__.py</code> <pre><code>@classmethod\n@abstractmethod\ndef pack(cls, mc: MetadorContainer, data_dir: Path):\n\"\"\"Pack a directory into an MetadorContainer.\n\n    The `container` is assumed to be writable and empty.\n\n    The `data_dir` is assumed to be suitable (according to `check_dir`).\n\n    If not implemented, initial packing is done using `update`\n    with an empty container and a diff containing all the files.\n\n    Args:\n        mc: Metador IH5 record to pack the data into or update\n        data_dir: Directory containing all the data to be packed\n    \"\"\"\n    # default fallback implementation using update\n    return cls.update(mc, data_dir, DirDiff.compare({}, dir_hashsums(data_dir)))\n</code></pre>"},{"location":"reference/metador_core/packer/#metador_core.packer.PackerInfo","title":"PackerInfo","text":"<p>             Bases: <code>MetadataSchema</code></p> <p>Schema for info about the packer that was used to create a container.</p> Source code in <code>src/metador_core/packer/__init__.py</code> <pre><code>class PackerInfo(MetadataSchema):\n\"\"\"Schema for info about the packer that was used to create a container.\"\"\"\n\n    class Plugin:\n        name = \"core.packerinfo\"\n        version = (0, 1, 0)\n\n    packer: PGPacker.PluginRef\n\"\"\"Packer plugin used to pack the container.\"\"\"\n\n    pkg: PluginPkgMeta\n\"\"\"Python package that provides the packer plugin.\"\"\"\n\n    source_dir: DirHashsums = {}\n\"\"\"Directory skeleton with hashsums of files at the time of packing.\"\"\"\n\n    @classmethod\n    def for_packer(cls, packer_name: str, packer_version=None) -&gt; PackerInfo:\n        from ..plugins import packers\n\n        p_ref = packers.resolve(packer_name, packer_version)\n        return PackerInfo(\n            packer=p_ref,\n            pkg=packers.provider(p_ref),\n        )\n</code></pre>"},{"location":"reference/metador_core/packer/#metador_core.packer.PackerInfo.packer","title":"packer  <code>instance-attribute</code>","text":"<pre><code>packer: PGPacker.PluginRef\n</code></pre> <p>Packer plugin used to pack the container.</p>"},{"location":"reference/metador_core/packer/#metador_core.packer.PackerInfo.pkg","title":"pkg  <code>instance-attribute</code>","text":"<pre><code>pkg: PluginPkgMeta\n</code></pre> <p>Python package that provides the packer plugin.</p>"},{"location":"reference/metador_core/packer/#metador_core.packer.PackerInfo.source_dir","title":"source_dir  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source_dir: DirHashsums = {}\n</code></pre> <p>Directory skeleton with hashsums of files at the time of packing.</p>"},{"location":"reference/metador_core/packer/#metador_core.packer.Unclosable","title":"Unclosable","text":"<p>             Bases: <code>ObjectProxy</code></p> <p>Wrapper to prevent packers from closing/completing a container file.</p> Source code in <code>src/metador_core/packer/__init__.py</code> <pre><code>class Unclosable(wrapt.ObjectProxy):\n\"\"\"Wrapper to prevent packers from closing/completing a container file.\"\"\"\n\n    _self_MSG = \"Packers must not finalize the container!\"\n\n    def close(self):\n        raise UnsupportedOperation(self._self_MSG)\n\n    # specific for IH5Record subtypes:\n\n    def discard_patch(self):\n        raise UnsupportedOperation(self._self_MSG)\n\n    def commit_patch(self):\n        raise UnsupportedOperation(self._self_MSG)\n</code></pre>"},{"location":"reference/metador_core/packer/#metador_core.packer.PGPacker","title":"PGPacker","text":"<p>             Bases: <code>PluginGroup[Packer]</code></p> <p>Packer plugin group interface.</p> Source code in <code>src/metador_core/packer/__init__.py</code> <pre><code>class PGPacker(pg.PluginGroup[Packer]):\n\"\"\"Packer plugin group interface.\"\"\"\n\n    class Plugin:\n        name = PACKER_GROUP_NAME\n        version = (0, 1, 0)\n        plugin_class = Packer\n        plugin_info_class = PackerPlugin\n        requires = [\n            plugingroups.PluginRef(name=\"schema\", version=(0, 1, 0)),\n            plugingroups.PluginRef(name=\"harvester\", version=(0, 1, 0)),\n        ]\n\n    _PACKER_INFO_NAME = PackerInfo.Plugin.name\n\n    @overrides\n    def check_plugin(self, ep_name: str, plugin: Type[Packer]):\n        pg.util.check_implements_method(ep_name, plugin, Packer.check_dir)\n        missing_pack = pg.util.implements_method(plugin, Packer.pack)\n        missing_update = pg.util.implements_method(plugin, Packer.update)\n        if missing_pack and missing_update:\n            raise TypeError(f\"{ep_name}: Neither pack nor update are implemented!\")\n\n    # ----\n\n    def _prepare(self, pname: str, srcdir: Path) -&gt; Tuple[Type[Packer], DirHashsums]:\n\"\"\"Return packer class and hashsums of given directory.\n\n        Raises an exception if packer is not found or `packer.check_dir` fails.\n        \"\"\"\n        packer = self[pname]\n        if errs := packer.check_dir(srcdir):\n            raise errs\n        return (packer, dir_hashsums(srcdir))\n\n    def pack(\n        self, packer_name: str, data_dir: Path, target: Path, h5like_cls: Callable\n    ):\n\"\"\"Pack a directory into a container using an installed packer.\n\n        `packer_name` must be an installed packer plugin.\n\n        `data_dir` must be an existing directory suitable for the packer.\n\n        `target` must be a non-existing path and will be passed into `h5like_cls` as-is.\n\n        `h5like_cls` must be a class compatible with MetadorContainer.\n\n        In case an exception happens during packing, notice that no cleanup is done.\n\n        The user is responsible for removing inconsistent files that were created.\n\n        Args:\n            packer_name: installed packer plugin name\n            data_dir: data source directory\n            target: target path for resulting container\n            h5like_cls: class to use for creating the container\n        \"\"\"\n        packer, hashsums = self._prepare(packer_name, data_dir)\n        # use skel_only to enforce stub-compatibility of packer\n        container = MetadorContainer(h5like_cls(target, \"x\")).restrict(skel_only=True)\n        packer.pack(Unclosable(container), data_dir)\n        self._finalize(packer_name, hashsums, container)\n\n    def update(self, packer_name: str, data_dir: Path, target: Path, h5like_cls):\n\"\"\"Update a container from its source directory using an installed packer.\n\n        Like `pack`, but the `target` must be a container which can be opened\n        with the `h5like_cls` and was packed by a compatible packer.\n\n        In case an exception happens during packing, notice that no cleanup is done\n        and if the container has been written to, the changes persist.\n\n        The user is responsible for removing inconsistent files that were created\n        and ensuring that the previous state can be restored, e.g. from a backup.\n        \"\"\"\n        packer, hashsums = self._prepare(packer_name, data_dir)\n        # use skel_only to enforce stub-compatibility of packer\n        container = MetadorContainer(h5like_cls(target, \"r+\")).restrict(skel_only=True)\n\n        # check compatibility\n        pinfo = container.meta.get(self._PACKER_INFO_NAME)\n        if not pinfo:\n            msg = f\"Container does not have {self._PACKER_INFO_NAME} metadata!\"\n        curr_ref = self.resolve(packer_name)\n        if not curr_ref.supports(pinfo.packer):\n            msg = f\"{curr_ref} (installed) does not support {pinfo.packer} (container)!\"\n            raise ValueError(msg)\n\n        diff = DirDiff.compare(pinfo.source_dir, hashsums)\n        packer.update(Unclosable(container), data_dir, diff)\n        self._finalize(packer_name, hashsums, container)\n\n    def _finalize(self, pname: str, hsums: DirHashsums, cont: MetadorContainer):\n\"\"\"Set or update packer info in container and close it.\"\"\"\n        if self._PACKER_INFO_NAME in cont.meta:\n            del cont.meta[self._PACKER_INFO_NAME]\n\n        pinfo = PackerInfo.for_packer(pname)\n        pinfo.source_dir = hsums\n        cont.meta[self._PACKER_INFO_NAME] = pinfo\n\n        if isinstance(cont, IH5MFRecord):\n            # when using IH5MFRecord,\n            # we want the packerinfo in the manifest, so tooling can decide\n            # if a container can be updated without having it available.\n            # (default manifest already has enough info for creating stubs)\n            cont.manifest.manifest_exts[self.name] = pinfo.dict()\n\n        cont.close()\n</code></pre>"},{"location":"reference/metador_core/packer/#metador_core.packer.PGPacker.pack","title":"pack","text":"<pre><code>pack(\n    packer_name: str,\n    data_dir: Path,\n    target: Path,\n    h5like_cls: Callable,\n)\n</code></pre> <p>Pack a directory into a container using an installed packer.</p> <p><code>packer_name</code> must be an installed packer plugin.</p> <p><code>data_dir</code> must be an existing directory suitable for the packer.</p> <p><code>target</code> must be a non-existing path and will be passed into <code>h5like_cls</code> as-is.</p> <p><code>h5like_cls</code> must be a class compatible with MetadorContainer.</p> <p>In case an exception happens during packing, notice that no cleanup is done.</p> <p>The user is responsible for removing inconsistent files that were created.</p> <p>Parameters:</p> Name Type Description Default <code>packer_name</code> <code>str</code> <p>installed packer plugin name</p> required <code>data_dir</code> <code>Path</code> <p>data source directory</p> required <code>target</code> <code>Path</code> <p>target path for resulting container</p> required <code>h5like_cls</code> <code>Callable</code> <p>class to use for creating the container</p> required Source code in <code>src/metador_core/packer/__init__.py</code> <pre><code>def pack(\n    self, packer_name: str, data_dir: Path, target: Path, h5like_cls: Callable\n):\n\"\"\"Pack a directory into a container using an installed packer.\n\n    `packer_name` must be an installed packer plugin.\n\n    `data_dir` must be an existing directory suitable for the packer.\n\n    `target` must be a non-existing path and will be passed into `h5like_cls` as-is.\n\n    `h5like_cls` must be a class compatible with MetadorContainer.\n\n    In case an exception happens during packing, notice that no cleanup is done.\n\n    The user is responsible for removing inconsistent files that were created.\n\n    Args:\n        packer_name: installed packer plugin name\n        data_dir: data source directory\n        target: target path for resulting container\n        h5like_cls: class to use for creating the container\n    \"\"\"\n    packer, hashsums = self._prepare(packer_name, data_dir)\n    # use skel_only to enforce stub-compatibility of packer\n    container = MetadorContainer(h5like_cls(target, \"x\")).restrict(skel_only=True)\n    packer.pack(Unclosable(container), data_dir)\n    self._finalize(packer_name, hashsums, container)\n</code></pre>"},{"location":"reference/metador_core/packer/#metador_core.packer.PGPacker.update","title":"update","text":"<pre><code>update(\n    packer_name: str,\n    data_dir: Path,\n    target: Path,\n    h5like_cls: Path,\n)\n</code></pre> <p>Update a container from its source directory using an installed packer.</p> <p>Like <code>pack</code>, but the <code>target</code> must be a container which can be opened with the <code>h5like_cls</code> and was packed by a compatible packer.</p> <p>In case an exception happens during packing, notice that no cleanup is done and if the container has been written to, the changes persist.</p> <p>The user is responsible for removing inconsistent files that were created and ensuring that the previous state can be restored, e.g. from a backup.</p> Source code in <code>src/metador_core/packer/__init__.py</code> <pre><code>def update(self, packer_name: str, data_dir: Path, target: Path, h5like_cls):\n\"\"\"Update a container from its source directory using an installed packer.\n\n    Like `pack`, but the `target` must be a container which can be opened\n    with the `h5like_cls` and was packed by a compatible packer.\n\n    In case an exception happens during packing, notice that no cleanup is done\n    and if the container has been written to, the changes persist.\n\n    The user is responsible for removing inconsistent files that were created\n    and ensuring that the previous state can be restored, e.g. from a backup.\n    \"\"\"\n    packer, hashsums = self._prepare(packer_name, data_dir)\n    # use skel_only to enforce stub-compatibility of packer\n    container = MetadorContainer(h5like_cls(target, \"r+\")).restrict(skel_only=True)\n\n    # check compatibility\n    pinfo = container.meta.get(self._PACKER_INFO_NAME)\n    if not pinfo:\n        msg = f\"Container does not have {self._PACKER_INFO_NAME} metadata!\"\n    curr_ref = self.resolve(packer_name)\n    if not curr_ref.supports(pinfo.packer):\n        msg = f\"{curr_ref} (installed) does not support {pinfo.packer} (container)!\"\n        raise ValueError(msg)\n\n    diff = DirDiff.compare(pinfo.source_dir, hashsums)\n    packer.update(Unclosable(container), data_dir, diff)\n    self._finalize(packer_name, hashsums, container)\n</code></pre>"},{"location":"reference/metador_core/packer/example/","title":"example","text":"<p>This is an example packer plugin for simple general data types.</p> <p>A packer plugin implements use-case specific container-related functionality for Metador containers.</p> <p>To develop your own packer plugin, implement a class deriving from <code>Packer</code> and register the class as an entrypoint of your package (see the <code>pyproject.toml</code> of this package, where <code>GenericPacker</code> is registered as a packer plugin called <code>example</code>.)</p>"},{"location":"reference/metador_core/packer/example/#metador_core.packer.example.GenericPacker","title":"GenericPacker","text":"<p>             Bases: <code>Packer</code></p> <p>The generic packer is demonstrating how a packer can be implemented.</p> <p>It will pack CSV tables with metadata into corresponding HDF5 containers, and it will pack all other kinds of files as embedded opaque blobs.</p> <p>Both kinds of nodes will have corresponding metadata attributes attached.</p> <p>The directory is expected to have a _meta.yaml file in the container root and each CSV file file.csv needs a companion metadata file file.csv_meta.yaml.</p> <p>All symlinks inside the directory are completely ignored.</p> <p>This packer does very verbose logging for didactic purposes. Other packers may log their actions as they deem appropriate.</p> Source code in <code>src/metador_core/packer/example.py</code> <pre><code>class GenericPacker(Packer):\n\"\"\"The generic packer is demonstrating how a packer can be implemented.\n\n    It will pack CSV tables with metadata into corresponding HDF5 containers,\n    and it will pack all other kinds of files as embedded opaque blobs.\n\n    Both kinds of nodes will have corresponding metadata attributes attached.\n\n    The directory is expected to have a _meta.yaml file in the container root\n    and each CSV file file.csv needs a companion metadata file file.csv_meta.yaml.\n\n    All symlinks inside the directory are completely ignored.\n\n    This packer does very verbose logging for didactic purposes.\n    Other packers may log their actions as they deem appropriate.\n    \"\"\"\n\n    class Plugin:\n        name = \"core.generic\"\n        version = (0, 1, 0)\n\n    META_SUFFIX: str = \"_meta.yaml\"\n\n    @classmethod\n    def sidecar_for(cls, path: Union[Path, str]) -&gt; str:\n\"\"\"Sidecar file name for given path.\"\"\"\n        return f\"{path}{cls.META_SUFFIX}\"\n\n    @classmethod\n    @overrides\n    def check_dir(cls, data_dir: Path) -&gt; DirValidationErrors:\n        print(\"--------\")\n        print(\"called check_dir\")\n        errs = DirValidationErrors()\n        errs.update(\n            check_metadata_file(\n                data_dir / cls.META_SUFFIX, required=True, schema=BibMeta\n            )\n        )\n        return errs\n\n    @classmethod\n    @overrides\n    def update(cls, mc: MetadorContainer, data_dir: Path, diff: DirDiff):\n        print(\"--------\")\n        print(\"called update\")\n\n        for path, dnode in diff.annotate(data_dir).items():\n            # the status indicates whether the file was added, removed or modified\n            status = diff.status(dnode)\n            print(status.value, path)\n\n            if dnode is None:  # unchanged paths in the directory have no diff node\n                print(\"IGNORE:\", path, \"(unchanged)\")\n                continue  # nothing to do\n\n            if path.is_symlink():  # we ignore symlinks in the data directory\n                print(\"IGNORE:\", path, \"(symlink)\")\n                continue\n\n            if path.name.lower().endswith(\".csv_meta.yaml\"):\n                # will be taken care of when the CSV file is processed\n                print(\"IGNORE:\", path, \"(sidecar file)\")\n                continue\n\n            # for this simple packer, each file maps 1-to-1 to a container path\n            key = f\"{dnode.path}\"  # path inside the container\n\n            if status == DiffNode.Status.removed:  # entity was removed -&gt;\n                # also remove in container, if it was not a symlink (which we ignored)\n                if dnode.prev_type != DiffNode.ObjType.symlink:\n                    print(\"DELETE:\", key)\n                    del mc[key]\n                continue\n\n            if status == DiffNode.Status.modified:  # changed\n                if dnode.prev_type == dnode.curr_type and path.is_dir():\n                    continue  # a changed dir should already exist + remain in container\n\n                # otherwise it was replaced either file -&gt; dir or dir -&gt; file, so\n                # remove entity, proceeding with loop body to add new entity version\n                print(\"DELETE:\", key)\n                del mc[key]\n\n            # now we (re-)add new or modified entities:\n            if path.is_dir():\n                print(\"CREATE:\", path, \"-&gt;\", key, \"(dir)\")\n\n                mc.create_group(key)\n\n            elif path.is_file():\n                if path.name.endswith(cls.META_SUFFIX):\n                    if key == cls.META_SUFFIX:\n                        # update root meta\n                        print(\"CREATE:\", path, \"-&gt;\", key, \"(biblio metadata)\")\n                        mc.meta[\"common_biblio\"] = BibMeta.parse_file(path)\n                else:\n                    if path.name.lower().endswith(\".csv\"):\n                        # embed CSV as numpy array with table metadata\n                        print(\"CREATE:\", path, \"-&gt;\", key, \"(table)\")\n\n                        mc[key] = pandas.read_csv(path).to_numpy()  # type: ignore\n                        mc[key].meta[\"common_table\"] = TableMeta.for_file(\n                            cls.sidecar_for(path)\n                        )\n\n                    elif path.name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n                        # embed image file with image-specific metadata\n                        print(\"CREATE:\", path, \"-&gt;\", key, \"(image)\")\n                        pack_file(mc, path, target=key)\n                        # mc[key].meta[\"common_image\"] = image_meta_for(path)\n\n                    else:\n                        # treat as opaque blob and add file metadata\n                        print(\"CREATE:\", path, \"-&gt;\", key, \"(file)\")\n                        pack_file(mc, path, target=key)\n</code></pre>"},{"location":"reference/metador_core/packer/example/#metador_core.packer.example.GenericPacker.sidecar_for","title":"sidecar_for  <code>classmethod</code>","text":"<pre><code>sidecar_for(path: Union[Path, str]) -&gt; str\n</code></pre> <p>Sidecar file name for given path.</p> Source code in <code>src/metador_core/packer/example.py</code> <pre><code>@classmethod\ndef sidecar_for(cls, path: Union[Path, str]) -&gt; str:\n\"\"\"Sidecar file name for given path.\"\"\"\n    return f\"{path}{cls.META_SUFFIX}\"\n</code></pre>"},{"location":"reference/metador_core/packer/types/","title":"types","text":""},{"location":"reference/metador_core/packer/types/#metador_core.packer.types.DirValidationErrors","title":"DirValidationErrors","text":"<p>             Bases: <code>ValueError</code></p> <p>Structure to collect record or directory validation errors.</p> <p>Returned from validation functions and thrown in other contexts.</p> Source code in <code>src/metador_core/packer/types.py</code> <pre><code>class DirValidationErrors(ValueError):\n\"\"\"Structure to collect record or directory validation errors.\n\n    Returned from validation functions and thrown in other contexts.\n    \"\"\"\n\n    errors: Dict[str, List[Any]]\n\"\"\"Common type used to collect errors.\n\n    Maps path in container or directory to list of errors with that path.\n\n    The list should contain either strings or other such dicts,\n    but Python type checkers are unable to understand recursive types.\n    \"\"\"\n\n    def __init__(self, errs: Optional[Dict[str, Any]] = None):\n        self.errors = errs or {}\n\n    def __bool__(self):\n        return bool(self.errors)\n\n    def __repr__(self):  # pragma: no cover\n        return repr(self.errors)\n\n    def update(self, *err_objs: DirValidationErrors):\n\"\"\"Add errors from other instances to this object.\"\"\"\n        errs = self.errors\n        for more_errs in err_objs:\n            for k, v in more_errs.errors.items():\n                if k not in errs:\n                    errs[k] = v\n                else:\n                    errs[k] += v\n\n    def add(self, k, v):\n        if k not in self.errors:\n            self.errors[k] = []\n        self.errors[k].append(v)\n</code></pre>"},{"location":"reference/metador_core/packer/types/#metador_core.packer.types.DirValidationErrors.errors","title":"errors  <code>instance-attribute</code>","text":"<pre><code>errors: Dict[str, List[Any]] = errs or {}\n</code></pre> <p>Common type used to collect errors.</p> <p>Maps path in container or directory to list of errors with that path.</p> <p>The list should contain either strings or other such dicts, but Python type checkers are unable to understand recursive types.</p>"},{"location":"reference/metador_core/packer/types/#metador_core.packer.types.DirValidationErrors.update","title":"update","text":"<pre><code>update(*err_objs: DirValidationErrors)\n</code></pre> <p>Add errors from other instances to this object.</p> Source code in <code>src/metador_core/packer/types.py</code> <pre><code>def update(self, *err_objs: DirValidationErrors):\n\"\"\"Add errors from other instances to this object.\"\"\"\n    errs = self.errors\n    for more_errs in err_objs:\n        for k, v in more_errs.errors.items():\n            if k not in errs:\n                errs[k] = v\n            else:\n                errs[k] += v\n</code></pre>"},{"location":"reference/metador_core/packer/utils/","title":"utils","text":"<p>Various helper functions that packers can use.</p>"},{"location":"reference/metador_core/packer/utils/#metador_core.packer.utils.check_metadata_file","title":"check_metadata_file","text":"<pre><code>check_metadata_file(path: Path, **kwargs: Path)\n</code></pre> <p>Check a metadata file, return error object.</p> <p>If <code>required</code> is set, will add an error if file is missing.</p> <p>If <code>schema</code> is passed and file exists, will validate the file and log errors.</p> <p>Combine both to check that a file does exist and is valid according to a schema.</p> Source code in <code>src/metador_core/packer/utils.py</code> <pre><code>def check_metadata_file(path: Path, **kwargs):\n\"\"\"Check a metadata file, return error object.\n\n    If `required` is set, will add an error if file is missing.\n\n    If `schema` is passed and file exists, will validate the file and log errors.\n\n    Combine both to check that a file does exist and is valid according to a schema.\n    \"\"\"\n    required: bool = kwargs.get(\"required\", False)\n    schema: MetadataSchema = kwargs.get(\"schema\", None)\n    errs = DirValidationErrors()\n\n    exists = path.is_file()\n    if required and not exists:\n        errs.add(str(path), f\"Required metadata file not found: '{path}'\")\n    if schema is not None and exists:\n        try:\n            schema.parse_file(path)\n        except (JSONDecodeError, ValidationError, FileNotFoundError) as e:\n            errs.add(str(path), str(e))\n\n    return errs\n</code></pre>"},{"location":"reference/metador_core/packer/utils/#metador_core.packer.utils.pack_file","title":"pack_file","text":"<pre><code>pack_file(\n    node: Union[MetadorContainer, MetadorGroup],\n    file_path: Union[Path, str],\n    *,\n    target: Optional[str] = None,\n    metadata: Optional[MetadataSchema] = None\n) -&gt; MetadorDataset\n</code></pre> <p>Embed a file, adding minimal generic metadata to it.</p> <p>Will also ensure that the attached metadata has RO-Crate compatible @id set correctly.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Union[MetadorContainer, MetadorGroup]</code> <p>Container where to embed the file contents</p> required <code>file_path</code> <code>Union[Path, str]</code> <p>Path of an existing file to be embedded</p> required <code>target</code> <code>Optional[str]</code> <p>Fresh path in container where to place the file</p> <code>None</code> <code>metadata</code> <code>Optional[MetadataSchema]</code> <p>If provided, will attach this instead of harvesting defaults.</p> <code>None</code> <p>Returns:</p> Type Description <code>MetadorDataset</code> <p>Dataset of new embedded file.</p> Source code in <code>src/metador_core/packer/utils.py</code> <pre><code>def pack_file(\n    node: Union[MetadorContainer, MetadorGroup],\n    file_path: Union[Path, str],\n    *,\n    target: Optional[str] = None,\n    metadata: Optional[MetadataSchema] = None,\n) -&gt; MetadorDataset:\n\"\"\"Embed a file, adding minimal generic metadata to it.\n\n    Will also ensure that the attached metadata has RO-Crate compatible @id set correctly.\n\n    Args:\n        node: Container where to embed the file contents\n        file_path: Path of an existing file to be embedded\n        target: Fresh path in container where to place the file\n        metadata: If provided, will attach this instead of harvesting defaults.\n\n    Returns:\n        Dataset of new embedded file.\n    \"\"\"\n    file_path = Path(file_path)\n\n    # if no target path given, use &lt;current node path / file name&gt;\n    if not target:\n        target = file_path.name\n\n    # check container and file\n    if target in node:\n        raise ValueError(f\"Path '{node}' already exists in given container or group!\")\n    if not file_path.is_file():\n        raise ValueError(f\"Path '{file_path}' does not look like an existing file!\")\n\n    if not metadata:\n        # no metadata given -&gt; harvest minimal information about a file\n        hv_file = harvesters[\"core.file.generic\"]\n        metadata = harvest(FileMeta, [hv_file(filepath=file_path)])\n    else:\n        metadata = metadata.copy()  # defensive copying!\n\n    # check metadata\n    if not isinstance(metadata, FileMeta):\n        msg = f\"Provided metadata is not compatible with '{FileMeta.Plugin.name}'!\"\n        raise ValueError(msg)\n    if not schemas.is_plugin(type(metadata)):\n        msg = f\"Given metadata is a {type(metadata)}, which is not a schema plugin!\"\n        raise ValueError(msg)\n\n    data = _h5_wrap_bytes(file_path.read_bytes())\n    ret = node.create_dataset(target, data=data)\n\n    # set file metadata @id to be relative to dataset root just like RO Crate wants\n    metadata.id_ = urllib.parse.quote(f\".{ret.name}\")\n\n    # embed file and metadata in container:\n    ret.meta[metadata.Plugin.name] = metadata\n    return ret\n</code></pre>"},{"location":"reference/metador_core/plugin/","title":"plugin","text":"<p>Entrypoint based plugin system for Metador.</p>"},{"location":"reference/metador_core/plugin/entrypoints/","title":"entrypoints","text":"<p>Processing of entry points for Metador plugins.</p>"},{"location":"reference/metador_core/plugin/entrypoints/#metador_core.plugin.entrypoints.pkg_meta","title":"pkg_meta  <code>module-attribute</code>","text":"<pre><code>pkg_meta = {}\n</code></pre> <p>Collected infos about packages that provide plugins (filled by get_group).</p>"},{"location":"reference/metador_core/plugin/entrypoints/#metador_core.plugin.entrypoints.get_group","title":"get_group","text":"<pre><code>get_group(group_name: str) -&gt; Dict[str, Any]\n</code></pre> <p>Get a dict of all available entrypoints for a Metador plugin group.</p> Source code in <code>src/metador_core/plugin/entrypoints.py</code> <pre><code>def get_group(group_name: str) -&gt; Dict[str, Any]:\n\"\"\"Get a dict of all available entrypoints for a Metador plugin group.\"\"\"\n    ep_grp = to_ep_group_name(group_name)\n    plugins: Dict[str, Any] = {}\n\n    for ep in _eps.select(group=ep_grp):\n        if ep.name in plugins:\n            # TODO: will importlib_metadata even return colliding packages?\n            # should be figured out (quite important to know)\n            msg = f\"{group_name}: a plugin named '{ep.name}' is already registered!\"\n            raise TypeError(msg)\n\n        plugins[ep.name] = ep\n        if ep.dist.name not in pkg_meta:\n            pkg_meta[ep.dist.name] = PluginPkgMeta.for_package(ep.dist.name)\n\n    return plugins\n</code></pre>"},{"location":"reference/metador_core/plugin/entrypoints/#metador_core.plugin.entrypoints.distmeta_for","title":"distmeta_for","text":"<pre><code>distmeta_for(dist: Distribution) -&gt; DistMeta\n</code></pre> <p>Extract required metadata from importlib_metadata distribution object.</p> Source code in <code>src/metador_core/plugin/entrypoints.py</code> <pre><code>def distmeta_for(dist: Distribution) -&gt; DistMeta:\n\"\"\"Extract required metadata from importlib_metadata distribution object.\"\"\"\n    ver = dist.version\n    if not re.fullmatch(\"[0-9]+\\\\.[0-9]+\\\\.[0-9]+\", ver):\n        msg = f\"Invalid version string of {dist.name}: {ver}\"\n        raise TypeError(msg)\n    parsed_ver: SemVerTuple = tuple(map(int, ver.split(\".\")))  # type: ignore\n\n    # parse entry point groups\n    epgs = filter(\n        is_metador_ep_group,\n        dist.entry_points.groups,\n    )\n    eps = {\n        from_ep_group_name(EPGroupName(epg)): list(\n            map(lambda x: x.name, dist.entry_points.select(group=epg))\n        )\n        for epg in epgs\n    }\n\n    repo_url: Optional[str] = None\n    try:\n        urls = dist.metadata.get_all(\"Project-URL\")\n        url = next(filter(lambda u: u.startswith(\"Repository,\"), urls or []))\n        repo_url = url.split()[1]  # from \"Repository, http://...\"\n    except StopIteration:\n        pass\n    return DistMeta(\n        name=dist.name, version=parsed_ver, plugins=eps, repository_url=repo_url\n    )\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/","title":"interface","text":"<p>Interface for plugin groups.</p>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroupMeta","title":"PluginGroupMeta","text":"<p>             Bases: <code>ABCMeta</code></p> <p>Metaclass to initialize some things on creation.</p> Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>class PluginGroupMeta(ABCMeta):\n\"\"\"Metaclass to initialize some things on creation.\"\"\"\n\n    def __init__(self, name, bases, dct):\n        assert is_pluginlike(self, check_group=False)\n\n        # attach generated subclass that auto-fills the group for plugin infos\n        self.PluginRef: Type[AnyPluginRef] = AnyPluginRef._subclass_for(\n            self.Plugin.name\n        )\n\n        if pgi_cls := self.Plugin.__dict__.get(\"plugin_info_class\"):\n            # attach group name to provided plugin info class\n            pgi_cls.group = self.Plugin.name\n        else:\n            # derive generic plugin info class with the group defined\n            class PGInfo(PluginBase):\n                group = self.Plugin.name\n\n            self.Plugin.plugin_info_class = PGInfo\n\n        # sanity checks... this magic should not mess with the PluginBase\n        assert self.Plugin.plugin_info_class is not PluginBase\n        assert PluginBase.group == \"\"\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup","title":"PluginGroup","text":"<p>             Bases: <code>Generic[T]</code></p> <p>All pluggable entities in metador are subclasses of this class.</p> <p>The type parameter is the (parent) class of all loaded plugins.</p> <p>They must implement the check method and be listed as plugin group. The name of their entrypoint defines the name of the plugin group.</p> Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>class PluginGroup(Generic[T], metaclass=PluginGroupMeta):\n\"\"\"All pluggable entities in metador are subclasses of this class.\n\n    The type parameter is the (parent) class of all loaded plugins.\n\n    They must implement the check method and be listed as plugin group.\n    The name of their entrypoint defines the name of the plugin group.\n    \"\"\"\n\n    PluginRef: TypeAlias = AnyPluginRef\n\"\"\"Plugin reference class for this plugin group.\"\"\"\n\n    _PKG_META: ClassVar[Dict[str, PluginPkgMeta]] = pkg_meta\n\"\"\"Package name -&gt; package metadata.\"\"\"\n\n    class Plugin:\n\"\"\"This is the plugin group plugin group, the first loaded group.\"\"\"\n\n        name = PG_GROUP_NAME\n        version = (0, 1, 0)\n        plugin_info_class = PGPlugin\n        plugin_class: Type\n        # plugin_class = PluginGroup  # can't set that -&gt; check manually\n\n    _ENTRY_POINTS: Dict[EPName, EntryPoint]\n\"\"\"Dict of entry points of versioned plugins (not loaded).\"\"\"\n\n    _VERSIONS: Dict[str, List[AnyPluginRef]]\n\"\"\"Mapping from plugin name to pluginrefs of available versions.\"\"\"\n\n    _LOADED_PLUGINS: Dict[AnyPluginRef, Type[T]]\n\"\"\"Dict from entry points to loaded plugins of that pluggable type.\"\"\"\n\n    def _add_ep(self, epname_str: str, ep_obj: EntryPoint):\n\"\"\"Add an entrypoint loaded from importlib_metadata.\"\"\"\n        try:\n            ep_name = EPName(epname_str)\n        except TypeError:\n            msg = f\"{epname_str}: Invalid entrypoint name, must match {EP_NAME_REGEX}\"\n            raise ValueError(msg)\n        if type(self) is not PluginGroup and not ep_name_has_namespace(ep_name):\n            msg = f\"{epname_str}: Plugin name has no qualifying namespace!\"\n            raise ValueError(msg)\n\n        name, version = from_ep_name(ep_name)\n        p_ref = AnyPluginRef(group=self.name, name=name, version=version)\n\n        if ep_name in self._ENTRY_POINTS:\n            self._LOADED_PLUGINS.pop(p_ref, None)  # unload, if loaded\n            pkg = ep_obj.dist\n            msg = f\"WARNING: {ep_name} is probably provided by multiple packages!\\n\"\n            msg += f\"The plugin will now be provided by: {pkg.name} {pkg.version}\"\n            eprint(msg)\n        self._ENTRY_POINTS[ep_name] = ep_obj\n\n        if ep_name not in self._VERSIONS:\n            self._VERSIONS[name] = []\n        self._VERSIONS[name].append(p_ref)\n        self._VERSIONS[name].sort()  # should be cheap\n\n    def __init__(self, entrypoints: Dict[str, EntryPoint]):\n        self._ENTRY_POINTS = {}\n        self._VERSIONS = {}\n\n        for k, v in entrypoints.items():\n            self._add_ep(k, v)\n\n        self._LOADED_PLUGINS = {}\n        self.__post_init__()\n\n    def __post_init__(self):\n        if type(self) is PluginGroup:\n            # make the magic plugingroup plugin add itself for consistency\n            ep_name = util.to_ep_name(self.Plugin.name, self.Plugin.version)\n            ep_path = f\"{type(self).__module__}:{type(self).__name__}\"\n            ep = EntryPoint(ep_name, ep_path, self.name)\n            self._add_ep(ep_name, ep)\n\n            self_ref = AnyPluginRef(\n                group=self.name, name=self.name, version=self.Plugin.version\n            )\n            self._LOADED_PLUGINS[self_ref] = self\n            self.provider(self_ref).plugins[self.name].append(self_ref)\n\n    @property\n    def name(self) -&gt; str:\n\"\"\"Return name of the plugin group.\"\"\"\n        return self.Plugin.name\n\n    @property\n    def packages(self) -&gt; Dict[str, PluginPkgMeta]:\n\"\"\"Return metadata of all packages providing metador plugins.\"\"\"\n        return dict(self._PKG_META)\n\n    def versions(\n        self, p_name: str, version: Optional[SemVerTuple] = None\n    ) -&gt; List[AnyPluginRef]:\n\"\"\"Return installed versions of a plugin (compatible with given version).\"\"\"\n        refs = list(self._VERSIONS.get(p_name) or [])\n        if version is None:\n            return refs\n        requested = self.PluginRef(name=p_name, version=version)\n        return [ref for ref in refs if ref.supports(requested)]\n\n    def resolve(\n        self, p_name: str, version: Optional[SemVerTuple] = None\n    ) -&gt; Optional[AnyPluginRef]:\n\"\"\"Return most recent compatible version of a plugin.\"\"\"\n        if refs := self.versions(p_name, version):\n            return refs[-1]  # latest (compatible) version\n        return None\n\n    def provider(self, ref: AnyPluginRef) -&gt; PluginPkgMeta:\n\"\"\"Return package metadata of Python package providing this plugin.\"\"\"\n        if type(self) is PluginGroup and ref.name == PG_GROUP_NAME:\n            # special case - the mother plugingroup plugin is not an EP,\n            # so we cheat a bit (schema is in same package, but is an EP)\n            return self.provider(self.resolve(\"schema\"))\n\n        ep_name = util.to_ep_name(ref.name, ref.version)\n        ep = self._ENTRY_POINTS[ep_name]\n        return self._PKG_META[cast(Any, ep).dist.name]\n\n    def is_plugin(self, p_cls):\n\"\"\"Return whether this class is a (possibly marked) installed plugin.\"\"\"\n        if not isinstance(p_cls, type) or not issubclass(\n            p_cls, self.Plugin.plugin_class\n        ):\n            return False\n\n        c = UndefVersion._unwrap(p_cls) or p_cls  # get real underlying class\n        # check its exactly a registered plugin, if it has a Plugin section\n        if info := c.__dict__.get(\"Plugin\"):\n            if not isinstance(info, PluginBase):\n                return False\n            loaded_p = self._get_unsafe(info.name, info.version)\n            return loaded_p is c\n        else:\n            return False\n\n    # ----\n\n    def __repr__(self):\n        return f\"&lt;PluginGroup '{self.name}' {list(self.keys())}&gt;\"\n\n    def __str__(self):\n        def pg_line(name_refs):\n            name, refs = name_refs\n            vs = list(map(lambda x: to_semver_str(x.version), refs))\n            # p = self.provider(pg_ref.name)\n            # pkg = f\"{p.name} {semver_str(p.version)}\"\n            return f\"\\t'{name}' ({', '.join(vs)})\"\n\n        pgs = \"\\n\".join(map(pg_line, self._VERSIONS.items()))\n        return f\"Available '{self.name}' plugins:\\n{pgs}\"\n\n    # ----\n    # dict-like interface will provide latest versions of plugins by default\n\n    def __contains__(self, key) -&gt; bool:\n        name, version = plugin_args(key)\n        if pg_versions := self._VERSIONS.get(name):\n            if not version:\n                return True\n            pg = self.PluginRef(name=name, version=version)\n            return pg in pg_versions\n        return False\n\n    def __getitem__(self, key) -&gt; Type[T]:\n        if key not in self:\n            raise KeyError(f\"{self.name} not found: {key}\")\n        return self.get(key)\n\n    def keys(self) -&gt; Iterator[AnyPluginRef]:\n\"\"\"Return all names of all plugins.\"\"\"\n        for pgs in self._VERSIONS.values():\n            yield from pgs\n\n    def values(self) -&gt; Iterator[Type[T]]:\n\"\"\"Return latest versions of all plugins (THIS LOADS ALL PLUGINS!).\"\"\"\n        return map(self.__getitem__, self.keys())\n\n    def items(self) -&gt; Iterator[Tuple[AnyPluginRef, Type[T]]]:\n\"\"\"Return pairs of plugin name and latest installed version (THIS LOADS ALL PLUGINS!).\"\"\"\n        return map(lambda k: (k, self[k]), self.keys())\n\n    # ----\n\n    def _get_unsafe(self, p_name: str, version: Optional[SemVerTuple] = None):\n\"\"\"Return most recent compatible version of given plugin name, without safety rails.\n\n        Raises KeyError if no (compatible) schema found.\n\n        For internal use only!\n        \"\"\"\n        if ref := self.resolve(p_name, version):\n            self._ensure_is_loaded(ref)\n            return self._LOADED_PLUGINS[ref]\n        else:  # error\n            msg = f\"{p_name}\"\n            if version:\n                msg += f\": no installed version is compatible with {version}\"\n            raise KeyError(msg)\n\n    # inspired by this nice trick: https://stackoverflow.com/a/60362860\n    PRX = TypeVar(\"PRX\", bound=\"Type[T]\")  # type: ignore\n\n    @overload\n    def get(self, key: str, version: Optional[SemVerTuple] = None) -&gt; Optional[Type[T]]:\n        ...  # pragma: no cover\n\n    @overload\n    def get(self, key: PRX, version: Optional[SemVerTuple] = None) -&gt; Optional[PRX]:\n        ...  # pragma: no cover\n\n    def get(\n        self, key: Union[str, PRX], version: Optional[SemVerTuple] = None\n    ) -&gt; Union[Type[T], PRX, None]:\n        key_, version = plugin_args(key, version)\n\n        # retrieve compatible plugin\n        try:\n            ret = self._get_unsafe(key_, version)\n        except KeyError:\n            return None\n\n        if version is None:\n            # no version constraint was passed or inferred -&gt; mark it\n            ret = UndefVersion._mark_class(ret)\n\n        if isinstance(key, str):\n            return cast(Type[T], ret)\n        else:\n            return ret\n\n    # ----\n\n    def _ensure_is_loaded(self, ref: AnyPluginRef):\n\"\"\"Load plugin from entrypoint, if it is not loaded yet.\"\"\"\n        assert ref.group == self.name\n        if ref in self._LOADED_PLUGINS:\n            return  # already loaded, all good\n\n        ep_name = util.to_ep_name(ref.name, ref.version)\n        ret = self._ENTRY_POINTS[ep_name].load()\n        self._LOADED_PLUGINS[ref] = ret\n\n        self._load_plugin(ep_name, ret)\n\n    def _explicit_plugin_deps(self, plugin) -&gt; Set[AnyPluginRef]:\n\"\"\"Return all plugin dependencies that must be taken into account.\"\"\"\n        def_deps = set(plugin.Plugin.requires)\n        extra_deps = set(self.plugin_deps(plugin) or set())\n        return def_deps.union(extra_deps)\n\n    def plugin_deps(self, plugin) -&gt; Set[AnyPluginRef]:\n\"\"\"Return additional automatically inferred dependencies for a plugin.\"\"\"\n\n    def _load_plugin(self, ep_name: EPName, plugin):\n\"\"\"Run checks and finalize loaded plugin.\"\"\"\n        from ..plugins import plugingroups\n\n        # run inner Plugin class checks (with possibly new Fields cls)\n        if not plugin.__dict__.get(\"Plugin\"):\n            raise TypeError(f\"{ep_name}: {plugin} is missing Plugin inner class!\")\n        # pass ep_name to check that it agrees with the plugin info\n        plugin.Plugin = self.Plugin.plugin_info_class.parse_info(\n            plugin.Plugin, ep_name=ep_name\n        )\n\n        # do general checks first, if they fail no need to continue\n        self._check_common(ep_name, plugin)\n        self.check_plugin(ep_name, plugin)\n\n        for dep_ref in self._explicit_plugin_deps(plugin):\n            dep_grp = plugingroups[dep_ref.group]\n            dep_grp._ensure_is_loaded(dep_ref)\n\n        self.init_plugin(plugin)\n\n    def _check_common(self, ep_name: EPName, plugin):\n\"\"\"Perform both the common and specific checks a registered plugin.\n\n        Raises a TypeError with message in case of failure.\n        \"\"\"\n        # check correct base class of plugin, if stated\n        if self.Plugin.plugin_class:\n            util.check_is_subclass(ep_name, plugin, self.Plugin.plugin_class)\n\n    def check_plugin(self, ep_name: EPName, plugin: Type[T]):\n\"\"\"Perform plugin group specific checks on a registered plugin.\n\n        Raises a TypeError with message in case of failure.\n\n        To be overridden in subclasses for plugin group specific checks.\n\n        Args:\n            ep_name: Declared entrypoint name.\n            plugin: Object the entrypoint is pointing to.\n        \"\"\"\n        # NOTE: following cannot happen as long as we enforce\n        # overriding check_plugin.\n        # keep that here for now, in case we loosen this\n        # if type(self) is not PluginGroup:\n        #    return  # is not the \"plugingroup\" group itself\n\n        # these are the checks done for other plugin group plugins:\n        util.check_is_subclass(ep_name, plugin, PluginGroup)\n        util.check_is_subclass(ep_name, self.Plugin.plugin_info_class, PluginBase)\n        if plugin != PluginGroup:  # exclude itself. this IS its check_plugin\n            util.check_implements_method(ep_name, plugin, PluginGroup.check_plugin)\n\n        # NOTE: following cannot happen as long as we set the group\n        # automatically using the metaclass.\n        # keep that in case we decide to change that / get rid of the metaclass\n        # ---\n        # make sure that the declared plugin_info_class for the group sets 'group'\n        # and it is also equal to the plugin group 'name'.\n        # this is the safest way to make sure that Plugin.ref() works correctly.\n        # ppgi_cls = plugin.Plugin.plugin_info_class\n        # if not ppgi_cls.group:\n        #    raise TypeError(f\"{ep_name}: {ppgi_cls} is missing 'group' attribute!\")\n        # if not ppgi_cls.group == plugin.Plugin.name:\n        #    msg = f\"{ep_name}: {ppgi_cls.__name__}.group != {plugin.__name__}.Plugin.name!\"\n        #    raise TypeError(msg)\n\n    def init_plugin(self, plugin: Type[T]):\n\"\"\"Override this to do something after the plugin has been checked.\"\"\"\n        if type(self) is not PluginGroup:\n            return  # is not the \"plugingroup\" group itself\n        create_pg(plugin)  # create plugin group if it does not exist\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.PluginRef","title":"PluginRef  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PluginRef: TypeAlias = AnyPluginRef\n</code></pre> <p>Plugin reference class for this plugin group.</p>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return name of the plugin group.</p>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.packages","title":"packages  <code>property</code>","text":"<pre><code>packages: Dict[str, PluginPkgMeta]\n</code></pre> <p>Return metadata of all packages providing metador plugins.</p>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.Plugin","title":"Plugin","text":"<p>This is the plugin group plugin group, the first loaded group.</p> Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>class Plugin:\n\"\"\"This is the plugin group plugin group, the first loaded group.\"\"\"\n\n    name = PG_GROUP_NAME\n    version = (0, 1, 0)\n    plugin_info_class = PGPlugin\n    plugin_class: Type\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.versions","title":"versions","text":"<pre><code>versions(\n    p_name: str, version: Optional[SemVerTuple] = None\n) -&gt; List[AnyPluginRef]\n</code></pre> <p>Return installed versions of a plugin (compatible with given version).</p> Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>def versions(\n    self, p_name: str, version: Optional[SemVerTuple] = None\n) -&gt; List[AnyPluginRef]:\n\"\"\"Return installed versions of a plugin (compatible with given version).\"\"\"\n    refs = list(self._VERSIONS.get(p_name) or [])\n    if version is None:\n        return refs\n    requested = self.PluginRef(name=p_name, version=version)\n    return [ref for ref in refs if ref.supports(requested)]\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.resolve","title":"resolve","text":"<pre><code>resolve(\n    p_name: str, version: Optional[SemVerTuple] = None\n) -&gt; Optional[AnyPluginRef]\n</code></pre> <p>Return most recent compatible version of a plugin.</p> Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>def resolve(\n    self, p_name: str, version: Optional[SemVerTuple] = None\n) -&gt; Optional[AnyPluginRef]:\n\"\"\"Return most recent compatible version of a plugin.\"\"\"\n    if refs := self.versions(p_name, version):\n        return refs[-1]  # latest (compatible) version\n    return None\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.provider","title":"provider","text":"<pre><code>provider(ref: AnyPluginRef) -&gt; PluginPkgMeta\n</code></pre> <p>Return package metadata of Python package providing this plugin.</p> Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>def provider(self, ref: AnyPluginRef) -&gt; PluginPkgMeta:\n\"\"\"Return package metadata of Python package providing this plugin.\"\"\"\n    if type(self) is PluginGroup and ref.name == PG_GROUP_NAME:\n        # special case - the mother plugingroup plugin is not an EP,\n        # so we cheat a bit (schema is in same package, but is an EP)\n        return self.provider(self.resolve(\"schema\"))\n\n    ep_name = util.to_ep_name(ref.name, ref.version)\n    ep = self._ENTRY_POINTS[ep_name]\n    return self._PKG_META[cast(Any, ep).dist.name]\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.is_plugin","title":"is_plugin","text":"<pre><code>is_plugin(p_cls)\n</code></pre> <p>Return whether this class is a (possibly marked) installed plugin.</p> Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>def is_plugin(self, p_cls):\n\"\"\"Return whether this class is a (possibly marked) installed plugin.\"\"\"\n    if not isinstance(p_cls, type) or not issubclass(\n        p_cls, self.Plugin.plugin_class\n    ):\n        return False\n\n    c = UndefVersion._unwrap(p_cls) or p_cls  # get real underlying class\n    # check its exactly a registered plugin, if it has a Plugin section\n    if info := c.__dict__.get(\"Plugin\"):\n        if not isinstance(info, PluginBase):\n            return False\n        loaded_p = self._get_unsafe(info.name, info.version)\n        return loaded_p is c\n    else:\n        return False\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.keys","title":"keys","text":"<pre><code>keys() -&gt; Iterator[AnyPluginRef]\n</code></pre> <p>Return all names of all plugins.</p> Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>def keys(self) -&gt; Iterator[AnyPluginRef]:\n\"\"\"Return all names of all plugins.\"\"\"\n    for pgs in self._VERSIONS.values():\n        yield from pgs\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.values","title":"values","text":"<pre><code>values() -&gt; Iterator[Type[T]]\n</code></pre> <p>Return latest versions of all plugins (THIS LOADS ALL PLUGINS!).</p> Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>def values(self) -&gt; Iterator[Type[T]]:\n\"\"\"Return latest versions of all plugins (THIS LOADS ALL PLUGINS!).\"\"\"\n    return map(self.__getitem__, self.keys())\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.items","title":"items","text":"<pre><code>items() -&gt; Iterator[Tuple[AnyPluginRef, Type[T]]]\n</code></pre> <p>Return pairs of plugin name and latest installed version (THIS LOADS ALL PLUGINS!).</p> Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>def items(self) -&gt; Iterator[Tuple[AnyPluginRef, Type[T]]]:\n\"\"\"Return pairs of plugin name and latest installed version (THIS LOADS ALL PLUGINS!).\"\"\"\n    return map(lambda k: (k, self[k]), self.keys())\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.plugin_deps","title":"plugin_deps","text":"<pre><code>plugin_deps(plugin) -&gt; Set[AnyPluginRef]\n</code></pre> <p>Return additional automatically inferred dependencies for a plugin.</p> Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>def plugin_deps(self, plugin) -&gt; Set[AnyPluginRef]:\n\"\"\"Return additional automatically inferred dependencies for a plugin.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.check_plugin","title":"check_plugin","text":"<pre><code>check_plugin(ep_name: EPName, plugin: Type[T])\n</code></pre> <p>Perform plugin group specific checks on a registered plugin.</p> <p>Raises a TypeError with message in case of failure.</p> <p>To be overridden in subclasses for plugin group specific checks.</p> <p>Parameters:</p> Name Type Description Default <code>ep_name</code> <code>EPName</code> <p>Declared entrypoint name.</p> required <code>plugin</code> <code>Type[T]</code> <p>Object the entrypoint is pointing to.</p> required Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>def check_plugin(self, ep_name: EPName, plugin: Type[T]):\n\"\"\"Perform plugin group specific checks on a registered plugin.\n\n    Raises a TypeError with message in case of failure.\n\n    To be overridden in subclasses for plugin group specific checks.\n\n    Args:\n        ep_name: Declared entrypoint name.\n        plugin: Object the entrypoint is pointing to.\n    \"\"\"\n    # NOTE: following cannot happen as long as we enforce\n    # overriding check_plugin.\n    # keep that here for now, in case we loosen this\n    # if type(self) is not PluginGroup:\n    #    return  # is not the \"plugingroup\" group itself\n\n    # these are the checks done for other plugin group plugins:\n    util.check_is_subclass(ep_name, plugin, PluginGroup)\n    util.check_is_subclass(ep_name, self.Plugin.plugin_info_class, PluginBase)\n    if plugin != PluginGroup:  # exclude itself. this IS its check_plugin\n        util.check_implements_method(ep_name, plugin, PluginGroup.check_plugin)\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.PluginGroup.init_plugin","title":"init_plugin","text":"<pre><code>init_plugin(plugin: Type[T])\n</code></pre> <p>Override this to do something after the plugin has been checked.</p> Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>def init_plugin(self, plugin: Type[T]):\n\"\"\"Override this to do something after the plugin has been checked.\"\"\"\n    if type(self) is not PluginGroup:\n        return  # is not the \"plugingroup\" group itself\n    create_pg(plugin)  # create plugin group if it does not exist\n</code></pre>"},{"location":"reference/metador_core/plugin/interface/#metador_core.plugin.interface.create_pg","title":"create_pg","text":"<pre><code>create_pg(pg_cls)\n</code></pre> <p>Create plugin group instance if it does not exist.</p> Source code in <code>src/metador_core/plugin/interface.py</code> <pre><code>def create_pg(pg_cls):\n\"\"\"Create plugin group instance if it does not exist.\"\"\"\n    pg_ref = AnyPluginRef(\n        group=PG_GROUP_NAME, name=pg_cls.Plugin.name, version=pg_cls.Plugin.version\n    )\n    if pg_ref in _plugin_groups:\n        return _plugin_groups[pg_ref]\n\n    if not isinstance(pg_cls.Plugin, PluginBase):\n        # magic - substitute Plugin class with parsed plugin object\n        pg_cls.Plugin = PGPlugin.parse_info(pg_cls.Plugin)\n\n    # TODO: currently we cannot distinguish entrypoints\n    # for different versions of the plugin group.\n    # should not be problematic for now,\n    # as the groups should not change much\n    pg = pg_cls(get_group(pg_ref.name))\n    _plugin_groups[pg_ref] = pg\n</code></pre>"},{"location":"reference/metador_core/plugin/metaclass/","title":"metaclass","text":"<p>This module defines a metaclass that should be used for all plugin types.</p>"},{"location":"reference/metador_core/plugin/metaclass/#metador_core.plugin.metaclass.MarkerMixin","title":"MarkerMixin","text":"<p>Base class for Metador-internal marker mixins.</p> <p>It can be used to hand out copies of classes with markings, without actually modifying the original class.</p> Source code in <code>src/metador_core/plugin/metaclass.py</code> <pre><code>class MarkerMixin:\n\"\"\"Base class for Metador-internal marker mixins.\n\n    It can be used to hand out copies of classes with markings,\n    without actually modifying the original class.\n    \"\"\"\n\n    @classmethod\n    def _fieldname(cls):\n        return f\"__{cls.__name__}_unwrapped__\"\n\n    @classmethod\n    def _is_marked(cls, c) -&gt; bool:\n\"\"\"Return whether `c` is proper subclass of this marker.\"\"\"\n        return c is not cls and issubclass(c, cls)\n\n    @classmethod\n    def _mark_class(cls, c):\n\"\"\"Mark a class with this marker mixin.\"\"\"\n        if cls._is_marked(c):\n            raise TypeError(f\"{c} already marked by {cls}!\")\n\n        ret = c.__class__(c.__name__, (cls, c), {})\n        # ret.__module__ = c.__module__\n        setattr(ret, cls._fieldname(), c)\n\n        # NOTE: discouraged!\n        # https://docs.python.org/3/howto/annotations.html#annotations-howto\n        # ----\n        # anns = getattr(ret, \"__annotations__\", {})\n        # anns[unw_field] = ClassVar[Type]\n        # ret.__annotations__ = anns\n\n        return ret\n\n    @classmethod\n    def _unwrap(cls, c):\n\"\"\"Return the original class, or None if given argument is not marked.\"\"\"\n        if issubclass(c, cls):\n            return getattr(c, cls._fieldname())\n        else:\n            return None\n</code></pre>"},{"location":"reference/metador_core/plugin/metaclass/#metador_core.plugin.metaclass.UndefVersion","title":"UndefVersion","text":"<p>             Bases: <code>MarkerMixin</code></p> <p>Marker for a plugin class retrieved with no specified version.</p> <p>We have to do this crazy thing, because wrapt.ObjectProxy-wrapped classes can be transparently derived, and what is even worse, the derived class is not wrapped anymore.</p> <p>The mixin subclass approach therefore makes more sense here, as the metaclass then can check for its presence.</p> Source code in <code>src/metador_core/plugin/metaclass.py</code> <pre><code>class UndefVersion(MarkerMixin):\n\"\"\"Marker for a plugin class retrieved with no specified version.\n\n    We have to do this crazy thing, because wrapt.ObjectProxy-wrapped\n    classes can be transparently derived, and what is even worse,\n    the derived class is not wrapped anymore.\n\n    The mixin subclass approach therefore makes more sense here, as\n    the metaclass then can check for its presence.\n    \"\"\"\n\n    @classmethod\n    def _mark_class(cls, c):\n        # NOTE: we also want to mark nested non-plugins to prevent subclassing\n        # so we do not assume that cls.Plugin is defined\n        ret = super()._mark_class(c)\n        # make sure that the Plugin section *is* actually inherited,\n        # (normally this is prevented by the plugin metaclass)\n        # that way we can use the marked class as if it was the real one\n        if not ret.__dict__.get(\"Plugin\"):\n            ret.Plugin = c.Plugin\n\n        return ret\n</code></pre>"},{"location":"reference/metador_core/plugin/metaclass/#metador_core.plugin.metaclass.PluginMetaclassMixin","title":"PluginMetaclassMixin","text":"<p>             Bases: <code>type</code></p> <p>Metaclass mixin to be used with plugins of any group.</p> <p>It provides an is_plugin property to classes to quickly check if they seem to be valid registered plugins.</p> <p>It ensures that: * the <code>Plugin</code> inner class is not automatically inherited * registered Plugin classes cannot be subclassed if loaded without a fixed version</p> <p>For the second part, this works together with the PluginGroup implementation, which makes sure that schemas requested without versions are not actually handed out, but instead users get a subclass with the <code>UndefVersion</code> mixin we can detect here.</p> Source code in <code>src/metador_core/plugin/metaclass.py</code> <pre><code>class PluginMetaclassMixin(type):\n\"\"\"Metaclass mixin to be used with plugins of any group.\n\n    It provides an is_plugin property to classes to quickly check if they\n    seem to be valid registered plugins.\n\n    It ensures that:\n    * the `Plugin` inner class is not automatically inherited\n    * registered Plugin classes cannot be subclassed if loaded without a fixed version\n\n    For the second part, this works together with the PluginGroup implementation,\n    which makes sure that schemas requested without versions are not actually handed out,\n    but instead users get a subclass with the `UndefVersion` mixin we can detect here.\n    \"\"\"\n\n    def __repr__(self):\n        # add plugin name and version to default class repr\n        if c := UndefVersion._unwrap(self):\n            # indicate the attached UndefVersion\n            return f\"{repr(c)} (version unspecified)\"\n        else:\n            # indicate loaded plugin name and version\n            pg_str = \"\"\n            if pgi := self.__dict__.get(\"Plugin\"):\n                pgi = self.Plugin\n                pg_str = f\" ({pgi.name} {to_semver_str(pgi.version)})\"\n\n            return f\"{super().__repr__()}{pg_str}\"\n\n    def __new__(cls, name, bases, dct):\n        # prevent inheriting from a plugin accessed without stated version\n        for b in bases:\n            if UndefVersion._is_marked(b):\n                if pgi := b.__dict__.get(\"Plugin\"):\n                    ref = f\"plugin '{pgi.name}'\"\n                else:\n                    ref = f\"{UndefVersion._unwrap(b)} originating from a plugin\"\n                msg = f\"{name}: Cannot inherit from {ref} of unspecified version!\"\n                raise TypeError(msg)\n\n        # prevent inheriting inner Plugin class by setting it to None\n        if \"Plugin\" not in dct:\n            dct[\"Plugin\"] = None\n\n        # hide special marker base class from parent metaclass (if present)\n        # so it does not have to know about any of this happening\n        # (otherwise it could interfere with other checks)\n        # NOTE: needed e.g. for schemas to work properly\n        filt_bases = tuple(b for b in bases if b is not UndefVersion)\n        ret = super().__new__(cls, name, filt_bases, dct)\n\n        # add marker back, as if it was present all along\n        if len(filt_bases) &lt; len(bases):\n            ret.__bases__ = (UndefVersion, *ret.__bases__)\n\n        return ret\n</code></pre>"},{"location":"reference/metador_core/plugin/types/","title":"types","text":"<p>Simple types and utilities for plugins.</p>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.SemVerTuple","title":"SemVerTuple  <code>module-attribute</code>","text":"<pre><code>SemVerTuple: TypeAlias = Tuple[\n    NonNegativeInt, NonNegativeInt, NonNegativeInt\n]\n</code></pre> <p>Type to be used for SemVer triples.</p>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.NAME","title":"NAME  <code>module-attribute</code>","text":"<pre><code>NAME: Final[str] = f'{LETTER}{ALNUM}({LETSEP}?{ALNUM})*'\n</code></pre> <p>An unqualified name begins with a letter and ends with a letter or number.</p> <p>It consists of: lowercase letters, digits, _ and -</p>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.QUAL_NAME","title":"QUAL_NAME  <code>module-attribute</code>","text":"<pre><code>QUAL_NAME: Final[str] = f'{NAME}({NSSEP}{NAME})*'\n</code></pre> <p>A qualified name is a sequence of unqualified names separated with .</p>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.EP_NAME_VER_SEP","title":"EP_NAME_VER_SEP  <code>module-attribute</code>","text":"<pre><code>EP_NAME_VER_SEP: str = '__'\n</code></pre> <p>Separator between plugin name and semantic version in entry point name.</p>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.EP_NAME_REGEX","title":"EP_NAME_REGEX  <code>module-attribute</code>","text":"<pre><code>EP_NAME_REGEX: Final[\n    str\n] = f\"{QUAL_NAME}{EP_NAME_VER_SEP}{SEMVER_STR_REGEX}\"\n</code></pre> <p>Regular expression that all metador plugin entry points must match.</p>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.PG_PREFIX","title":"PG_PREFIX  <code>module-attribute</code>","text":"<pre><code>PG_PREFIX: str = 'metador_'\n</code></pre> <p>Group prefix for metador plugin entry point groups.</p>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.SemVerStr","title":"SemVerStr","text":"<p>             Bases: <code>FullMatch</code></p> <p>Semantic version string (x.y.z).</p> Source code in <code>src/metador_core/plugin/types.py</code> <pre><code>class SemVerStr(FullMatch, pattern=SEMVER_STR_REGEX):\n\"\"\"Semantic version string (x.y.z).\"\"\"\n</code></pre>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.EPName","title":"EPName","text":"<p>             Bases: <code>FullMatch</code></p> <p>Valid entry point for a metador plugin.</p> Source code in <code>src/metador_core/plugin/types.py</code> <pre><code>class EPName(FullMatch, pattern=EP_NAME_REGEX):\n\"\"\"Valid entry point for a metador plugin.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.EPGroupName","title":"EPGroupName","text":"<p>             Bases: <code>FullMatch</code></p> <p>Valid internal group name for a metador plugin group.</p> Source code in <code>src/metador_core/plugin/types.py</code> <pre><code>class EPGroupName(FullMatch, pattern=rf\"{PG_PREFIX}.*\"):\n\"\"\"Valid internal group name for a metador plugin group.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.PluginLike","title":"PluginLike","text":"<p>             Bases: <code>Protocol</code></p> <p>A Plugin has a Plugin inner class with plugin infos.</p> Source code in <code>src/metador_core/plugin/types.py</code> <pre><code>class PluginLike(Protocol):\n\"\"\"A Plugin has a Plugin inner class with plugin infos.\"\"\"\n\n    Plugin: ClassVar[Any]  # actually its PluginBase, but this happens at runtime\n</code></pre>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.to_ep_name","title":"to_ep_name","text":"<pre><code>to_ep_name(p_name: str, p_version: SemVerTuple) -&gt; EPName\n</code></pre> <p>Return canonical entrypoint name <code>PLUGIN_NAME__MAJ.MIN.FIX</code>.</p> Source code in <code>src/metador_core/plugin/types.py</code> <pre><code>def to_ep_name(p_name: str, p_version: SemVerTuple) -&gt; EPName:\n\"\"\"Return canonical entrypoint name `PLUGIN_NAME__MAJ.MIN.FIX`.\"\"\"\n    return EPName(f\"{p_name}{EP_NAME_VER_SEP}{to_semver_str(p_version)}\")\n</code></pre>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.from_ep_name","title":"from_ep_name","text":"<pre><code>from_ep_name(ep_name: EPName) -&gt; Tuple[str, SemVerTuple]\n</code></pre> <p>Split entrypoint name into <code>(PLUGIN_NAME, (MAJ,MIN,FIX))</code>.</p> Source code in <code>src/metador_core/plugin/types.py</code> <pre><code>def from_ep_name(ep_name: EPName) -&gt; Tuple[str, SemVerTuple]:\n\"\"\"Split entrypoint name into `(PLUGIN_NAME, (MAJ,MIN,FIX))`.\"\"\"\n    pname, pverstr = ep_name.split(EP_NAME_VER_SEP)\n    return (pname, from_semver_str(SemVerStr(pverstr)))\n</code></pre>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.ep_name_has_namespace","title":"ep_name_has_namespace","text":"<pre><code>ep_name_has_namespace(ep_name: EPName)\n</code></pre> <p>Check whether the passed name has a namespace prefix.</p> Source code in <code>src/metador_core/plugin/types.py</code> <pre><code>def ep_name_has_namespace(ep_name: EPName):\n\"\"\"Check whether the passed name has a namespace prefix.\"\"\"\n    name, ver = from_ep_name(ep_name)\n    return len(name.split(\".\", 1)) &gt; 1\n</code></pre>"},{"location":"reference/metador_core/plugin/types/#metador_core.plugin.types.plugin_args","title":"plugin_args","text":"<pre><code>plugin_args(\n    plugin=\"\",\n    version: Optional[SemVerTuple] = None,\n    *,\n    require_version: bool = False\n) -&gt; Tuple[str, Optional[SemVerTuple]]\n</code></pre> <p>Return requested plugin name and version based on passed arguments.</p> <p>Helper for function argument parsing.</p> Source code in <code>src/metador_core/plugin/types.py</code> <pre><code>def plugin_args(\n    plugin=\"\",  # actually takes: Union[str, PluginRef, PluginLike]\n    version: Optional[SemVerTuple] = None,\n    *,\n    require_version: bool = False,\n    # group: Optional[str]\n) -&gt; Tuple[str, Optional[SemVerTuple]]:\n\"\"\"Return requested plugin name and version based on passed arguments.\n\n    Helper for function argument parsing.\n    \"\"\"\n    name: str = \"\"\n    vers: Optional[SemVerTuple] = version\n\n    if isinstance(plugin, str):\n        name = plugin\n    if isinstance(plugin, tuple) and len(plugin) == 2:\n        name = plugin[0]\n        if not vers:\n            vers = plugin[1]\n    elif isinstance(plugin, HasNameVersion):\n        name = plugin.name\n        if not vers:\n            vers = plugin.version\n    elif pgi := getattr(plugin, \"Plugin\", None):\n        if isinstance(pgi, HasNameVersion):\n            return plugin_args(pgi, version, require_version=require_version)\n\n    if require_version and vers is None:\n        raise ValueError(f\"No version of {name} specified, but is required!\")\n    return (name, vers)\n</code></pre>"},{"location":"reference/metador_core/plugin/util/","title":"util","text":"<p>General utitilies with relevance for plugins.</p>"},{"location":"reference/metador_core/plugin/util/#metador_core.plugin.util.check_implements_method","title":"check_implements_method","text":"<pre><code>check_implements_method(\n    name: str, plugin: str, base_method: str\n)\n</code></pre> <p>Check whether plugin overrides a method of its superclass.</p> Source code in <code>src/metador_core/plugin/util.py</code> <pre><code>def check_implements_method(name: str, plugin, base_method):\n\"\"\"Check whether plugin overrides a method of its superclass.\"\"\"\n    if not implements_method(plugin, base_method):\n        msg = f\"{name}: {plugin} does not implement {base_method.__name__}!\"\n        raise TypeError(msg)\n</code></pre>"},{"location":"reference/metador_core/plugin/util/#metador_core.plugin.util.check_is_subclass","title":"check_is_subclass","text":"<pre><code>check_is_subclass(name: str, plugin: str, base: str)\n</code></pre> <p>Check whether plugin has expected parent class (helper method).</p> Source code in <code>src/metador_core/plugin/util.py</code> <pre><code>def check_is_subclass(name: str, plugin, base):\n\"\"\"Check whether plugin has expected parent class (helper method).\"\"\"\n    if not issubclass(plugin, base):\n        msg = f\"{name}: {plugin} is not subclass of {base}!\"\n        raise TypeError(msg)\n</code></pre>"},{"location":"reference/metador_core/plugin/util/#metador_core.plugin.util.register_in_group","title":"register_in_group","text":"<pre><code>register_in_group(\n    pgroup: PluginGroup,\n    plugin: Optional[Type[T]] = None,\n    *,\n    violently: bool = False\n)\n</code></pre> <p>Register and load a plugin manually, without defining an entry point.</p> Source code in <code>src/metador_core/plugin/util.py</code> <pre><code>def register_in_group(\n    pgroup: PluginGroup,\n    plugin: Optional[Type[T]] = None,\n    *,\n    violently: bool = False,\n):\n\"\"\"Register and load a plugin manually, without defining an entry point.\"\"\"\n    if not violently and not is_notebook():\n        raise RuntimeError(\"This is not supposed to be used outside of notebooks!\")\n\n    def manual_register(plugin: Type[T]) -&gt; Type[T]:\n        pginfo = plugin.Plugin\n        ep_name = to_ep_name(pginfo.name, pginfo.version)\n        pg_ref = pgroup.PluginRef(name=pginfo.name, version=pginfo.version)\n\n        pgroup._ENTRY_POINTS[ep_name] = None\n        pgroup._LOADED_PLUGINS[pg_ref] = plugin\n        if pg_ref.name not in pgroup._VERSIONS:\n            pgroup._VERSIONS[pg_ref.name] = []\n        pgroup._VERSIONS[pg_ref.name].append(pg_ref)\n\n        pgroup._load_plugin(ep_name, plugin)\n        if not violently:\n            eprint(\n                f\"Notebook: Plugin '{pginfo.name}' registered in '{pgroup.name}' group!\"\n            )  # pragma: no cover\n        return plugin\n\n    if not plugin:\n        return manual_register  # used as decorator\n    else:\n        if not is_pluginlike(plugin, check_group=False):\n            raise RuntimeError(\"This class has no inner Plugin class!\")\n\n        manual_register(plugin)  # used as normal function\n</code></pre>"},{"location":"reference/metador_core/rdf/","title":"rdf","text":"<p>Utilities that help with schema development.</p> <p>Mostly for import of schemas etc. from other sources and formats.</p>"},{"location":"reference/metador_core/rdf/lib/","title":"lib","text":"<p>Wrappers and helpers for RDFlib.</p>"},{"location":"reference/metador_core/rdf/lib/#metador_core.rdf.lib.GraphNode","title":"GraphNode","text":"<p>             Bases: <code>ObjectProxy</code></p> <p>Wrapper for rdflib helping to navigate entities in an <code>rdflib.Graph</code>.</p> Source code in <code>src/metador_core/rdf/lib.py</code> <pre><code>class GraphNode(wrapt.ObjectProxy):\n\"\"\"Wrapper for rdflib helping to navigate entities in an `rdflib.Graph`.\"\"\"\n\n    _self_graph: rdflib.Graph\n    obj: Identifier\n\n    def __init__(self, graph: rdflib.Graph, obj: Identifier = None):\n        super().__init__(obj)\n        self._self_graph = graph\n\n    @property\n    def graph(self):\n        return self._self_graph\n\n    @property\n    def node(self):\n        return self.__wrapped__\n\n    def wrap(self, obj: Identifier):\n        return GraphNode(self.graph, obj)\n\n    def is_literal(self):\n        return isinstance(self.__wrapped__, rdflib.Literal)\n\n    def is_uriref(self):\n        return isinstance(self.__wrapped__, rdflib.URIRef)\n\n    # ----\n\n    def edges_in(self):\n        return map(self.wrap, self.graph.predicates(object=self.node, unique=True))  # type: ignore\n\n    def edges_out(self):\n        return map(self.wrap, self.graph.predicates(subject=self.node, unique=True))  # type: ignore\n\n    def objects(self, pred):\n        return map(self.wrap, self.graph.objects(self.node, pred, unique=True))  # type: ignore\n\n    def subjects(self, pred):\n        return map(self.wrap, self.graph.subjects(pred, self.node, unique=True))  # type: ignore\n\n    def subject(self, pred):\n        try:\n            return at_most_one(self.subjects(pred))\n        except ValueError:\n            msg = f\"Expected to get exactly one match for: (*, {pred}, {self.obj})\"\n            raise ValueError(msg)\n\n    def object(self, pred):\n        try:\n            return at_most_one(self.objects(pred))\n        except ValueError:\n            msg = f\"Expected to get exactly one match for: ({self.obj}, {pred}, *)\"\n            raise ValueError(msg)\n</code></pre>"},{"location":"reference/metador_core/rdf/lib/#metador_core.rdf.lib.RDFParser","title":"RDFParser","text":"<p>             Bases: <code>ObjectProxy</code></p> <p>Helper wrapper to access entity properties backed by an RDFlib graph.</p> <p>Ensures that queries are only performed when needed (more efficient).</p> <p>Use this e.g. to parse linked data with expected structure into your custom data structures, e.g. for use in schemas.</p> Source code in <code>src/metador_core/rdf/lib.py</code> <pre><code>class RDFParser(wrapt.ObjectProxy):\n\"\"\"Helper wrapper to access entity properties backed by an RDFlib graph.\n\n    Ensures that queries are only performed when needed (more efficient).\n\n    Use this e.g. to parse linked data with expected structure into your custom\n    data structures, e.g. for use in schemas.\n    \"\"\"\n\n    __wrapped__: GraphNode\n\n    def __init__(self, node: GraphNode):\n        super().__init__(node)\n        self._self_parsed: Dict[str, Any] = {}\n\n    def __getattr__(self, key: str):\n        # special case: return the method\n        if key.startswith(\"parse_\"):\n            return wrapt.ObjectProxy.__getattr__(self, key)\n\n        # try to return the attribute\n        if val := self._self_parsed.get(key):\n            return val\n        # need first to compute the attribute\n        if pfunc := getattr(self, f\"parse_{key}\"):\n            val = pfunc(self.__wrapped__)\n            self._self_parsed[key] = val\n            return val\n        # no parser defined -&gt; pass through\n        return getattr(self.__wrapped__, key)\n\n    @classmethod\n    def from_graph(cls, g: rdflib.Graph, obj: Union[str, rdflib.URIRef]):\n        return cls(GraphNode(g, rdflib.URIRef(obj)))\n</code></pre>"},{"location":"reference/metador_core/rdf/lib/#metador_core.rdf.lib.at_most_one","title":"at_most_one","text":"<pre><code>at_most_one(g)\n</code></pre> <p>Return at most one element from a generator.</p> <p>If there are more, will raise ValueError.</p> Source code in <code>src/metador_core/rdf/lib.py</code> <pre><code>def at_most_one(g):\n\"\"\"Return at most one element from a generator.\n\n    If there are more, will raise ValueError.\n    \"\"\"\n    ret = next(g, None)\n    if next(g, None) is None:\n        return ret\n    else:\n        raise ValueError()\n</code></pre>"},{"location":"reference/metador_core/rdf/skos/","title":"skos","text":"<p>Parse a SKOS ConceptScheme, explore it and generate enums for schemas.</p> <p>Assumptions: * ConceptScheme collects Concepts via hasTopConcept * Concepts have 0-1 broader Concept and 0-n narrower Concept * Concepts define a prefLabel * Concepts have a unique IRI (ideally resolving to the concept sub-graph)</p> <p>All of this applies to e.g. https://data.nist.gov/od/dm/nmrr/vocab</p>"},{"location":"reference/metador_core/rdf/skos/#metador_core.rdf.skos.SemanticEnum","title":"SemanticEnum","text":"<p>             Bases: <code>Enum</code></p> <p>Enum subclass for Enums generated from a semantic taxonomy.</p> Source code in <code>src/metador_core/rdf/skos.py</code> <pre><code>class SemanticEnum(Enum):\n\"\"\"Enum subclass for Enums generated from a semantic taxonomy.\"\"\"\n\n    __self__term__: Tuple[str, str]\n</code></pre>"},{"location":"reference/metador_core/rdf/skos/#metador_core.rdf.skos.Concept","title":"Concept","text":"<p>             Bases: <code>RDFParser</code></p> <p>A concept is a node in a taxonomy defining a term.</p> Source code in <code>src/metador_core/rdf/skos.py</code> <pre><code>class Concept(RDFParser):\n\"\"\"A concept is a node in a taxonomy defining a term.\"\"\"\n\n    _depth: int = 0\n\"\"\"Depth of the concept in the taxonomy forest.\"\"\"\n\n    id: str\n\"\"\"ID of the concept (should be IRI resolving to concept definition).\"\"\"\n\n    prefLabel: str\n\"\"\"Canonical name of the concept.\"\"\"\n\n    broader: Optional[Concept]\n\"\"\"Unique more general concept in the taxonomy (unless it is a root).\"\"\"\n\n    narrower: List[Concept]\n\"\"\"A list of more specific sub-concepts.\"\"\"\n\n    def __eq__(self, other):\n\"\"\"Return whether two concept objects are equal.\n\n        For our purposes, concepts are the same if they come from the same graph\n        and refer to the same IRI.\n        \"\"\"\n        return (\n            isinstance(other, Concept)\n            and self.__wrapped__.graph == other.__wrapped__.graph\n            and self.id == other.id\n        )\n\n    def new_subconcept(self, node: GraphNode):\n        ret = Concept(node)\n        ret._depth = self._depth + 1\n        return ret\n\n    def new_superconcept(self, node: GraphNode):\n        ret = Concept(node)\n        ret._depth = self._depth - 1\n        return ret\n\n    # ----\n\n    def parse_id(self, node) -&gt; str:\n        assert isinstance(node, rdflib.URIRef)\n        return node.toPython()\n\n    def parse_prefLabel(self, node: GraphNode) -&gt; str:\n        val = node.object(SKOS.prefLabel)\n        assert val.is_literal()\n        return val.value\n\n    def parse_broader(self, node: GraphNode) -&gt; Optional[Concept]:\n        if v := node.object(SKOS.broader):\n            return self.new_superconcept(v)\n        return None\n\n    def parse_narrower(self, node: GraphNode) -&gt; List[Concept]:\n        q = node.objects(SKOS.narrower)\n        return list(map(self.new_subconcept, q))\n\n    # ----\n\n    def pretty_print(\n        self, *, max_depth=None, indent: Optional[int] = None, indent_unit: str = \"\\t\"\n    ):\n        indent = indent or 0\n        line = f\"{indent*indent_unit}{self.prefLabel} -&gt; {self.id}\"\n        lines = [line]\n\n        if max_depth is None or max_depth &gt; 0:\n            max_depth_next = max_depth - 1 if max_depth else None\n            lines += list(\n                map(\n                    lambda x: x.pretty_print(\n                        indent=indent + 1, max_depth=max_depth_next\n                    ),\n                    self.narrower,\n                )\n            )\n\n        return \"\\n\".join(lines)\n\n    def __str__(self):\n        return self.pretty_print()\n\n    # ----\n\n    @property\n    def term(self):\n\"\"\"Return (ID, string) pair for this concept.\"\"\"\n        return (self.id, pythonize_name(self.prefLabel))\n\n    def sub_terms(self, *, deep: bool = False):\n\"\"\"Return dict of subconcepts (recursively, if desired).\"\"\"\n        ret = dict(map(lambda x: x.term, self.narrower))\n        if deep:\n            _, pyname = self.term\n            ret.update(\n                dict(\n                    t\n                    for dct in map(lambda x: x.sub_terms(deep=deep), self.narrower)\n                    for t in dct.items()\n                )\n            )\n        return ret\n\n    def to_enum(self, deep: bool = False) -&gt; Type[SemanticEnum]:\n\"\"\"Return Enum with immediate child concepts as possible values.\"\"\"\n        if deep:  # pragma: no cover\n            # TODO: think how to combine the enums in the best way\n            raise NotImplementedError\n\n        ts = self.sub_terms(deep=deep)\n        assert len(ts) == len(\n            set(ts.values())\n        )  # expect that human values are also unique\n        ret = cast(\n            Type[SemanticEnum],\n            SemanticEnum(  # type: ignore\n                f\"{self.term[1].capitalize()}_Enum\", {v: k for k, v in ts.items()}\n            ),\n        )\n        # useful information for \"deep\" mode:\n        ret.__self_term__ = self.term  # type: ignore\n        return ret\n</code></pre>"},{"location":"reference/metador_core/rdf/skos/#metador_core.rdf.skos.Concept.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>ID of the concept (should be IRI resolving to concept definition).</p>"},{"location":"reference/metador_core/rdf/skos/#metador_core.rdf.skos.Concept.prefLabel","title":"prefLabel  <code>instance-attribute</code>","text":"<pre><code>prefLabel: str\n</code></pre> <p>Canonical name of the concept.</p>"},{"location":"reference/metador_core/rdf/skos/#metador_core.rdf.skos.Concept.broader","title":"broader  <code>instance-attribute</code>","text":"<pre><code>broader: Optional[Concept]\n</code></pre> <p>Unique more general concept in the taxonomy (unless it is a root).</p>"},{"location":"reference/metador_core/rdf/skos/#metador_core.rdf.skos.Concept.narrower","title":"narrower  <code>instance-attribute</code>","text":"<pre><code>narrower: List[Concept]\n</code></pre> <p>A list of more specific sub-concepts.</p>"},{"location":"reference/metador_core/rdf/skos/#metador_core.rdf.skos.Concept.term","title":"term  <code>property</code>","text":"<pre><code>term\n</code></pre> <p>Return (ID, string) pair for this concept.</p>"},{"location":"reference/metador_core/rdf/skos/#metador_core.rdf.skos.Concept.__eq__","title":"__eq__","text":"<pre><code>__eq__(other)\n</code></pre> <p>Return whether two concept objects are equal.</p> <p>For our purposes, concepts are the same if they come from the same graph and refer to the same IRI.</p> Source code in <code>src/metador_core/rdf/skos.py</code> <pre><code>def __eq__(self, other):\n\"\"\"Return whether two concept objects are equal.\n\n    For our purposes, concepts are the same if they come from the same graph\n    and refer to the same IRI.\n    \"\"\"\n    return (\n        isinstance(other, Concept)\n        and self.__wrapped__.graph == other.__wrapped__.graph\n        and self.id == other.id\n    )\n</code></pre>"},{"location":"reference/metador_core/rdf/skos/#metador_core.rdf.skos.Concept.sub_terms","title":"sub_terms","text":"<pre><code>sub_terms(*, deep: bool = False)\n</code></pre> <p>Return dict of subconcepts (recursively, if desired).</p> Source code in <code>src/metador_core/rdf/skos.py</code> <pre><code>def sub_terms(self, *, deep: bool = False):\n\"\"\"Return dict of subconcepts (recursively, if desired).\"\"\"\n    ret = dict(map(lambda x: x.term, self.narrower))\n    if deep:\n        _, pyname = self.term\n        ret.update(\n            dict(\n                t\n                for dct in map(lambda x: x.sub_terms(deep=deep), self.narrower)\n                for t in dct.items()\n            )\n        )\n    return ret\n</code></pre>"},{"location":"reference/metador_core/rdf/skos/#metador_core.rdf.skos.Concept.to_enum","title":"to_enum","text":"<pre><code>to_enum(deep: bool = False) -&gt; Type[SemanticEnum]\n</code></pre> <p>Return Enum with immediate child concepts as possible values.</p> Source code in <code>src/metador_core/rdf/skos.py</code> <pre><code>def to_enum(self, deep: bool = False) -&gt; Type[SemanticEnum]:\n\"\"\"Return Enum with immediate child concepts as possible values.\"\"\"\n    if deep:  # pragma: no cover\n        # TODO: think how to combine the enums in the best way\n        raise NotImplementedError\n\n    ts = self.sub_terms(deep=deep)\n    assert len(ts) == len(\n        set(ts.values())\n    )  # expect that human values are also unique\n    ret = cast(\n        Type[SemanticEnum],\n        SemanticEnum(  # type: ignore\n            f\"{self.term[1].capitalize()}_Enum\", {v: k for k, v in ts.items()}\n        ),\n    )\n    # useful information for \"deep\" mode:\n    ret.__self_term__ = self.term  # type: ignore\n    return ret\n</code></pre>"},{"location":"reference/metador_core/rdf/skos/#metador_core.rdf.skos.ConceptScheme","title":"ConceptScheme","text":"<p>             Bases: <code>RDFParser</code></p> <p>A concept scheme points to the roots of a taxonomy forest.</p> <p>The the top level concepts are assumed to be unrelated (otherwise they should be united by the broader super-concept).</p> <p>For this reason, you cannot generate an enum on this level.</p> Source code in <code>src/metador_core/rdf/skos.py</code> <pre><code>class ConceptScheme(RDFParser):\n\"\"\"A concept scheme points to the roots of a taxonomy forest.\n\n    The the top level concepts are assumed to be unrelated\n    (otherwise they should be united by the broader super-concept).\n\n    For this reason, you cannot generate an enum on this level.\n    \"\"\"\n\n    id: str\n    hasTopConcept: List[Concept]\n\n    def parse_id(self, node):\n        assert isinstance(node, rdflib.URIRef)\n        return node.toPython()\n\n    def parse_hasTopConcept(self, node: GraphNode):\n        q = node.objects(SKOS.hasTopConcept)\n        return list(map(Concept, q))\n\n    # ----\n\n    def pretty_print(self, **kwargs):\n        return \"\\n\".join(map(lambda x: x.pretty_print(**kwargs), self.hasTopConcept))\n\n    def __str__(self):\n        return self.pretty_print()\n</code></pre>"},{"location":"reference/metador_core/schema/","title":"schema","text":"<p>Metador schemas.</p>"},{"location":"reference/metador_core/schema/#metador_core.schema.MetadataSchema","title":"MetadataSchema","text":"<p>             Bases: <code>SchemaBase</code></p> <p>Extends Pydantic models with custom serializers and functions.</p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>class MetadataSchema(SchemaBase, metaclass=SchemaMetaclass):\n\"\"\"Extends Pydantic models with custom serializers and functions.\"\"\"\n\n    Plugin: ClassVar[Optional[Type]]  # user-defined inner class (for schema plugins)\n</code></pre>"},{"location":"reference/metador_core/schema/base/","title":"base","text":""},{"location":"reference/metador_core/schema/base/#metador_core.schema.base.BaseModelPlus","title":"BaseModelPlus","text":"<p>             Bases: <code>ParserMixin</code>, <code>BaseModel</code></p> <p>Extended pydantic BaseModel with some good defaults.</p> <p>Used as basis for various entities, including: * Metadata schemas * Harvester arguments</p> Source code in <code>src/metador_core/schema/base.py</code> <pre><code>class BaseModelPlus(ParserMixin, BaseModel, metaclass=DynEncoderModelMetaclass):\n\"\"\"Extended pydantic BaseModel with some good defaults.\n\n    Used as basis for various entities, including:\n    * Metadata schemas\n    * Harvester arguments\n    \"\"\"\n\n    class Config:\n        # keep extra fields by default\n        extra = Extra.allow\n        # make PrivateAttr wrappers not always needed\n        underscore_attrs_are_private = True\n        # serialize enums properly\n        use_enum_values = True\n        # when alias is set, still allow using field name\n        # (we use aliases for invalid attribute names in Python)\n        allow_population_by_field_name = True\n        # users should jump through hoops to add invalid stuff\n        validate_assignment = True\n        # defaults should also be validated\n        validate_all = True\n        # for JSON compat\n        allow_inf_nan = False\n        # pydantic anystr config: non-empty, non-whitespace\n        # (but we prefer NonEmptyStr anyway for inheritance)\n        anystr_strip_whitespace = True\n        min_anystr_length = 1\n\n    def dict(self, *args, **kwargs):\n\"\"\"Return a dict.\n\n        Nota that this will eliminate all pydantic models,\n        but might still contain complex value types.\n        \"\"\"\n        return super().dict(*args, **_mod_def_dump_args(kwargs))\n\n    def json(self, *args, **kwargs) -&gt; str:\n\"\"\"Return serialized JSON as string.\"\"\"\n        return super().json(*args, **_mod_def_dump_args(kwargs))\n\n    def json_dict(self, **kwargs):\n\"\"\"Return a JSON-compatible dict.\n\n        Uses round-trip through JSON serialization.\n        \"\"\"\n        return json.loads(self.json(**kwargs))\n\n    def yaml(self, **kwargs) -&gt; str:\n\"\"\"Return serialized YAML as string.\"\"\"\n        # Current way: use round trip through JSON to kick out non-JSON entities\n        # (more elegant: allow ruamel yaml to reuse defined custom JSON dumpers)\n        # tmp = self.json_dict(**_mod_def_dump_args(kwargs))\n        return to_yaml_str(self)\n\n    @classmethod\n    def parse_file(cls, path: Union[str, Path]):\n        return parse_yaml_file_as(cls, path)\n\n    @classmethod\n    def parse_raw(cls, dat: Union[str, bytes], **kwargs):\n        try:\n            return super().parse_raw(dat, **kwargs)\n        except ValidationError:\n            return parse_yaml_raw_as(cls, dat)\n\n    def __bytes__(self) -&gt; bytes:\n\"\"\"Serialize to JSON and return UTF-8 encoded bytes to be written in a file.\"\"\"\n        # add a newline, as otherwise behaviour with text editors will be confusing\n        # (e.g. vim automatically adds a trailing newline that it hides)\n        # https://stackoverflow.com/questions/729692/why-should-text-files-end-with-a-newline\n        return (self.json() + \"\\n\").encode(encoding=\"utf-8\")\n\n    def __str__(self) -&gt; str:\n        return self.json(indent=2)\n</code></pre>"},{"location":"reference/metador_core/schema/base/#metador_core.schema.base.BaseModelPlus.dict","title":"dict","text":"<pre><code>dict(*args, **kwargs)\n</code></pre> <p>Return a dict.</p> <p>Nota that this will eliminate all pydantic models, but might still contain complex value types.</p> Source code in <code>src/metador_core/schema/base.py</code> <pre><code>def dict(self, *args, **kwargs):\n\"\"\"Return a dict.\n\n    Nota that this will eliminate all pydantic models,\n    but might still contain complex value types.\n    \"\"\"\n    return super().dict(*args, **_mod_def_dump_args(kwargs))\n</code></pre>"},{"location":"reference/metador_core/schema/base/#metador_core.schema.base.BaseModelPlus.json","title":"json","text":"<pre><code>json(*args, **kwargs) -&gt; str\n</code></pre> <p>Return serialized JSON as string.</p> Source code in <code>src/metador_core/schema/base.py</code> <pre><code>def json(self, *args, **kwargs) -&gt; str:\n\"\"\"Return serialized JSON as string.\"\"\"\n    return super().json(*args, **_mod_def_dump_args(kwargs))\n</code></pre>"},{"location":"reference/metador_core/schema/base/#metador_core.schema.base.BaseModelPlus.json_dict","title":"json_dict","text":"<pre><code>json_dict(**kwargs)\n</code></pre> <p>Return a JSON-compatible dict.</p> <p>Uses round-trip through JSON serialization.</p> Source code in <code>src/metador_core/schema/base.py</code> <pre><code>def json_dict(self, **kwargs):\n\"\"\"Return a JSON-compatible dict.\n\n    Uses round-trip through JSON serialization.\n    \"\"\"\n    return json.loads(self.json(**kwargs))\n</code></pre>"},{"location":"reference/metador_core/schema/base/#metador_core.schema.base.BaseModelPlus.yaml","title":"yaml","text":"<pre><code>yaml(**kwargs) -&gt; str\n</code></pre> <p>Return serialized YAML as string.</p> Source code in <code>src/metador_core/schema/base.py</code> <pre><code>def yaml(self, **kwargs) -&gt; str:\n\"\"\"Return serialized YAML as string.\"\"\"\n    # Current way: use round trip through JSON to kick out non-JSON entities\n    # (more elegant: allow ruamel yaml to reuse defined custom JSON dumpers)\n    # tmp = self.json_dict(**_mod_def_dump_args(kwargs))\n    return to_yaml_str(self)\n</code></pre>"},{"location":"reference/metador_core/schema/base/#metador_core.schema.base.BaseModelPlus.__bytes__","title":"__bytes__","text":"<pre><code>__bytes__() -&gt; bytes\n</code></pre> <p>Serialize to JSON and return UTF-8 encoded bytes to be written in a file.</p> Source code in <code>src/metador_core/schema/base.py</code> <pre><code>def __bytes__(self) -&gt; bytes:\n\"\"\"Serialize to JSON and return UTF-8 encoded bytes to be written in a file.\"\"\"\n    # add a newline, as otherwise behaviour with text editors will be confusing\n    # (e.g. vim automatically adds a trailing newline that it hides)\n    # https://stackoverflow.com/questions/729692/why-should-text-files-end-with-a-newline\n    return (self.json() + \"\\n\").encode(encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/metador_core/schema/core/","title":"core","text":"<p>Core Metadata schemas for Metador that are essential for the container API.</p>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.KEY_SCHEMA_PG","title":"KEY_SCHEMA_PG  <code>module-attribute</code>","text":"<pre><code>KEY_SCHEMA_PG = '$metador_plugin'\n</code></pre> <p>Key in JSON schema to put metador plugin name an version.</p>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.KEY_SCHEMA_CONSTFLDS","title":"KEY_SCHEMA_CONSTFLDS  <code>module-attribute</code>","text":"<pre><code>KEY_SCHEMA_CONSTFLDS = '$metador_constants'\n</code></pre> <p>Key in JSON schema to put metador 'constant fields'.</p>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.ALLOWED_SCHEMA_CONFIG_FIELDS","title":"ALLOWED_SCHEMA_CONFIG_FIELDS  <code>module-attribute</code>","text":"<pre><code>ALLOWED_SCHEMA_CONFIG_FIELDS = {\n    \"title\",\n    \"extra\",\n    \"allow_mutation\",\n}\n</code></pre> <p>Allowed pydantic Config fields to be overridden in schema models.</p>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.SchemaBase","title":"SchemaBase","text":"<p>             Bases: <code>BaseModelPlus</code></p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>class SchemaBase(BaseModelPlus):\n    __constants__: ClassVar[Dict[str, Any]]\n\"\"\"Constant model fields, usually added with a decorator, ignored on input.\"\"\"\n\n    __overrides__: ClassVar[Set[str]]\n\"\"\"Field names explicitly overriding inherited field type.\n\n    Those not listed (by @overrides decorator) must, if they are overridden,\n    be strict subtypes of the inherited type.\"\"\"\n\n    __types_checked__: ClassVar[bool]\n\"\"\"Helper flag used by check_overrides to avoid re-checking.\"\"\"\n\n    class Config:\n        @staticmethod\n        def schema_extra(schema: Dict[str, Any], model: Type[BaseModelPlus]) -&gt; None:\n            model = UndefVersion._unwrap(model) or model\n\n            # custom extra key to connect back to metador schema:\n            # TODO: does not always work?!\n            # if pgi := model.__dict__.get(\"Plugin\"):\n            #     schema[KEY_SCHEMA_PG] = pgi.ref().copy(exclude={\"group\"}).json_dict()\n\n            # enrich schema with descriptions retrieved from e.g. docstrings\n            if model is not MetadataSchema:\n                add_missing_field_descriptions(schema, model)\n\n            # special handling for \"constant fields\"\n            if model.__constants__:\n                schema[KEY_SCHEMA_CONSTFLDS] = {}\n                for cname, cval in model.__constants__.items():\n                    # list them (so they are not rejected even with additionalProperties=False)\n                    schema[\"properties\"][cname] = True\n                    # store the constant alongside the schema\n                    schema[KEY_SCHEMA_CONSTFLDS][cname] = cval\n\n            # do magic (TODO: fix/rewrite)\n            # finalize_schema_extra(schema, model, base_model=MetadataSchema)\n\n    # NOTE: custom JSON schema feature is broken\n    # @classmethod\n    # def schema(cls, *args, **kwargs):\n    #    \"\"\"Return customized JSONSchema for this model.\"\"\"\n    #    return schema_of(UndefVersion._unwrap(cls) or cls, *args, **kwargs)\n\n    @root_validator(pre=True)\n    def override_consts(cls, values):\n\"\"\"Override/add defined schema constants.\n\n        They must be present on dump, but are ignored on load.\n        \"\"\"\n        values.update(cls.__constants__)\n        return values\n</code></pre>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.SchemaBase.__constants__","title":"__constants__  <code>class-attribute</code>","text":"<pre><code>__constants__: Dict[str, Any]\n</code></pre> <p>Constant model fields, usually added with a decorator, ignored on input.</p>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.SchemaBase.__overrides__","title":"__overrides__  <code>class-attribute</code>","text":"<pre><code>__overrides__: Set[str]\n</code></pre> <p>Field names explicitly overriding inherited field type.</p> <p>Those not listed (by @overrides decorator) must, if they are overridden, be strict subtypes of the inherited type.</p>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.SchemaBase.__types_checked__","title":"__types_checked__  <code>class-attribute</code>","text":"<pre><code>__types_checked__: bool\n</code></pre> <p>Helper flag used by check_overrides to avoid re-checking.</p>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.SchemaBase.override_consts","title":"override_consts","text":"<pre><code>override_consts(values)\n</code></pre> <p>Override/add defined schema constants.</p> <p>They must be present on dump, but are ignored on load.</p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>@root_validator(pre=True)\ndef override_consts(cls, values):\n\"\"\"Override/add defined schema constants.\n\n    They must be present on dump, but are ignored on load.\n    \"\"\"\n    values.update(cls.__constants__)\n    return values\n</code></pre>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.SchemaMagic","title":"SchemaMagic","text":"<p>             Bases: <code>DynEncoderModelMetaclass</code></p> <p>Metaclass for doing some magic.</p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>class SchemaMagic(DynEncoderModelMetaclass):\n\"\"\"Metaclass for doing some magic.\"\"\"\n\n    def __new__(cls, name, bases, dct):\n        # enforce single inheritance\n        if len(bases) &gt; 1:\n            raise TypeError(\"A schema can only have one parent schema!\")\n        baseschema = bases[0]\n\n        # only allow inheriting from other schemas:\n        # NOTE: can't normally happen (this metaclass won't be triggered)\n        # if not issubclass(baseschema, SchemaBase):\n        #     raise TypeError(f\"Base class {baseschema} is not a MetadataSchema!\")\n\n        # prevent user from defining special schema fields by hand\n        for atr in SchemaBase.__annotations__.keys():\n            if atr in dct:\n                raise TypeError(f\"{name}: Invalid attribute '{atr}'\")\n\n        # prevent most changes to pydantic config\n        if conf := dct.get(\"Config\"):\n            for conffield in conf.__dict__:\n                if (\n                    is_public_name(conffield)\n                    and conffield not in ALLOWED_SCHEMA_CONFIG_FIELDS\n                ):\n                    raise TypeError(f\"{name}: {conffield} must not be set or changed!\")\n\n        # generate pydantic model of schema (further checks are easier that way)\n        # can't do these checks in __init__, because in __init__ the bases could be mangled\n        ret = super().__new__(cls, name, bases, dct)\n\n        # prevent user defining fields that are constants in a parent\n        if base_consts := set(getattr(baseschema, \"__constants__\", {}).keys()):\n            new_defs = set(get_annotations(ret).keys())\n            if illegal := new_defs.intersection(base_consts):\n                msg = (\n                    f\"{name}: Cannot define {illegal}, defined as const field already!\"\n                )\n                raise TypeError(msg)\n\n        # prevent parent-compat breaking change of extra handling / new fields:\n        parent_forbids_extras = baseschema.__config__.extra is Extra.forbid\n        if parent_forbids_extras:\n            # if parent forbids, child does not -&gt; problem (child can parse, parent can't)\n            extra = ret.__config__.extra\n            if extra is not Extra.forbid:\n                msg = (\n                    f\"{name}: cannot {extra.value} extra fields if parent forbids them!\"\n                )\n                raise TypeError(msg)\n\n            # parent forbids extras, child has new fields -&gt; same problem\n            if new_flds := set(ret.__fields__.keys()) - set(\n                baseschema.__fields__.keys()\n            ):\n                msg = f\"{name}: Cannot define new fields {new_flds} if parent forbids extra fields!\"\n                raise TypeError(msg)\n\n        # everything looks ok\n        return ret\n\n    def __init__(self, name, bases, dct):\n        self.__types_checked__ = False  # marker used by check_types (for performance)\n\n        # prevent implicit inheritance of class-specific internal/meta stuff:\n        # should be taken care of by plugin metaclass\n        assert self.Plugin is None or self.Plugin != bases[0].Plugin\n\n        # also prevent inheriting override marker\n        self.__overrides__ = set()\n\n        # and manually prevent inheriting annotations (for Python &lt; 3.10)\n        if \"__annotations__\" not in self.__dict__:\n            self.__annotations__ = {}\n\n        # \"constant fields\" are inherited, but copied - not shared\n        self.__constants__ = {}\n        for b in bases:\n            self.__constants__.update(getattr(b, \"__constants__\", {}))\n\n    @property  # type: ignore\n    @cache  # noqa: B019\n    def _typehints(self):\n\"\"\"Return typehints of this class.\"\"\"\n        return get_type_hints(self)\n\n    @property  # type: ignore\n    @cache  # noqa: B019\n    def _base_typehints(self):\n\"\"\"Return typehints accumulated from base class chain.\"\"\"\n        return ChainMap(\n            *(b._typehints for b in self.__bases__ if issubclass(b, SchemaBase))\n        )\n\n    # ---- for public use ----\n\n    def __str__(self):\n\"\"\"Show schema and field documentation.\"\"\"\n        unwrapped = UndefVersion._unwrap(self)\n        schema = unwrapped or self\n        defstr = f\"Schema {super().__str__()}\"\n        defstr = f\"{defstr}\\n{'='*len(defstr)}\"\n        descstr = \"\"\n        if schema.__doc__ is not None and schema.__doc__.strip():\n            desc = schema.__doc__\n            descstr = f\"\\nDescription:\\n------------\\n\\t{desc}\"\n        fieldsstr = f\"Fields:\\n-------\\n\\n{str(self.Fields)}\"\n        return \"\\n\".join([defstr, descstr, fieldsstr])\n\n    @property\n    def Fields(self: Any) -&gt; Any:\n\"\"\"Access the field introspection interface.\"\"\"\n        fields = make_schema_inspector(self)\n        # make sure that subschemas accessed in a schema without explicit version\n        # are also marked so we can check if someone uses them illegally\n        if issubclass(self, UndefVersion):\n            return WrappedLiftedDict(fields, UndefVersionFieldInspector)\n        else:\n            return fields\n\n    @property\n    def Partial(self):\n\"\"\"Access the partial schema based on the current schema.\"\"\"\n        return PartialSchemas.get_partial(self)\n</code></pre>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.SchemaMagic.Fields","title":"Fields  <code>property</code>","text":"<pre><code>Fields: Any\n</code></pre> <p>Access the field introspection interface.</p>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.SchemaMagic.Partial","title":"Partial  <code>property</code>","text":"<pre><code>Partial\n</code></pre> <p>Access the partial schema based on the current schema.</p>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.SchemaMagic.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Show schema and field documentation.</p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>def __str__(self):\n\"\"\"Show schema and field documentation.\"\"\"\n    unwrapped = UndefVersion._unwrap(self)\n    schema = unwrapped or self\n    defstr = f\"Schema {super().__str__()}\"\n    defstr = f\"{defstr}\\n{'='*len(defstr)}\"\n    descstr = \"\"\n    if schema.__doc__ is not None and schema.__doc__.strip():\n        desc = schema.__doc__\n        descstr = f\"\\nDescription:\\n------------\\n\\t{desc}\"\n    fieldsstr = f\"Fields:\\n-------\\n\\n{str(self.Fields)}\"\n    return \"\\n\".join([defstr, descstr, fieldsstr])\n</code></pre>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.SchemaMetaclass","title":"SchemaMetaclass","text":"<p>             Bases: <code>PluginMetaclassMixin</code>, <code>SchemaMagic</code></p> <p>Combine schema magic with general plugin magic.</p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>class SchemaMetaclass(PluginMetaclassMixin, SchemaMagic):\n\"\"\"Combine schema magic with general plugin magic.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.MetadataSchema","title":"MetadataSchema","text":"<p>             Bases: <code>SchemaBase</code></p> <p>Extends Pydantic models with custom serializers and functions.</p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>class MetadataSchema(SchemaBase, metaclass=SchemaMetaclass):\n\"\"\"Extends Pydantic models with custom serializers and functions.\"\"\"\n\n    Plugin: ClassVar[Optional[Type]]  # user-defined inner class (for schema plugins)\n</code></pre>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.SchemaFieldInspector","title":"SchemaFieldInspector","text":"<p>             Bases: <code>FieldInspector</code></p> <p>MetadataSchema-specific field inspector.</p> <p>It adds a user-friendly repr and access to nested subschemas.</p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>class SchemaFieldInspector(FieldInspector):\n\"\"\"MetadataSchema-specific field inspector.\n\n    It adds a user-friendly repr and access to nested subschemas.\n    \"\"\"\n\n    schemas: LiftedRODict\n    _origin_name: str\n\n    def _get_description(self):\n        fld = self.origin._typehints[self.name]\n        if any(map(lambda x: fld is x, (None, type(None), bool, int, float, str))):\n            return None\n        return super()._get_description()\n\n    def __init__(self, model: Type[MetadataSchema], name: str, hint: str):\n        super().__init__(model, name, hint)\n\n        # to show plugin name and version in case of registered plugin schemas:\n        og = self.origin\n        self._origin_name = f\"{og.__module__}.{og.__qualname__}\"\n        if pgi := og.Plugin:\n            self._origin_name += f\" (plugin: {pgi.name} {to_semver_str(pgi.version)})\"\n\n        # access to sub-entities/schemas:\n        subschemas = list(field_atomic_types(og.__fields__[name], bound=MetadataSchema))\n        self.schemas = lift_dict(\"Schemas\", {s.__name__: s for s in set(subschemas)})\n\n    def __repr__(self) -&gt; str:\n        desc_str = \"\"\n        if self.description:\n            desc_str = f\"description:\\n{_indent_text(self.description)}\\n\"\n        schemas_str = (\n            f\"schemas: {', '.join(iter(self.schemas))}\\n\" if self.schemas else \"\"\n        )\n        info = f\"type: {str(self.type)}\\norigin: {self._origin_name}\\n{schemas_str}{desc_str}\"\n        return f\"{self.name}\\n{_indent_text(info)}\"\n</code></pre>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.PartialSchemas","title":"PartialSchemas","text":"<p>             Bases: <code>PartialFactory</code></p> <p>Partial model for MetadataSchema model.</p> <p>Needed for harvesters to work (which can provide validated but partial metadata).</p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>class PartialSchemas(PartialFactory):\n\"\"\"Partial model for MetadataSchema model.\n\n    Needed for harvesters to work (which can provide validated but partial metadata).\n    \"\"\"\n\n    base_model = SchemaBase\n\n    # override to ignore \"constant fields\" for partials\n    @classmethod\n    def _get_field_vals(cls, obj):\n        return (\n            (k, v)\n            for k, v in super()._get_field_vals(obj)\n            if k not in obj.__constants__\n        )\n\n    # override to add some fixes to partials\n    @classmethod\n    def _create_partial(cls, mcls, *, typehints=...):\n        th = getattr(mcls, \"_typehints\", None)\n        unw = UndefVersion._unwrap(mcls) or mcls\n        ret, nested = super()._create_partial(unw, typehints=th)\n        # attach constant field list for field filtering\n        ret.__constants__ = getattr(mcls, \"__constants__\", set())\n        # copy custom parser to partial\n        # (otherwise partial can't parse correctly with parser mixin)\n        if parser := getattr(mcls, \"Parser\", None):\n            ret.Parser = parser\n        return (ret, nested)\n</code></pre>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.add_missing_field_descriptions","title":"add_missing_field_descriptions","text":"<pre><code>add_missing_field_descriptions(schema, model)\n</code></pre> <p>Add missing field descriptions from own Fields info, if possible.</p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>def add_missing_field_descriptions(schema, model):\n\"\"\"Add missing field descriptions from own Fields info, if possible.\"\"\"\n    for fname, fjsdef in schema.get(\"properties\", {}).items():\n        if not fjsdef.get(\"description\"):\n            with contextlib.suppress(KeyError):\n                if desc := model.Fields[fname].description:\n                    fjsdef[\"description\"] = desc\n</code></pre>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.check_allowed_types","title":"check_allowed_types","text":"<pre><code>check_allowed_types(schema: Type[MetadataSchema])\n</code></pre> <p>Check that shape of defined fields is suitable for deep merging.</p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>def check_allowed_types(schema: Type[MetadataSchema]):\n\"\"\"Check that shape of defined fields is suitable for deep merging.\"\"\"\n    hints = cast(Any, schema._typehints)\n    for field, hint in hints.items():\n        if not is_public_name(field):\n            continue  # private field\n        if not is_mergeable_type(hint):\n            msg = f\"{schema}:\\n\\ttype of '{field}' contains a forbidden pattern!\"\n            raise TypeError(msg)\n\n        # check that no nested schemas from undefVersion plugins are used in field definitions\n        # (the Plugin metaclass cannot check this, but it checks for inheritance)\n        if illegal := next(\n            filter(\n                is_subclass_of(UndefVersion),\n                filter(is_instance_of(type), traverse_typehint(hint)),\n            ),\n            None,\n        ):\n            msg = f\"{schema}:\\n\\ttype of '{field}' contains an illegal subschema:\\n\\t\\t{illegal}\"\n            raise TypeError(msg)\n</code></pre>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.infer_parent","title":"infer_parent","text":"<pre><code>infer_parent(\n    plugin: Type[MetadataSchema],\n) -&gt; Optional[Type[MetadataSchema]]\n</code></pre> <p>Return closest base schema that is a plugin, or None.</p> <p>This allows to skip over intermediate schemas and bases that are not plugins.</p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>def infer_parent(plugin: Type[MetadataSchema]) -&gt; Optional[Type[MetadataSchema]]:\n\"\"\"Return closest base schema that is a plugin, or None.\n\n    This allows to skip over intermediate schemas and bases that are not plugins.\n    \"\"\"\n    return next(\n        filter(\n            lambda c: issubclass(c, MetadataSchema) and c.__dict__.get(\"Plugin\"),\n            plugin.__mro__[1:],\n        ),\n        None,\n    )\n</code></pre>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.is_pub_instance_field","title":"is_pub_instance_field","text":"<pre><code>is_pub_instance_field(schema, name, hint)\n</code></pre> <p>Return whether field <code>name</code> in <code>schema</code> is a non-constant, public schema instance field.</p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>def is_pub_instance_field(schema, name, hint):\n\"\"\"Return whether field `name` in `schema` is a non-constant, public schema instance field.\"\"\"\n    return (\n        is_public_name(name)\n        and not is_classvar(hint)\n        and name not in schema.__constants__\n    )\n</code></pre>"},{"location":"reference/metador_core/schema/core/#metador_core.schema.core.check_overrides","title":"check_overrides","text":"<pre><code>check_overrides(schema: Type[MetadataSchema])\n</code></pre> <p>Check that fields are overridden to subtypes or explicitly declared as overridden.</p> Source code in <code>src/metador_core/schema/core.py</code> <pre><code>def check_overrides(schema: Type[MetadataSchema]):\n\"\"\"Check that fields are overridden to subtypes or explicitly declared as overridden.\"\"\"\n    hints = cast(Any, schema._typehints)\n    base_hints = cast(Any, schema._base_typehints)\n\n    actual_overrides = detect_field_overrides(schema)\n    undecl_override = actual_overrides - schema.__overrides__\n    if unreal_override := schema.__overrides__ - set(base_hints.keys()):\n        msg = f\"{schema.__name__}: No parent field to override: {unreal_override}\"\n        raise ValueError(msg)\n\n    if miss_override := schema.__overrides__ - actual_overrides:\n        msg = f\"{schema.__name__}: Missing claimed field overrides: {miss_override}\"\n        raise ValueError(msg)\n\n    # all undeclared overrides must be strict subtypes of the inherited type:\n    for fname in undecl_override:\n        hint, parent_hint = hints[fname], base_hints[fname]\n        if not is_subtype(hint, parent_hint):\n            parent = infer_parent(schema)\n            parent_name = (\n                parent.Fields[fname]._origin_name\n                if parent\n                else schema.__base__.__name__\n            )\n            msg = f\"\"\"The type assigned to field '{fname}'\nin schema {repr(schema)}:\n\n{hint}\n\ndoes not look like a valid subtype of the inherited type:\n\n{parent_hint}\n\nfrom schema {parent_name}.\n\nIf you are ABSOLUTELY sure that this is a false alarm,\nuse the @overrides decorator to silence this error\nand live forever with the burden of responsibility.\n\"\"\"\n            raise TypeError(msg)\n</code></pre>"},{"location":"reference/metador_core/schema/decorators/","title":"decorators","text":""},{"location":"reference/metador_core/schema/decorators/#metador_core.schema.decorators.make_mandatory","title":"make_mandatory","text":"<pre><code>make_mandatory(*names: str)\n</code></pre> <p>Make a field inherited from a base class mandatory if it is optional.</p> <p>The field must exist in a base class and must not be defined in the decorated class.</p> <p>Use this decorator instead of manually declaring an annotation, if all you need to do is making an existing field mandatory.</p> Source code in <code>src/metador_core/schema/decorators.py</code> <pre><code>def make_mandatory(*names: str):\n\"\"\"Make a field inherited from a base class mandatory if it is optional.\n\n    The field must exist in a base class and must not be defined in the\n    decorated class.\n\n    Use this decorator instead of manually declaring an annotation,\n    if all you need to do is making an existing field mandatory.\n    \"\"\"\n    # NOTE: idea: could take a dict, then values are the new default for non-optional\n    # but is this good? defaults are implicit optionality -&gt; we discourage it, so no.\n    _check_names_public(names)\n\n    def make_fields_mandatory(mcls):\n        _expect_schema_class(mcls)\n\n        for name in names:\n            if name not in mcls.__fields__:\n                raise ValueError(f\"{mcls.__name__} has no field named '{name}'!\")\n            if name in get_annotations(mcls):\n                raise ValueError(\n                    f\"{mcls.__name__} already defines '{name}', cannot use decorator!\"\n                )\n\n            hint = unoptional(field_parent_type(mcls, name))\n            # update model and type hint (important for type analysis)\n            mcls.__fields__[name].required = True\n            mcls.__annotations__[name] = hint\n\n        return mcls\n\n    return make_fields_mandatory\n</code></pre>"},{"location":"reference/metador_core/schema/decorators/#metador_core.schema.decorators.add_const_fields","title":"add_const_fields","text":"<pre><code>add_const_fields(\n    consts: Dict[str, Any], *, override: bool = False\n)\n</code></pre> <p>Add constant fields to pydantic models.</p> <p>Must be passed a dict of field names and the constant values (only JSON-like types).</p> <p>Constant fields are optional during input. If present during parsing, they are be ignored and overriden with the constant. Constant fields are included in serialization, unless <code>exclude_defaults</code> is set.</p> <p>This can be used e.g. to attach JSON-LD annotations to schemas.</p> <p>Constant fields are inherited and may only be overridden by other constant fields using this decorator, they cannot become normal fields again.</p> Source code in <code>src/metador_core/schema/decorators.py</code> <pre><code>def add_const_fields(consts: Dict[str, Any], *, override: bool = False):\n\"\"\"Add constant fields to pydantic models.\n\n    Must be passed a dict of field names and the constant values (only JSON-like types).\n\n    Constant fields are optional during input.\n    If present during parsing, they are be ignored and overriden with the constant.\n    Constant fields are included in serialization, unless `exclude_defaults` is set.\n\n    This can be used e.g. to attach JSON-LD annotations to schemas.\n\n    Constant fields are inherited and may only be overridden by other constant fields\n    using this decorator, they cannot become normal fields again.\n    \"\"\"\n    _check_names_public(consts.keys())\n\n    def add_fields(mcls):\n        _expect_schema_class(mcls)\n\n        # hacking it in-place approach:\n        overridden = set()\n        for name, value in consts.items():\n            if field_def := mcls.__fields__.get(name):  # overriding a field\n                # we allow to silently override of enum/literal types with suitable values\n                # to support a schema design pattern of marked subclasses\n                # but check that it is actually used correctly.\n                enum_specialization = is_enum(field_def.type_)\n                literal_specialization = is_literal(field_def.type_)\n\n                valid_specialization = False\n                if enum_specialization:\n                    valid_specialization = isinstance(value, field_def.type_)\n                elif literal_specialization:\n                    lit_const = Literal[value]  # type: ignore\n                    valid_specialization = is_subtype(lit_const, field_def.type_)\n\n                if (\n                    enum_specialization or literal_specialization\n                ) and not valid_specialization:\n                    msg = f\"{mcls.__name__}.{name} cannot be overriden with '{value}', \"\n                    msg += f\"because it is not a valid value of {field_def.type_}!\"\n                    raise TypeError(msg)\n\n                # reject if not force override or allowed special cases\n                if not (override or enum_specialization or literal_specialization):\n                    msg = f\"{mcls.__name__} already has a field '{name}'!\"\n                    msg += f\" (override={override})\"\n                    raise ValueError(msg)\n\n                else:  # new field\n                    overridden.add(name)\n\n            # this would force the exact constant on load\n            # but this breaks parent compatibility if consts overridden!\n            # ----\n            # val = value.default if isinstance(value, FieldInfo) else value\n            # ctype = Optional[make_literal(val)]  # type: ignore\n            # ----\n            # we simply ignore the constants as opaque somethings\n            ctype = Optional[Any]  # type: ignore\n\n            # configure pydantic field\n            field = ModelField.infer(\n                name=name,\n                value=value,\n                annotation=ctype,\n                class_validators=None,\n                config=mcls.__config__,\n            )\n            mcls.__fields__[name] = field\n            # add type hint (important for our field analysis!)\n            mcls.__annotations__[name] = field.type_\n        ret = mcls\n\n        # dynamic subclass approach:\n        # ret = create_model(mcls.__name__, __base__=mcls, __module__=mcls.__module__, **consts)\n        # if hasattr(mcls, \"Plugin\"):\n        #     ret.Plugin = mcls.Plugin\n\n        # to later distinguish \"const\" fields from normal fields:\n        ret.__constants__.update(consts)\n        return ret\n\n    return add_fields\n</code></pre>"},{"location":"reference/metador_core/schema/decorators/#metador_core.schema.decorators.override","title":"override","text":"<pre><code>override(*names: str)\n</code></pre> <p>Declare fields that are overridden (and not valid as subtypes).</p> <p>These are checked during plugin loading, in order to catch accidental overridden fields in schemas.</p> Source code in <code>src/metador_core/schema/decorators.py</code> <pre><code>def override(*names: str):\n\"\"\"Declare fields that are overridden (and not valid as subtypes).\n\n    These are checked during plugin loading, in order to catch accidental\n    overridden fields in schemas.\n    \"\"\"\n    _check_names_public(names)\n\n    def add_overrides(mcls):\n        _expect_schema_class(mcls)\n        mcls.__overrides__.update(set(names))\n        return mcls\n\n    return add_overrides\n</code></pre>"},{"location":"reference/metador_core/schema/encoder/","title":"encoder","text":"<p>Support for dynamically registered JSON encoders in pydantic.</p> <p>Preparation:</p> <p>For your top level BaseModel, set <code>DynEncoderModelMeta</code> as metaclass, e.g.</p> <pre><code>class MyBaseModel(BaseModel, metaclass=DynEncoderModelMeta):\n    ...\n</code></pre> <p>(If you already use a custom metaclass for your base model, add the <code>DynJsonEncoder</code> metaclass mixin)</p> <p>Usage:</p> <p>Decorate any class with <code>@json_encoder(ENCODER_FUNCTION)</code>.</p> <p>To add an encoder for some existing class you cannot decorate, use <code>add_json_encoder(ClassName, func)</code>.</p> <p>Note that <code>json_encoders</code> declared as intended by Pydantic in the <code>Config</code> section will always be prioritized over the dynamic encoder. This means, that the dynamic encoders are only triggered for classes that are not models themselves (because pydantic handles them already).</p> <p>Also note that to prevent bugs, you cannot override encoders for a class that already has a registered dynamic encoder. Use the normal pydantic mechanisms for cases where this is really needed.</p> <p>Ideally, design your classes in a way that there is a 1-to-1 relationship between class and desired JSON encoder (e.g. you can have different subclasses with different encoders).</p>"},{"location":"reference/metador_core/schema/encoder/#metador_core.schema.encoder.DynJsonEncoderMetaMixin","title":"DynJsonEncoderMetaMixin","text":"<p>             Bases: <code>type</code></p> <p>Metaclass mixin to first look in dynamic encoder registry.</p> <p>Combine this with (a subclass of) <code>ModelMetaClass</code> and use it for your custom base model.</p> Source code in <code>src/metador_core/schema/encoder.py</code> <pre><code>class DynJsonEncoderMetaMixin(type):\n\"\"\"Metaclass mixin to first look in dynamic encoder registry.\n\n    Combine this with (a subclass of) `ModelMetaClass` and use it for your custom base model.\n    \"\"\"\n\n    def __init__(self, name, bases, dct):\n        super().__init__(name, bases, dct)\n        self.__json_encoder__ = staticmethod(_dynamize_encoder(self.__json_encoder__))\n</code></pre>"},{"location":"reference/metador_core/schema/encoder/#metador_core.schema.encoder.DynEncoderModelMetaclass","title":"DynEncoderModelMetaclass","text":"<p>             Bases: <code>DynJsonEncoderMetaMixin</code>, <code>ModelMetaclass</code></p> <p>Set this metaclass for your custom base model to enable dynamic encoders.</p> Source code in <code>src/metador_core/schema/encoder.py</code> <pre><code>class DynEncoderModelMetaclass(DynJsonEncoderMetaMixin, ModelMetaclass):\n\"\"\"Set this metaclass for your custom base model to enable dynamic encoders.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/encoder/#metador_core.schema.encoder.json_encoder","title":"json_encoder","text":"<pre><code>json_encoder(func)\n</code></pre> <p>Decorate a class to register a new JSON encoder for it.</p> Source code in <code>src/metador_core/schema/encoder.py</code> <pre><code>def json_encoder(func):\n\"\"\"Decorate a class to register a new JSON encoder for it.\"\"\"\n\n    def reg_encoder(cls):\n        if issubclass(cls.__class__, ModelMetaclass):\n            raise TypeError(\"This decorator does not work for pydantic models!\")\n        if hasattr(cls, \"__dataclass_fields__\"):\n            raise TypeError(\"This decorator does not work for dataclasses!\")\n\n        if cls in _reg_json_encoders:\n            raise ValueError(f\"A JSON encoder function for {cls} already exists!\")\n\n        _reg_json_encoders[cls] = func\n        return cls\n\n    return reg_encoder\n</code></pre>"},{"location":"reference/metador_core/schema/encoder/#metador_core.schema.encoder.add_json_encoder","title":"add_json_encoder","text":"<pre><code>add_json_encoder(cls, func)\n</code></pre> <p>Register a JSON encoder function for a class.</p> Source code in <code>src/metador_core/schema/encoder.py</code> <pre><code>def add_json_encoder(cls, func):\n\"\"\"Register a JSON encoder function for a class.\"\"\"\n    return json_encoder(func)(cls)\n</code></pre>"},{"location":"reference/metador_core/schema/inspect/","title":"inspect","text":""},{"location":"reference/metador_core/schema/inspect/#metador_core.schema.inspect.LiftedRODict","title":"LiftedRODict","text":"<p>             Bases: <code>type</code></p> <p>Metaclass for classes providing dict keys as attributes.</p> <p>Mostly for aesthetic reasons and to be used for things where the dict is actually a fixed lookup table.</p> <p>We don't provide explicit <code>keys</code>/<code>values</code>/<code>items</code>, because these could be key names in the dict.</p> <p>You can use <code>iter</code> to go through the keys and use dict-like access, if dynamic iteration is needed.</p> Source code in <code>src/metador_core/schema/inspect.py</code> <pre><code>class LiftedRODict(type):\n\"\"\"Metaclass for classes providing dict keys as attributes.\n\n    Mostly for aesthetic reasons and to be used for things\n    where the dict is actually a fixed lookup table.\n\n    We don't provide explicit `keys`/`values`/`items`, because\n    these could be key names in the dict.\n\n    You can use `iter` to go through the keys and use dict-like\n    access, if dynamic iteration is needed.\n    \"\"\"\n\n    # NOTE: we don't want to add non-default methods\n    # because method names collide with dict keys\n\n    _dict: Mapping[str, Any]\n\"\"\"The underlying dict.\"\"\"\n\n    _keys: Optional[List[str]] = None\n\"\"\"Optionally, list of keys in desired order.\"\"\"\n\n    _repr: Optional[Callable] = None\n\"\"\"Optional custom repr string or function.\"\"\"\n\n    def __repr__(self):\n        # choose best representation based on configuration\n        if self._repr:\n            return self._repr(self)\n        if self._keys:\n            return repr(self._keys)\n        return repr(list(self._dict.keys()))\n\n    def __dir__(self):\n        # helpful for tab completion\n        return list(self._dict.keys())\n\n    def __bool__(self):\n        return bool(self._dict)\n\n    def __contains__(self, key):\n        return key in self._dict\n\n    def __iter__(self):\n        if self._keys:\n            return iter(self._keys)\n        return iter(self._dict)\n\n    def __getitem__(self, key):\n        return self._dict[key]\n\n    def __getattr__(self, key):\n        try:\n            return self[key]\n        except KeyError as e:\n            raise AttributeError(str(e))\n\n    def __setattr__(self, key, value):\n        # this is supposed to be read-only\n        raise UnsupportedOperation\n</code></pre>"},{"location":"reference/metador_core/schema/inspect/#metador_core.schema.inspect.WrappedLiftedDict","title":"WrappedLiftedDict","text":"<p>             Bases: <code>ObjectProxy</code></p> <p>Wrap values returned by a LiftedRODict.</p> Source code in <code>src/metador_core/schema/inspect.py</code> <pre><code>class WrappedLiftedDict(wrapt.ObjectProxy):\n\"\"\"Wrap values returned by a LiftedRODict.\"\"\"\n\n    def __init__(self, obj, wrapperfun):\n        assert isinstance(obj, LiftedRODict)\n        super().__init__(obj)\n        self._self_wrapperfun = wrapperfun\n\n    def __getitem__(self, key):\n        return self._self_wrapperfun(self.__wrapped__[key])\n\n    def __getattr__(self, key):\n        return LiftedRODict.__getattr__(self, key)\n\n    def __repr__(self):\n        return repr(self.__wrapped__)\n</code></pre>"},{"location":"reference/metador_core/schema/inspect/#metador_core.schema.inspect.FieldInspector","title":"FieldInspector  <code>dataclass</code>","text":"<p>Basic field inspector carrying type and description of a field.</p> Source code in <code>src/metador_core/schema/inspect.py</code> <pre><code>@dataclass\nclass FieldInspector:\n\"\"\"Basic field inspector carrying type and description of a field.\"\"\"\n\n    origin: Type\n    name: str\n    type: str\n\n    description: str  # declared for proper repr generation\n\n    def _get_description(self):\n        desc = get_attribute_docstring(self.origin, self.name).docstring_below\n        if not desc:\n            # if none set, try getting docstring from field type\n            if ths := getattr(self.origin, \"_typehints\", None):\n                th = ths[self.name]\n                if isinstance(th, type):  # it's a class-like thing?\n                    desc = th.__doc__\n\n        return desc\n\n    @property  # type: ignore\n    def description(self):\n        # look up on-demand and cache, could be expensive (parses source)\n        if not hasattr(self, \"_description\"):\n            self._description = self._get_description()\n        return self._description\n\n    def __init__(self, model: Type[BaseModel], name: str, hint: str):\n        origin = next(field_origins(model, name))\n        self.origin = origin\n        self.name = name\n        self.type = hint\n</code></pre>"},{"location":"reference/metador_core/schema/inspect/#metador_core.schema.inspect.lift_dict","title":"lift_dict","text":"<pre><code>lift_dict(name, dct, *, keys = None, repr = None)\n</code></pre> <p>Return LiftedRODict class based on passed dict.</p> Source code in <code>src/metador_core/schema/inspect.py</code> <pre><code>def lift_dict(name, dct, *, keys=None, repr=None):\n\"\"\"Return LiftedRODict class based on passed dict.\"\"\"\n    assert hasattr(dct, \"__getitem__\")\n    kwargs = {\"_dict\": dct}\n    if keys is not None:\n        assert set(keys) == set(iter(dct))\n        kwargs[\"_keys\"] = keys\n    if repr is not None:\n        kwargs[\"_repr\"] = repr\n    return LiftedRODict(name, (), kwargs)\n</code></pre>"},{"location":"reference/metador_core/schema/inspect/#metador_core.schema.inspect.make_field_inspector","title":"make_field_inspector","text":"<pre><code>make_field_inspector(\n    model: Type[BaseModel],\n    prop_name: str,\n    *,\n    bound: Optional[Type[BaseModel]] = BaseModel,\n    key_filter: Optional[Callable[[str], bool]],\n    i_cls: Optional[Type[FieldInspector]] = FieldInspector\n) -&gt; Type[LiftedRODict]\n</code></pre> <p>Create a field inspector class for the given model.</p> <p>This can be used for introspection about fields and also enables users to access subschemas without extra imports, improving decoupling of plugins and packages.</p> <p>To be used in a metaclass for a custom top level model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[BaseModel]</code> <p>Class for which to return the inspector</p> required <code>prop_name</code> <code>str</code> <p>Name of the metaclass property that wraps this function</p> required <code>i_cls</code> <code>Optional[Type[FieldInspector]]</code> <p>Optional subclass of FieldInspector to customize it</p> <code>FieldInspector</code> <code>bound</code> <code>Optional[Type[BaseModel]]</code> <p>Top level class using the custom metaclass that uses this function</p> <code>BaseModel</code> <code>key_filter</code> <code>Optional[Callable[[str], bool]]</code> <p>Predicate used to filter the annotations that are to be inspectable</p> required <p>Returns:     A fresh inspector class for the fields.</p> Source code in <code>src/metador_core/schema/inspect.py</code> <pre><code>def make_field_inspector(\n    model: Type[BaseModel],\n    prop_name: str,\n    *,\n    bound: Optional[Type[BaseModel]] = BaseModel,\n    key_filter: Optional[Callable[[str], bool]],\n    i_cls: Optional[Type[FieldInspector]] = FieldInspector,\n) -&gt; Type[LiftedRODict]:\n\"\"\"Create a field inspector class for the given model.\n\n    This can be used for introspection about fields and also\n    enables users to access subschemas without extra imports,\n    improving decoupling of plugins and packages.\n\n    To be used in a metaclass for a custom top level model.\n\n    Args:\n        model: Class for which to return the inspector\n        prop_name: Name of the metaclass property that wraps this function\n        i_cls: Optional subclass of FieldInspector to customize it\n        bound: Top level class using the custom metaclass that uses this function\n        key_filter: Predicate used to filter the annotations that are to be inspectable\n    Returns:\n        A fresh inspector class for the fields.\n    \"\"\"\n    # get hints corresponding to fields that are not inherited\n    field_hints = {\n        k: v\n        for k, v in get_annotations(model).items()\n        if not key_filter or key_filter(k)\n    }\n\n    # inspectors for fields declared in the given model (for inherited, will reuse/create parent inspectors)\n    new_inspectors = {k: i_cls(model, k, v) for k, v in field_hints.items()}\n    # manually compute desired traversal order (from newest overwritten to oldest inherited fields)\n    # as the default chain map order semantically is not suitable.\n    inspectors = [new_inspectors] + [\n        getattr(b, prop_name)._dict for b in model.__bases__ if issubclass(b, bound)\n    ]\n    covered_keys: Set[str] = set()\n    ordered_keys: List[str] = []\n    for d in inspectors:\n        rem_keys = set(iter(d)) - covered_keys\n        covered_keys.update(rem_keys)\n        ordered_keys += [k for k in d if k in rem_keys]\n\n    # construct and return the class\n    return lift_dict(\n        f\"{model.__name__}.{prop_name}\",\n        ChainMap(*inspectors),\n        keys=ordered_keys,\n        repr=lambda self: \"\\n\".join(map(str, (self[k] for k in self))),\n    )\n</code></pre>"},{"location":"reference/metador_core/schema/jsonschema/","title":"jsonschema","text":"<p>Hacks to improve pydantic JSON Schema generation.</p>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.KEY_SCHEMA_DEFS","title":"KEY_SCHEMA_DEFS  <code>module-attribute</code>","text":"<pre><code>KEY_SCHEMA_DEFS = '$defs'\n</code></pre> <p>JSON schema key to store subschema definitions.</p>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.KEY_SCHEMA_HASH","title":"KEY_SCHEMA_HASH  <code>module-attribute</code>","text":"<pre><code>KEY_SCHEMA_HASH = '$jsonschema_hash'\n</code></pre> <p>Custom key to store schema hashsum.</p>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.JSONSCHEMA_STRIP","title":"JSONSCHEMA_STRIP  <code>module-attribute</code>","text":"<pre><code>JSONSCHEMA_STRIP = {\n    \"title\",\n    \"description\",\n    \"examples\",\n    \"$comment\",\n    \"readOnly\",\n    \"writeOnly\",\n    \"deprecated\",\n    \"$id\",\n    \"definitions\",\n    KEY_SCHEMA_DEFS,\n    KEY_SCHEMA_HASH,\n}\n</code></pre> <p>Fields to be removed for JSON Schema hashsum computation.</p>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.KEY_PYD_DEFS","title":"KEY_PYD_DEFS  <code>module-attribute</code>","text":"<pre><code>KEY_PYD_DEFS = 'definitions'\n</code></pre> <p>key name where pydantic stores subschema definitions.</p>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.REF_PREFIX","title":"REF_PREFIX  <code>module-attribute</code>","text":"<pre><code>REF_PREFIX = f'#/{KEY_PYD_DEFS}/'\n</code></pre> <p>default $refs prefix of pydantic.</p>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.jsonschema_id","title":"jsonschema_id","text":"<pre><code>jsonschema_id(schema: JSONType)\n</code></pre> <p>Compute robust semantic schema identifier.</p> <p>A schema identifier is based on the schema plugin name + version and its JSON Schema representation, which includes all parent and nested schemas.</p> Source code in <code>src/metador_core/schema/jsonschema.py</code> <pre><code>def jsonschema_id(schema: JSONType):\n\"\"\"Compute robust semantic schema identifier.\n\n    A schema identifier is based on the schema plugin name + version\n    and its JSON Schema representation, which includes all parent and nested schemas.\n    \"\"\"\n    return hashsum(normalized_json(clean_jsonschema(schema)), \"sha256\")\n</code></pre>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.lift_nested_defs","title":"lift_nested_defs","text":"<pre><code>lift_nested_defs(schema: JSONObject)\n</code></pre> <p>Flatten nested $defs ($defs -&gt; key -&gt; $defs) in-place.</p> Source code in <code>src/metador_core/schema/jsonschema.py</code> <pre><code>def lift_nested_defs(schema: JSONObject):\n\"\"\"Flatten nested $defs ($defs -&gt; key -&gt; $defs) in-place.\"\"\"\n    if mydefs := schema.get(KEY_SCHEMA_DEFS):\n        inner = []\n        for schema in mydefs.values():\n            lift_nested_defs(schema)\n            if nested := schema.pop(KEY_SCHEMA_DEFS, None):\n                inner.append(nested)\n        for nested in inner:\n            mydefs.update(nested)\n</code></pre>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.merge_nested_defs","title":"merge_nested_defs","text":"<pre><code>merge_nested_defs(schema: JSONObject)\n</code></pre> <p>Merge definitions in-place.</p> Source code in <code>src/metador_core/schema/jsonschema.py</code> <pre><code>def merge_nested_defs(schema: JSONObject):\n\"\"\"Merge definitions in-place.\"\"\"\n    if defs := schema.pop(KEY_PYD_DEFS, None):\n        my_defs = schema.get(KEY_SCHEMA_DEFS)\n        if not my_defs:\n            schema[KEY_SCHEMA_DEFS] = {}\n            my_defs = schema[KEY_SCHEMA_DEFS]\n        # update, by preserve existing\n        defs.update(my_defs)\n        my_defs.update(defs)\n</code></pre>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.collect_defmap","title":"collect_defmap","text":"<pre><code>collect_defmap(defs: JSONObject)\n</code></pre> <p>Compute dict mapping current name in $defs to new name based on metador_hash.</p> Source code in <code>src/metador_core/schema/jsonschema.py</code> <pre><code>def collect_defmap(defs: JSONObject):\n\"\"\"Compute dict mapping current name in $defs to new name based on metador_hash.\"\"\"\n    defmap = {}\n    for name, subschema in defs.items():\n        if KEY_SCHEMA_HASH in subschema:\n            defmap[name] = subschema[KEY_SCHEMA_HASH].strip(\"/\")\n        else:\n            # print(\"no hashsum: \", name)\n            defmap[name] = name\n\n    return defmap\n</code></pre>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.map_ref","title":"map_ref","text":"<pre><code>map_ref(defmap, refstr: str)\n</code></pre> <p>Update the <code>$ref</code> string based on defmap.</p> <p>Will replace <code>#/definitions/orig</code> with <code>#/$defs/mapped</code>.</p> Source code in <code>src/metador_core/schema/jsonschema.py</code> <pre><code>def map_ref(defmap, refstr: str):\n\"\"\"Update the `$ref` string based on defmap.\n\n    Will replace `#/definitions/orig`\n    with `#/$defs/mapped`.\n    \"\"\"\n    if refstr.startswith(REF_PREFIX):\n        # print(\"remap\", refstr)\n        plen = len(REF_PREFIX)\n        if new_name := defmap.get(refstr[plen:]):\n            return f\"#/{KEY_SCHEMA_DEFS}/{new_name}\"\n    return refstr\n</code></pre>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.update_refs","title":"update_refs","text":"<pre><code>update_refs(defmap, obj)\n</code></pre> <p>Recursively update <code>$ref</code> in <code>obj</code> based on defmap.</p> Source code in <code>src/metador_core/schema/jsonschema.py</code> <pre><code>def update_refs(defmap, obj):\n\"\"\"Recursively update `$ref` in `obj` based on defmap.\"\"\"\n    print(\"update\", obj)\n    if isinstance(obj, (type(None), bool, int, float, str)):\n        return obj\n    elif isinstance(obj, list):\n        return list(map(partial(update_refs, defmap), obj))\n    elif isinstance(obj, dict):\n        return {\n            k: (update_refs(defmap, v) if k != \"$ref\" else map_ref(defmap, v))\n            for k, v in obj.items()\n        }\n    raise ValueError(f\"Object {obj} not of a JSON type: {type(obj)}\")\n</code></pre>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.remap_refs","title":"remap_refs","text":"<pre><code>remap_refs(schema)\n</code></pre> <p>Remap the $refs to use metador_hash-based keys.</p> <p>Input must be a completed schema with a global <code>$defs</code> section that all nested entities use for local references.</p> Source code in <code>src/metador_core/schema/jsonschema.py</code> <pre><code>def remap_refs(schema):\n\"\"\"Remap the $refs to use metador_hash-based keys.\n\n    Input must be a completed schema with a global `$defs` section\n    that all nested entities use for local references.\n    \"\"\"\n    defs = schema.pop(KEY_SCHEMA_DEFS, None)\n    if not defs:  # nothing to do\n        return schema\n\n    # get name map, old -&gt; new\n    defmap = collect_defmap(defs)\n    # update refs\n    defs.update(update_refs(defmap, defs))\n    schema.update(update_refs(defmap, schema))\n    # rename defs\n    schema[KEY_SCHEMA_DEFS] = {defmap[k]: v for k, v in defs.items()}\n</code></pre>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.schema_of","title":"schema_of","text":"<pre><code>schema_of(\n    model: Type[BaseModel],\n    *args: Type[BaseModel],\n    **kwargs: Type[BaseModel]\n)\n</code></pre> <p>Return JSON Schema for a model.</p> <p>Improved version of <code>pydantic.schema_of</code>, returns result in $defs normal form, with $ref pointing to the model.</p> Source code in <code>src/metador_core/schema/jsonschema.py</code> <pre><code>def schema_of(model: Type[BaseModel], *args, **kwargs):\n\"\"\"Return JSON Schema for a model.\n\n    Improved version of `pydantic.schema_of`, returns result\n    in $defs normal form, with $ref pointing to the model.\n    \"\"\"\n    schema = pyd_schema_of(model, *args, **kwargs)\n    print(type(schema), schema)\n    schema.pop(\"title\", None)\n    fixup_jsonschema(schema)\n    return schema\n</code></pre>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.schemas","title":"schemas","text":"<pre><code>schemas(\n    models: Iterable[Type[BaseModel]],\n    *args: Iterable[Type[BaseModel]],\n    **kwargs: Iterable[Type[BaseModel]]\n)\n</code></pre> <p>Return JSON Schema for multiple models.</p> <p>Improved version of <code>pydantic.schema.schema</code>, returns result in $defs normal form.</p> Source code in <code>src/metador_core/schema/jsonschema.py</code> <pre><code>def schemas(models: Iterable[Type[BaseModel]], *args, **kwargs):\n\"\"\"Return JSON Schema for multiple models.\n\n    Improved version of `pydantic.schema.schema`,\n    returns result in $defs normal form.\n    \"\"\"\n    schema = pyd_schemas(tuple(models), *args, **kwargs)\n    fixup_jsonschema(schema)\n    return schema\n</code></pre>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.split_model_inheritance","title":"split_model_inheritance","text":"<pre><code>split_model_inheritance(\n    schema: JSONObject, model: Type[BaseModel]\n)\n</code></pre> <p>Decompose a model into an allOf combination with a parent model.</p> <p>This is ugly because pydantic does in-place wrangling and caching, and we need to hack around it.</p> Source code in <code>src/metador_core/schema/jsonschema.py</code> <pre><code>def split_model_inheritance(schema: JSONObject, model: Type[BaseModel]):\n\"\"\"Decompose a model into an allOf combination with a parent model.\n\n    This is ugly because pydantic does in-place wrangling and caching,\n    and we need to hack around it.\n    \"\"\"\n    # NOTE: important - we assume to get the $defs standard form\n    # print(\"want schema of\", model.__base__.__name__)\n    base_schema = model.__base__.schema()  # type: ignore\n\n    # compute filtered properties / required section\n    schema_new = dict(schema)\n    ps = schema_new.pop(\"properties\", None)\n    rq = schema_new.pop(\"required\", None)\n\n    lst_fields = updated_fields(model)\n    ps_new = {k: v for k, v in ps.items() if k in lst_fields}\n    rq_new = None if not rq else [k for k in rq if k in ps_new]\n    schema_this = {k: v for k, v in [(\"properties\", ps_new), (\"required\", rq_new)] if v}\n\n    # construct new schema as combination of base schema and remainder schema\n    schema_new.update(\n        {\n            # \"rdfs:subClassOf\": f\"/{base_id}\",\n            \"allOf\": [{\"$ref\": base_schema[\"$ref\"]}, schema_this],\n        }\n    )\n\n    # we need to add the definitions to/from the base schema as well\n    if KEY_SCHEMA_DEFS not in schema_new:\n        schema_new[KEY_SCHEMA_DEFS] = {}\n    schema_new[KEY_SCHEMA_DEFS].update(base_schema.get(KEY_SCHEMA_DEFS, {}))\n\n    schema.clear()\n    schema.update(schema_new)\n</code></pre>"},{"location":"reference/metador_core/schema/jsonschema/#metador_core.schema.jsonschema.finalize_schema_extra","title":"finalize_schema_extra","text":"<pre><code>finalize_schema_extra(\n    schema: JSONObject,\n    model: Type[BaseModel],\n    *,\n    base_model: Type[BaseModel] = None\n) -&gt; None\n</code></pre> <p>Perform custom JSON Schema postprocessing.</p> <p>To be called as last action in custom schema_extra method in the used base model.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>JSONObject</code> <p>The JSON object containing the schema</p> required <code>model</code> <code>Type[BaseModel]</code> <p>The underlying pydantic model</p> required <code>base_model</code> <code>Type[BaseModel]</code> <p>The custom base model that this function is called for.</p> <code>None</code> Source code in <code>src/metador_core/schema/jsonschema.py</code> <pre><code>def finalize_schema_extra(\n    schema: JSONObject,\n    model: Type[BaseModel],\n    *,\n    base_model: Type[BaseModel] = None,\n) -&gt; None:\n\"\"\"Perform custom JSON Schema postprocessing.\n\n    To be called as last action in custom schema_extra method in the used base model.\n\n    Arguments:\n        schema: The JSON object containing the schema\n        model: The underlying pydantic model\n        base_model: The custom base model that this function is called for.\n    \"\"\"\n    base_model = base_model or BaseModel\n    assert issubclass(model, base_model)\n\n    # a schema should have a specified standard\n    schema[\"$schema\"] = \"https://json-schema.org/draft/2020-12/schema\"\n\n    if model.__base__ is not base_model:\n        # tricky part: de-duplicate fields from parent class\n        split_model_inheritance(schema, model)\n\n    # do this last, because it needs everything else to compute the correct hashsum:\n    schema[KEY_SCHEMA_HASH] = f\"{jsonschema_id(schema)}\"\n    fixup_jsonschema(schema)\n</code></pre>"},{"location":"reference/metador_core/schema/ld/","title":"ld","text":"<p>Helpers to create pydantic models that parse into and (de-)serialize to JSON-LD.</p>"},{"location":"reference/metador_core/schema/ld/#metador_core.schema.ld.ld","title":"ld  <code>module-attribute</code>","text":"<pre><code>ld = ld_decorator()\n</code></pre> <p>Decorator to add constant JSON-LD fields equal for all instances of a schema.</p>"},{"location":"reference/metador_core/schema/ld/#metador_core.schema.ld.LDOrRef","title":"LDOrRef  <code>module-attribute</code>","text":"<pre><code>LDOrRef: TypeAlias = Union[LDIdRef, T]\n</code></pre> <p>LDOrRef[T] is either an object of LD Schema T, or a reference to an object.</p> <p>An LD reference is just an object with an @id.</p>"},{"location":"reference/metador_core/schema/ld/#metador_core.schema.ld.LDRef","title":"LDRef  <code>module-attribute</code>","text":"<pre><code>LDRef: TypeAlias = LDIdRef\n</code></pre> <p>LDRef[T] is a reference to an object of type T.</p> <p>An LD reference is just an object with an @id.</p>"},{"location":"reference/metador_core/schema/ld/#metador_core.schema.ld.LDSchema","title":"LDSchema","text":"<p>             Bases: <code>MetadataSchema</code></p> <p>Semantically enriched schema for JSON-LD.</p> Source code in <code>src/metador_core/schema/ld.py</code> <pre><code>class LDSchema(MetadataSchema):\n\"\"\"Semantically enriched schema for JSON-LD.\"\"\"\n\n    id_: Annotated[Optional[NonEmptyStr], Field(alias=\"@id\")]\n\n    def ref(self) -&gt; LDIdRef:\n\"\"\"Return LDIdRef, i.e. a pure @id reference for object.\n\n        Throws an exception if no @id is found.\n        \"\"\"\n        if self.id_ is None:\n            raise ValueError(\"Object has no @id attribute!\")\n        return LDIdRef(id_=self.id_)\n\n    @property\n    def is_ld_ref(self):\n        return False\n</code></pre>"},{"location":"reference/metador_core/schema/ld/#metador_core.schema.ld.LDSchema.ref","title":"ref","text":"<pre><code>ref() -&gt; LDIdRef\n</code></pre> <p>Return LDIdRef, i.e. a pure @id reference for object.</p> <p>Throws an exception if no @id is found.</p> Source code in <code>src/metador_core/schema/ld.py</code> <pre><code>def ref(self) -&gt; LDIdRef:\n\"\"\"Return LDIdRef, i.e. a pure @id reference for object.\n\n    Throws an exception if no @id is found.\n    \"\"\"\n    if self.id_ is None:\n        raise ValueError(\"Object has no @id attribute!\")\n    return LDIdRef(id_=self.id_)\n</code></pre>"},{"location":"reference/metador_core/schema/ld/#metador_core.schema.ld.LDIdRef","title":"LDIdRef","text":"<p>             Bases: <code>LDSchema</code></p> <p>Object with just an @id reference (more info is given elsewhere).</p> Source code in <code>src/metador_core/schema/ld.py</code> <pre><code>class LDIdRef(LDSchema):\n\"\"\"Object with just an @id reference (more info is given elsewhere).\"\"\"\n\n    class Config:\n        extra = Extra.forbid\n\n    id_: Annotated[NonEmptyStr, Field(alias=\"@id\")]\n\n    def ref(self) -&gt; LDIdRef:\n        return self\n\n    @property\n    def is_ld_ref(self):\n        return True\n</code></pre>"},{"location":"reference/metador_core/schema/ld/#metador_core.schema.ld.with_key_prefix","title":"with_key_prefix","text":"<pre><code>with_key_prefix(\n    prefix: str, dct: Mapping[str, Any]\n) -&gt; Dict[str, Any]\n</code></pre> <p>Return new dict with all keys prefixed by <code>prefix</code>.</p> Source code in <code>src/metador_core/schema/ld.py</code> <pre><code>def with_key_prefix(prefix: str, dct: Mapping[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"Return new dict with all keys prefixed by `prefix`.\"\"\"\n    return {f\"{prefix}{k}\": v for k, v in dct.items() if v is not None}\n</code></pre>"},{"location":"reference/metador_core/schema/ld/#metador_core.schema.ld.ld_decorator","title":"ld_decorator","text":"<pre><code>ld_decorator(**presets)\n</code></pre> <p>Return LD schema decorator with pre-set fields, e.g. <code>@context</code>.</p> <p>The returned decorator will attach the passed fields to a schema.</p> <p>All additional fields passed to the decorator will also be added, if not present, or override the default that is passed to this function.</p> <p>Note that the pre-set fields will ALWAYS override existing fields, regardless of the state of the override flag.</p> <p>Example usage: Pass your <code>@context</code> as <code>context</code> to this decorator factory. Use the returned decorator with <code>type</code> in order to set both the <code>@context</code> and <code>@type</code> for a schema.</p> <p>By default, will silently override any inherited constant fields that already exist in the schema.</p> Source code in <code>src/metador_core/schema/ld.py</code> <pre><code>def ld_decorator(**presets):\n\"\"\"Return LD schema decorator with pre-set fields, e.g. `@context`.\n\n    The returned decorator will attach the passed fields to a schema.\n\n    All additional fields passed to the decorator will also be added,\n    if not present, or override the default that is passed to this function.\n\n    Note that the pre-set fields will ALWAYS override existing fields,\n    regardless of the state of the override flag.\n\n    Example usage:\n    Pass your `@context` as `context` to this decorator factory.\n    Use the returned decorator with `type` in order to\n    set both the `@context` and `@type` for a schema.\n\n    By default, will silently override any inherited\n    constant fields that already exist in the schema.\n    \"\"\"\n\n    def decorator(schema=None, *, override: bool = True, **kwargs):\n        fields = with_key_prefix(\"@\", ChainMap(kwargs, presets))\n        dec = add_const_fields(fields, override=override)\n        return dec if schema is None else dec(schema)\n\n    return decorator\n</code></pre>"},{"location":"reference/metador_core/schema/parser/","title":"parser","text":"<p>Simplify creation of custom parsers for pydantic models.</p>"},{"location":"reference/metador_core/schema/parser/#metador_core.schema.parser.BaseParser","title":"BaseParser","text":"<p>Parsers that work with the ParserMixin must inherit from this class.</p> Source code in <code>src/metador_core/schema/parser.py</code> <pre><code>class BaseParser:\n\"\"\"Parsers that work with the ParserMixin must inherit from this class.\"\"\"\n\n    schema_info: Dict[str, Any] = {}\n    strict: bool = True\n\n    @classmethod\n    def parse(cls, target: Type[T], v: Any) -&gt; T:\n\"\"\"Override and implement this method for custom parsing.\n\n        The default implementation will simply pass through\n        any instances of `tcls` unchanged and fail on anything else.\n\n        Make sure that the parser can also handle any object that itself\n        produces as an input.\n\n        By default, parsers are expected to normalize the input,\n        i.e. produce an instance of `tcls`, any other returned type\n        will lead to an exception.\n\n        If you know what you are doing, set `strict=False` to\n        disable this behavior.\n\n        Args:\n            target: class the value should be parsed into\n            v: value to be parsed\n        \"\"\"\n        if target is not None and not isinstance(v, target):\n            raise TypeError(f\"Expected {target.__name__}, but got {type(v).__name__}!\")\n        return v\n</code></pre>"},{"location":"reference/metador_core/schema/parser/#metador_core.schema.parser.BaseParser.parse","title":"parse  <code>classmethod</code>","text":"<pre><code>parse(target: Type[T], v: Any) -&gt; T\n</code></pre> <p>Override and implement this method for custom parsing.</p> <p>The default implementation will simply pass through any instances of <code>tcls</code> unchanged and fail on anything else.</p> <p>Make sure that the parser can also handle any object that itself produces as an input.</p> <p>By default, parsers are expected to normalize the input, i.e. produce an instance of <code>tcls</code>, any other returned type will lead to an exception.</p> <p>If you know what you are doing, set <code>strict=False</code> to disable this behavior.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Type[T]</code> <p>class the value should be parsed into</p> required <code>v</code> <code>Any</code> <p>value to be parsed</p> required Source code in <code>src/metador_core/schema/parser.py</code> <pre><code>@classmethod\ndef parse(cls, target: Type[T], v: Any) -&gt; T:\n\"\"\"Override and implement this method for custom parsing.\n\n    The default implementation will simply pass through\n    any instances of `tcls` unchanged and fail on anything else.\n\n    Make sure that the parser can also handle any object that itself\n    produces as an input.\n\n    By default, parsers are expected to normalize the input,\n    i.e. produce an instance of `tcls`, any other returned type\n    will lead to an exception.\n\n    If you know what you are doing, set `strict=False` to\n    disable this behavior.\n\n    Args:\n        target: class the value should be parsed into\n        v: value to be parsed\n    \"\"\"\n    if target is not None and not isinstance(v, target):\n        raise TypeError(f\"Expected {target.__name__}, but got {type(v).__name__}!\")\n    return v\n</code></pre>"},{"location":"reference/metador_core/schema/parser/#metador_core.schema.parser.ParserMixin","title":"ParserMixin","text":"<p>Mixin class to simplify creation of custom pydantic field types.</p> <p>Can also be mixed into arbitrary classes, not just pydantic models, that is why it is kept separately from the top level base model we use.</p> <p>Also, we avoid using a custom metaclass for the mixin itself, to increase compatibility with various classes.</p> Source code in <code>src/metador_core/schema/parser.py</code> <pre><code>class ParserMixin:\n\"\"\"Mixin class to simplify creation of custom pydantic field types.\n\n    Can also be mixed into arbitrary classes, not just pydantic models,\n    that is why it is kept separately from the top level base model we use.\n\n    Also, we avoid using a custom metaclass for the mixin itself,\n    to increase compatibility with various classes.\n    \"\"\"\n\n    Parser: ClassVar[Type[BaseParser]]\n\n    @classmethod\n    def __get_validators__(cls):\n        pfunc = cls.__dict__.get(\"__parser_func__\")\n        if pfunc is None:\n            if parser := get_parser(cls):\n\n                def wrapper_func(cls, value, values=None, config=None, field=None):\n                    return run_parser(parser, field.type_, value)\n\n                pfunc = wrapper_func\n            else:\n                pfunc = NoParserDefined\n            cls.__parser_func__ = pfunc  # cache it\n\n        if pfunc is not NoParserDefined:  # return cached parser function\n            yield pfunc\n\n        # important: if no parser is given\n        # and class is a model,\n        # also return the default validate function of the model!\n        if issubclass(cls, BaseModel):\n            yield cls.validate\n\n    @classmethod\n    def __modify_schema__(cls, schema):\n        if parser := get_parser(cls):\n            if schema_info := parser.schema_info:\n                schema.update(**schema_info)\n</code></pre>"},{"location":"reference/metador_core/schema/parser/#metador_core.schema.parser.run_parser","title":"run_parser","text":"<pre><code>run_parser(\n    cls: Type[BaseParser], target: Type[T], value: Any\n)\n</code></pre> <p>Parse and validate passed value.</p> Source code in <code>src/metador_core/schema/parser.py</code> <pre><code>def run_parser(cls: Type[BaseParser], target: Type[T], value: Any):\n\"\"\"Parse and validate passed value.\"\"\"\n    # print(\"call parser\", cls, \"from\", tcls, \"on\", value, \":\", type(value))\n    ret = cls.parse(target, value)\n    if cls.strict and not isinstance(ret, target):\n        msg = f\"Parser returned: {type(ret).__name__}, \"\n        msg += f\"expected: {target.__name__} (strict=True)\"\n        raise RuntimeError(msg)\n    return ret\n</code></pre>"},{"location":"reference/metador_core/schema/parser/#metador_core.schema.parser.get_parser","title":"get_parser","text":"<pre><code>get_parser(cls)\n</code></pre> <p>Return inner Parser class, or None.</p> <p>If the inner Parser class is not a subclass of <code>BaseParser</code>, will raise an exception, as this is most likely an error.</p> Source code in <code>src/metador_core/schema/parser.py</code> <pre><code>def get_parser(cls):\n\"\"\"Return inner Parser class, or None.\n\n    If the inner Parser class is not a subclass of `BaseParser`,\n    will raise an exception, as this is most likely an error.\n    \"\"\"\n    if parser := cls.__dict__.get(\"Parser\"):\n        if not issubclass(parser, BaseParser):\n            msg = f\"{cls}: {cls.Parser.__name__} must be a subclass of {BaseParser.__name__}!\"\n            raise TypeError(msg)\n        return parser\n</code></pre>"},{"location":"reference/metador_core/schema/partial/","title":"partial","text":"<p>Partial pydantic models.</p> <p>Partial models replicate another model with all fields made optional. However, they cannot be part of the inheritance chain of the original models, because semantically it does not make sense or leads to a diamond problem.</p> <p>Therefore they are independent models, unrelated to the original models, but convertable through methods.</p> <p>Partial models are implemented as a mixin class to be combined with the top level base model that you are using in your model hierarchy, e.g. if you use plain <code>BaseModel</code>, you can e.g. define:</p> <pre><code>class MyPartial(DeepPartialModel, BaseModel): ...\n</code></pre> <p>And use <code>MyPartial.get_partial</code> on your models.</p> <p>If different compatible model instances are merged, the merge will produce an instance of the left type.</p> <p>Some theory - partial schemas form a monoid with: * the empty partial schema as neutral element * merge of the fields as the binary operation * associativity follows from associativity of used merge operations</p>"},{"location":"reference/metador_core/schema/partial/#metador_core.schema.partial.PartialModel","title":"PartialModel","text":"<p>Base partial metadata model mixin.</p> <p>In this variant merging is done by simply overwriting old field values with new values (if the new value is not <code>None</code>) in a shallow way.</p> <p>For more fancy merging, consider <code>DeepPartialModel</code>.</p> Source code in <code>src/metador_core/schema/partial.py</code> <pre><code>class PartialModel:\n\"\"\"Base partial metadata model mixin.\n\n    In this variant merging is done by simply overwriting old field values\n    with new values (if the new value is not `None`) in a shallow way.\n\n    For more fancy merging, consider `DeepPartialModel`.\n    \"\"\"\n\n    __partial_src__: ClassVar[Type[BaseModel]]\n\"\"\"Original model class this partial class is based on.\"\"\"\n\n    __partial_fac__: ClassVar[Type[PartialFactory]]\n\"\"\"Factory class that created this partial.\"\"\"\n\n    def from_partial(self):\n\"\"\"Return a new non-partial model instance (will run full validation).\n\n        Raises ValidationError on failure (e.g. if the partial is missing fields).\n        \"\"\"\n        fields = {\n            k: val_from_partial(v)\n            for k, v in self.__partial_fac__._get_field_vals(self)\n        }\n        return self.__partial_src__.parse_obj(fields)\n\n    @classmethod\n    def to_partial(cls, obj, *, ignore_invalid: bool = False):\n\"\"\"Transform `obj` into a new instance of this partial model.\n\n        If passed object is instance of (a subclass of) this or the original model,\n        no validation is performed.\n\n        Returns partial instance with the successfully parsed fields.\n\n        Raises ValidationError if parsing fails.\n        (usless ignore_invalid is set by default or passed).\n        \"\"\"\n        if isinstance(obj, (cls, cls.__partial_src__)):\n            # safe, because subclasses are \"stricter\"\n            return cls.construct(**obj.__dict__)  # type: ignore\n\n        if ignore_invalid:\n            # validate data and keep only valid fields\n            data, fields, _ = validate_model(cls, obj)  # type: ignore\n            return cls.construct(_fields_set=fields, **data)  # type: ignore\n\n        # parse a dict or another pydantic model\n        if isinstance(obj, BaseModel):\n            obj = obj.dict(exclude_none=True)  # type: ignore\n        return cls.parse_obj(obj)  # type: ignore\n\n    @classmethod\n    def cast(\n        cls,\n        obj: Union[BaseModel, PartialModel],\n        *,\n        ignore_invalid: bool = False,\n    ):\n\"\"\"Cast given object into this partial model if needed.\n\n        If it already is an instance, will do nothing.\n        Otherwise, will call `to_partial`.\n        \"\"\"\n        if isinstance(obj, cls):\n            return obj\n        return cls.to_partial(obj, ignore_invalid=ignore_invalid)\n\n    def _update_field(\n        self,\n        v_old,\n        v_new,\n        *,\n        path: List[str] = None,\n        allow_overwrite: bool = False,\n    ):\n\"\"\"Return merged result of the two passed arguments.\n\n        None is always overwritten by a non-None value,\n        lists are concatenated, sets are unionized,\n        partial models are recursively merged,\n        otherwise the new value overwrites the old one.\n        \"\"\"\n        path = path or []\n        # None -&gt; missing value -&gt; just use new value (shortcut)\n        if v_old is None or v_new is None:\n            return v_new or v_old\n\n        # list -&gt; new one must also be a list -&gt; concatenate\n        if isinstance(v_old, list):\n            return v_old + v_new\n\n        # set -&gt; new one must also be a set -&gt; set union\n        if isinstance(v_old, set):\n            # NOTE: we could try being smarter for sets of partial models\n            # https://github.com/Materials-Data-Science-and-Informatics/metador-core/issues/20\n            return v_old.union(v_new)  # set union\n\n        # another model -&gt; recursive merge of partials, if compatible\n        old_is_model = isinstance(v_old, self.__partial_fac__.base_model)\n        new_is_model = isinstance(v_new, self.__partial_fac__.base_model)\n        if old_is_model and new_is_model:\n            v_old_p = self.__partial_fac__.get_partial(type(v_old)).cast(v_old)\n            v_new_p = self.__partial_fac__.get_partial(type(v_new)).cast(v_new)\n            new_subclass_old = issubclass(type(v_new_p), type(v_old_p))\n            old_subclass_new = issubclass(type(v_old_p), type(v_new_p))\n            if new_subclass_old or old_subclass_new:\n                try:\n                    return v_old_p.merge_with(\n                        v_new_p, allow_overwrite=allow_overwrite, _path=path\n                    )\n                except ValidationError:\n                    # casting failed -&gt; proceed to next merge variant\n                    # TODO: maybe raise unless \"ignore invalid\"?\n                    pass\n\n        # if we're here, treat it as an opaque value\n        if not allow_overwrite:\n            msg_title = (\n                f\"Can't overwrite (allow_overwrite=False) at {' -&gt; '.join(path)}:\"\n            )\n            msg = f\"{msg_title}\\n\\t{repr(v_old)}\\n\\twith\\n\\t{repr(v_new)}\"\n            raise ValueError(msg)\n        return v_new\n\n    def merge_with(\n        self,\n        obj,\n        *,\n        ignore_invalid: bool = False,\n        allow_overwrite: bool = False,\n        _path: List[str] = None,\n    ):\n\"\"\"Return a new partial model with updated fields (without validation).\n\n        Raises `ValidationError` if passed `obj` is not suitable,\n        unless `ignore_invalid` is set to `True`.\n\n        Raises `ValueError` if `allow_overwrite=False` and a value would be overwritten.\n        \"\"\"\n        _path = _path or []\n        obj = self.cast(obj, ignore_invalid=ignore_invalid)  # raises on failure\n\n        ret = self.copy()  # type: ignore\n        for f_name, v_new in self.__partial_fac__._get_field_vals(obj):\n            v_old = ret.__dict__.get(f_name)\n            v_merged = self._update_field(\n                v_old, v_new, path=_path + [f_name], allow_overwrite=allow_overwrite\n            )\n            ret.__dict__[f_name] = v_merged\n        return ret\n\n    @classmethod\n    def merge(cls, *objs: PartialModel, **kwargs) -&gt; PartialModel:\n\"\"\"Merge all passed partial models in given order using `merge_with`.\"\"\"\n        # sadly it looks like *args and named kwargs (,*,) syntax cannot be mixed\n        ignore_invalid = kwargs.get(\"ignore_invalid\", False)\n        allow_overwrite = kwargs.get(\"allow_overwrite\", False)\n\n        if not objs:\n            return cls()\n\n        def merge_two(x, y):\n            return cls.cast(x).merge_with(\n                y, ignore_invalid=ignore_invalid, allow_overwrite=allow_overwrite\n            )\n\n        return cls.cast(reduce(merge_two, objs))\n</code></pre>"},{"location":"reference/metador_core/schema/partial/#metador_core.schema.partial.PartialModel.__partial_src__","title":"__partial_src__  <code>class-attribute</code>","text":"<pre><code>__partial_src__: Type[BaseModel]\n</code></pre> <p>Original model class this partial class is based on.</p>"},{"location":"reference/metador_core/schema/partial/#metador_core.schema.partial.PartialModel.__partial_fac__","title":"__partial_fac__  <code>class-attribute</code>","text":"<pre><code>__partial_fac__: Type[PartialFactory]\n</code></pre> <p>Factory class that created this partial.</p>"},{"location":"reference/metador_core/schema/partial/#metador_core.schema.partial.PartialModel.from_partial","title":"from_partial","text":"<pre><code>from_partial()\n</code></pre> <p>Return a new non-partial model instance (will run full validation).</p> <p>Raises ValidationError on failure (e.g. if the partial is missing fields).</p> Source code in <code>src/metador_core/schema/partial.py</code> <pre><code>def from_partial(self):\n\"\"\"Return a new non-partial model instance (will run full validation).\n\n    Raises ValidationError on failure (e.g. if the partial is missing fields).\n    \"\"\"\n    fields = {\n        k: val_from_partial(v)\n        for k, v in self.__partial_fac__._get_field_vals(self)\n    }\n    return self.__partial_src__.parse_obj(fields)\n</code></pre>"},{"location":"reference/metador_core/schema/partial/#metador_core.schema.partial.PartialModel.to_partial","title":"to_partial  <code>classmethod</code>","text":"<pre><code>to_partial(obj, *, ignore_invalid: bool = False)\n</code></pre> <p>Transform <code>obj</code> into a new instance of this partial model.</p> <p>If passed object is instance of (a subclass of) this or the original model, no validation is performed.</p> <p>Returns partial instance with the successfully parsed fields.</p> <p>Raises ValidationError if parsing fails. (usless ignore_invalid is set by default or passed).</p> Source code in <code>src/metador_core/schema/partial.py</code> <pre><code>@classmethod\ndef to_partial(cls, obj, *, ignore_invalid: bool = False):\n\"\"\"Transform `obj` into a new instance of this partial model.\n\n    If passed object is instance of (a subclass of) this or the original model,\n    no validation is performed.\n\n    Returns partial instance with the successfully parsed fields.\n\n    Raises ValidationError if parsing fails.\n    (usless ignore_invalid is set by default or passed).\n    \"\"\"\n    if isinstance(obj, (cls, cls.__partial_src__)):\n        # safe, because subclasses are \"stricter\"\n        return cls.construct(**obj.__dict__)  # type: ignore\n\n    if ignore_invalid:\n        # validate data and keep only valid fields\n        data, fields, _ = validate_model(cls, obj)  # type: ignore\n        return cls.construct(_fields_set=fields, **data)  # type: ignore\n\n    # parse a dict or another pydantic model\n    if isinstance(obj, BaseModel):\n        obj = obj.dict(exclude_none=True)  # type: ignore\n    return cls.parse_obj(obj)  # type: ignore\n</code></pre>"},{"location":"reference/metador_core/schema/partial/#metador_core.schema.partial.PartialModel.cast","title":"cast  <code>classmethod</code>","text":"<pre><code>cast(\n    obj: Union[BaseModel, PartialModel],\n    *,\n    ignore_invalid: bool = False\n)\n</code></pre> <p>Cast given object into this partial model if needed.</p> <p>If it already is an instance, will do nothing. Otherwise, will call <code>to_partial</code>.</p> Source code in <code>src/metador_core/schema/partial.py</code> <pre><code>@classmethod\ndef cast(\n    cls,\n    obj: Union[BaseModel, PartialModel],\n    *,\n    ignore_invalid: bool = False,\n):\n\"\"\"Cast given object into this partial model if needed.\n\n    If it already is an instance, will do nothing.\n    Otherwise, will call `to_partial`.\n    \"\"\"\n    if isinstance(obj, cls):\n        return obj\n    return cls.to_partial(obj, ignore_invalid=ignore_invalid)\n</code></pre>"},{"location":"reference/metador_core/schema/partial/#metador_core.schema.partial.PartialModel.merge_with","title":"merge_with","text":"<pre><code>merge_with(\n    obj,\n    *,\n    ignore_invalid: bool = False,\n    allow_overwrite: bool = False,\n    _path: List[str] = None\n)\n</code></pre> <p>Return a new partial model with updated fields (without validation).</p> <p>Raises <code>ValidationError</code> if passed <code>obj</code> is not suitable, unless <code>ignore_invalid</code> is set to <code>True</code>.</p> <p>Raises <code>ValueError</code> if <code>allow_overwrite=False</code> and a value would be overwritten.</p> Source code in <code>src/metador_core/schema/partial.py</code> <pre><code>def merge_with(\n    self,\n    obj,\n    *,\n    ignore_invalid: bool = False,\n    allow_overwrite: bool = False,\n    _path: List[str] = None,\n):\n\"\"\"Return a new partial model with updated fields (without validation).\n\n    Raises `ValidationError` if passed `obj` is not suitable,\n    unless `ignore_invalid` is set to `True`.\n\n    Raises `ValueError` if `allow_overwrite=False` and a value would be overwritten.\n    \"\"\"\n    _path = _path or []\n    obj = self.cast(obj, ignore_invalid=ignore_invalid)  # raises on failure\n\n    ret = self.copy()  # type: ignore\n    for f_name, v_new in self.__partial_fac__._get_field_vals(obj):\n        v_old = ret.__dict__.get(f_name)\n        v_merged = self._update_field(\n            v_old, v_new, path=_path + [f_name], allow_overwrite=allow_overwrite\n        )\n        ret.__dict__[f_name] = v_merged\n    return ret\n</code></pre>"},{"location":"reference/metador_core/schema/partial/#metador_core.schema.partial.PartialModel.merge","title":"merge  <code>classmethod</code>","text":"<pre><code>merge(\n    *objs: PartialModel, **kwargs: PartialModel\n) -&gt; PartialModel\n</code></pre> <p>Merge all passed partial models in given order using <code>merge_with</code>.</p> Source code in <code>src/metador_core/schema/partial.py</code> <pre><code>@classmethod\ndef merge(cls, *objs: PartialModel, **kwargs) -&gt; PartialModel:\n\"\"\"Merge all passed partial models in given order using `merge_with`.\"\"\"\n    # sadly it looks like *args and named kwargs (,*,) syntax cannot be mixed\n    ignore_invalid = kwargs.get(\"ignore_invalid\", False)\n    allow_overwrite = kwargs.get(\"allow_overwrite\", False)\n\n    if not objs:\n        return cls()\n\n    def merge_two(x, y):\n        return cls.cast(x).merge_with(\n            y, ignore_invalid=ignore_invalid, allow_overwrite=allow_overwrite\n        )\n\n    return cls.cast(reduce(merge_two, objs))\n</code></pre>"},{"location":"reference/metador_core/schema/partial/#metador_core.schema.partial.PartialFactory","title":"PartialFactory","text":"<p>Factory class to create and manage partial models.</p> Source code in <code>src/metador_core/schema/partial.py</code> <pre><code>class PartialFactory:\n\"\"\"Factory class to create and manage partial models.\"\"\"\n\n    base_model: Type[BaseModel] = BaseModel\n    partial_mixin: Type[PartialModel] = PartialModel\n\n    # TODO: how to configure the whole partial family\n    # with default parameters for merge() efficiently?\n    # ----\n    # default arguments for merge (if not explicitly passed)\n    # allow_overwrite: bool = False\n    # \"\"\"Default argument for merge() of partials.\"\"\"\n\n    # ignore_invalid: bool = False\n    # \"\"\"Default argument for merge() of partials.\"\"\"\n\n    @classmethod\n    def _is_base_subclass(cls, obj: Any) -&gt; bool:\n        if not isinstance(obj, type):\n            return False  # not a class (probably just a hint)\n        if not issubclass(obj, cls.base_model):\n            return False  # not a suitable model\n        return True\n\n    @classmethod\n    def _partial_name(cls, mcls: Type[BaseModel]) -&gt; str:\n\"\"\"Return class name for partial of model `mcls`.\"\"\"\n        return f\"{mcls.__qualname__}.{cls.partial_mixin.__name__}\"\n\n    @classmethod\n    def _partial_forwardref_name(cls, mcls: Type[BaseModel]) -&gt; str:\n\"\"\"Return ForwardRef string for partial of model `mcls`.\"\"\"\n        return f\"__{mcls.__module__}_{cls._partial_name(mcls)}\".replace(\".\", \"_\")\n\n    @classmethod\n    def _get_field_vals(cls, obj: BaseModel) -&gt; Iterator[Tuple[str, Any]]:\n\"\"\"Return field values, excluding None and private fields.\n\n        This is different from `BaseModel.dict` as it ignores the defined alias\n        and is used here only for \"internal representation\".\n        \"\"\"\n        return (\n            (k, v)\n            for k, v in obj.__dict__.items()\n            if is_public_name(k) and v is not None\n        )\n\n    @classmethod\n    def _nested_models(cls, field_types: Dict[str, t.TypeHint]) -&gt; Set[Type[BaseModel]]:\n\"\"\"Collect all compatible nested model classes (for which we need partials).\"\"\"\n        return {\n            cast(Type[BaseModel], h)\n            for th in field_types.values()\n            for h in t.traverse_typehint(th)\n            if cls._is_base_subclass(h)\n        }\n\n    @classmethod\n    def _model_to_partial_fref(cls, orig_type: t.TypeHint) -&gt; t.TypeHint:\n\"\"\"Substitute type hint with forward reference to partial models.\n\n        Will return unchanged type if passed argument is not suitable.\n        \"\"\"\n        if not cls._is_base_subclass(orig_type):\n            return orig_type\n        return ForwardRef(cls._partial_forwardref_name(orig_type))\n\n    @classmethod\n    def _model_to_partial(\n        cls, mcls: Type[BaseModel]\n    ) -&gt; Type[Union[BaseModel, PartialModel]]:\n\"\"\"Substitute a model class with partial model.\n\n        Will return unchanged type if argument not suitable.\n        \"\"\"\n        return cls.get_partial(mcls) if cls._is_base_subclass(mcls) else mcls\n\n    @classmethod\n    def _partial_type(cls, orig_type: t.TypeHint) -&gt; t.TypeHint:\n\"\"\"Convert a field type hint into a type hint for the partial.\n\n        This will make the field optional and also replace all nested models\n        derived from the configured base_model with the respective partial model.\n        \"\"\"\n        return Optional[t.map_typehint(orig_type, cls._model_to_partial_fref)]\n\n    @classmethod\n    def _partial_field(cls, orig_type: t.TypeHint) -&gt; Tuple[Type, Optional[FieldInfo]]:\n\"\"\"Return a field declaration tuple for dynamic model creation.\"\"\"\n        th, fi = orig_type, None\n\n        # if pydantic Field is added (in an Annotated[...]) - unwrap\n        if t.get_origin(orig_type) is Annotated:\n            args = t.get_args(orig_type)\n            th = args[0]\n            fi = next(filter(lambda ann: isinstance(ann, FieldInfo), args[1:]), None)\n\n        pth = cls._partial_type(th)  # map the (unwrapped) type to optional\n        return (pth, fi)\n\n    @classmethod\n    def _create_base_partial(cls):\n        class PartialBaseModel(cls.partial_mixin, cls.base_model):\n            class Config:\n                frozen = True  # make sure it's hashable\n\n        return PartialBaseModel\n\n    @classmethod\n    def _create_partial(cls, mcls: Type[BaseModel], *, typehints=None):\n\"\"\"Create a new partial model class based on `mcls`.\"\"\"\n        if not cls._is_base_subclass(mcls):\n            raise TypeError(f\"{mcls} is not subclass of {cls.base_model.__name__}!\")\n        if mcls is cls.base_model:\n            return (cls._create_base_partial(), [])\n        # ----\n        # get field type annotations (or use the passed ones / for performance)\n        hints = typehints or t.get_type_hints(mcls)\n        field_types = {k: v for k, v in hints.items() if k in mcls.__fields__}\n        # get dependencies that must be substituted\n        missing_partials = cls._nested_models(field_types)\n        # compute new field types\n        new_fields = {k: cls._partial_field(v) for k, v in field_types.items()}\n        # replace base classes with corresponding partial bases\n        new_bases = tuple(map(cls._model_to_partial, mcls.__bases__))\n        # create partial model\n        ret: Type[PartialModel] = create_model(\n            cls._partial_name(mcls),\n            __base__=new_bases,\n            __module__=mcls.__module__,\n            __validators__=mcls.__validators__,  # type: ignore\n            **new_fields,\n        )\n        ret.__partial_src__ = mcls  # connect to original model\n        ret.__partial_fac__ = cls  # connect to this class\n        # ----\n        return ret, missing_partials\n\n    @classmethod\n    def get_partial(cls, mcls: Type[BaseModel], *, typehints=None):\n\"\"\"Return a partial schema with all fields of the given schema optional.\n\n        Original default values are not respected and are set to `None`.\n\n        The use of the returned class is for validating partial data before\n        zipping together partial results into a completed one.\n\n        This is a much more fancy version of e.g.\n        https://github.com/pydantic/pydantic/issues/1799\n\n        because it recursively substitutes with partial models.\n        This allows us to implement smart deep merge for partials.\n        \"\"\"\n        if cls not in _partials:  # first use of this partial factory\n            _partials[cls] = {}\n            _forwardrefs[cls] = {}\n\n        if partial := _partials[cls].get(mcls):\n            return partial  # already have a partial\n        else:  # block the spot (to break recursion)\n            _partials[cls][mcls] = None\n\n        # ----\n        # create a partial for a model:\n        # localns = {cls._partial_forwardref_name(k): v for k,v in _partials[cls].items() if v}\n        localns: Dict[str, Any] = {}\n        mcls.update_forward_refs(**localns)  # to be sure\n\n        partial, nested = cls._create_partial(mcls, typehints=typehints)\n        partial_ref = cls._partial_forwardref_name(mcls)\n        # store result\n        _forwardrefs[cls][partial_ref] = partial\n        _partials[cls][mcls] = partial\n        # create partials for nested models\n        for model in nested:\n            cls.get_partial(model)\n        # resolve possible circular references\n        partial.update_forward_refs(**_forwardrefs[cls])  # type: ignore\n        # ----\n        return partial\n</code></pre>"},{"location":"reference/metador_core/schema/partial/#metador_core.schema.partial.PartialFactory.get_partial","title":"get_partial  <code>classmethod</code>","text":"<pre><code>get_partial(\n    mcls: Type[BaseModel],\n    *,\n    typehints: Type[BaseModel] = None\n)\n</code></pre> <p>Return a partial schema with all fields of the given schema optional.</p> <p>Original default values are not respected and are set to <code>None</code>.</p> <p>The use of the returned class is for validating partial data before zipping together partial results into a completed one.</p> <p>This is a much more fancy version of e.g. https://github.com/pydantic/pydantic/issues/1799</p> <p>because it recursively substitutes with partial models. This allows us to implement smart deep merge for partials.</p> Source code in <code>src/metador_core/schema/partial.py</code> <pre><code>@classmethod\ndef get_partial(cls, mcls: Type[BaseModel], *, typehints=None):\n\"\"\"Return a partial schema with all fields of the given schema optional.\n\n    Original default values are not respected and are set to `None`.\n\n    The use of the returned class is for validating partial data before\n    zipping together partial results into a completed one.\n\n    This is a much more fancy version of e.g.\n    https://github.com/pydantic/pydantic/issues/1799\n\n    because it recursively substitutes with partial models.\n    This allows us to implement smart deep merge for partials.\n    \"\"\"\n    if cls not in _partials:  # first use of this partial factory\n        _partials[cls] = {}\n        _forwardrefs[cls] = {}\n\n    if partial := _partials[cls].get(mcls):\n        return partial  # already have a partial\n    else:  # block the spot (to break recursion)\n        _partials[cls][mcls] = None\n\n    # ----\n    # create a partial for a model:\n    # localns = {cls._partial_forwardref_name(k): v for k,v in _partials[cls].items() if v}\n    localns: Dict[str, Any] = {}\n    mcls.update_forward_refs(**localns)  # to be sure\n\n    partial, nested = cls._create_partial(mcls, typehints=typehints)\n    partial_ref = cls._partial_forwardref_name(mcls)\n    # store result\n    _forwardrefs[cls][partial_ref] = partial\n    _partials[cls][mcls] = partial\n    # create partials for nested models\n    for model in nested:\n        cls.get_partial(model)\n    # resolve possible circular references\n    partial.update_forward_refs(**_forwardrefs[cls])  # type: ignore\n    # ----\n    return partial\n</code></pre>"},{"location":"reference/metador_core/schema/partial/#metador_core.schema.partial.is_mergeable_type","title":"is_mergeable_type","text":"<pre><code>is_mergeable_type(hint) -&gt; bool\n</code></pre> <p>Return whether given type can be deeply merged.</p> <p>This imposes some constraints on the shape of valid hints.</p> Source code in <code>src/metador_core/schema/partial.py</code> <pre><code>def is_mergeable_type(hint) -&gt; bool:\n\"\"\"Return whether given type can be deeply merged.\n\n    This imposes some constraints on the shape of valid hints.\n    \"\"\"\n    return _check_type_mergeable(hint, allow_none=True)\n</code></pre>"},{"location":"reference/metador_core/schema/partial/#metador_core.schema.partial.val_from_partial","title":"val_from_partial","text":"<pre><code>val_from_partial(val)\n</code></pre> <p>Recursively convert back from a partial model if val is one.</p> Source code in <code>src/metador_core/schema/partial.py</code> <pre><code>def val_from_partial(val):\n\"\"\"Recursively convert back from a partial model if val is one.\"\"\"\n    if isinstance(val, PartialModel):\n        return val.from_partial()\n    if isinstance(val, list):\n        return [val_from_partial(x) for x in val]\n    if isinstance(val, set):\n        return {val_from_partial(x) for x in val}\n    return val\n</code></pre>"},{"location":"reference/metador_core/schema/pg/","title":"pg","text":"<p>Schema plugin group.</p>"},{"location":"reference/metador_core/schema/pg/#metador_core.schema.pg.SchemaPlugin","title":"SchemaPlugin","text":"<p>             Bases: <code>PluginBase</code></p> <p>Schema-specific Plugin section.</p> Source code in <code>src/metador_core/schema/pg.py</code> <pre><code>class SchemaPlugin(PluginBase):\n\"\"\"Schema-specific Plugin section.\"\"\"\n\n    auxiliary: bool = False\n\"\"\"If set to True, the schema is considered auxiliary.\n\n    The consequence is that metadata objects based on this schema cannot be\n    attached to containers.\n\n    The intended for schema plugins that are too general or not self-contained,\n    but could be useful in a larger context, e.g. as a parent schema or nested\n    schema.\n    \"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/pg/#metador_core.schema.pg.SchemaPlugin.auxiliary","title":"auxiliary  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>auxiliary: bool = False\n</code></pre> <p>If set to True, the schema is considered auxiliary.</p> <p>The consequence is that metadata objects based on this schema cannot be attached to containers.</p> <p>The intended for schema plugins that are too general or not self-contained, but could be useful in a larger context, e.g. as a parent schema or nested schema.</p>"},{"location":"reference/metador_core/schema/pg/#metador_core.schema.pg.PGSchema","title":"PGSchema","text":"<p>             Bases: <code>PluginGroup[MetadataSchema]</code></p> <p>Interface to access installed schema plugins.</p> <p>All registered schema plugins can be used anywhere in a Metador container to annotate any group or dataset with metadata objects following that schema.</p> <p>If you don't want that, do not register the schema as a plugin, but just use the schema class as a normal Python dependency. Schemas that are not registered as plugins still must inherit from MetadataSchema, to ensure that all required methods are available and work as expected by the system.</p> <p>Unregistered schemas can be used as \"abstract\" parent schemas that cannot be instantiated in containers because they are too general to be useful, or for schemas that are not intended to be used on their own in the container, but model a meaningful metadata object that can be part of a larger schema.</p> <p>Guidelines for field definition:</p> <ul> <li>Stick to the following types to construct your field annotation:</li> <li>basic types: (<code>bool, int, float, str</code>)</li> <li>basic hints from <code>typing</code>: <code>Optional, Literal, Union, Set, List, Tuple</code></li> <li>default pydantic types (such as <code>AnyHttpUrl</code>)</li> <li>default classes supported by pydantic (e.g. <code>enum.Enum</code>, <code>datetime</code>, etc.)</li> <li>constrained types defined using the <code>phantom</code> package</li> <li> <p>valid schema classes (subclasses of <code>MetadataSchema</code>)</p> </li> <li> <p><code>Optional</code> is for values that are semantically missing,   You must not assume that a <code>None</code> value represents anything else than that.</p> </li> <li> <p>Prefer <code>Set</code> over <code>List</code> when order is irrelevant and duplicates are not needed</p> </li> <li> <p>Avoid using plain <code>Dict</code>, always define a schema instead if you know the keys,   unless you really need to \"pass through\" whatever is given, which is usually   not necessary for schemas that you design from scratch.</p> </li> <li> <p>Prefer types from <code>phantom</code> over using pydantic <code>Field</code> settings for expressing   simple value constraints (e.g. minimal/maximal value or collection length, etc.),   because <code>phantom</code> types can be subclassed to narrow them down.</p> </li> <li> <p>In general, avoid using <code>Field</code> at all, except for defining an <code>alias</code> for   attributes that are not valid as Python variables (e.g. <code>@id</code> or <code>$schema</code>).</p> </li> <li> <p>When using <code>Field</code>, make sure to annotate it with <code>typing_extensions.Annotated</code>,   instead of assigning the <code>Field</code> object to the field name.</p> </li> </ul> <p>Rules for schema versioning:</p> <p>All schemas must be direct or indirect subclass of <code>MetadataSchema</code>.</p> <p>Semantic versioning (MAJOR, MINOR, PATCH) is to be followed. Bumping a version component means incrementing it and resetting the later ones to 0. When updating a schema, you must bump:</p> <ul> <li> <p>PATCH, if you do not modify the set of parsable instances,</p> </li> <li> <p>MINOR, if if your changes strictly increase parsable instances,</p> </li> <li> <p>MAJOR otherwise, i.e. some older metadata might not be valid anymore.</p> </li> </ul> <p>If you update a nested or inherited schema to a version with higher X (MAJOR, MINOR or PATCH), the version of your schema must be bumped in X as well.</p> <p>Rules for schema subclassing:</p> <p>A child schema that only extends a parent with new fields is safe. To schemas that redefine parent fields additional rules apply:</p> <p>EACH instance of a schema MUST also be parsable by the parent schema</p> <p>This means that a child schema may only override parent fields with more specific types, i.e., only RESTRICT the set of acceptable values compared to the parent field (safe examples include adding new or narrowing existing bounds and constraints, or excluding some values that are allowed by the parent schema).</p> <p>As automatically verifying this in full generality is not feasible, but the ability to \"restrict\" fields is very much needed in practical use, the schema developer MUST create suitable represantative test cases that check whether this property is satisfied.</p> <p>Try expressing field value restrictions by:</p> <ul> <li>removing alternatives from a <code>Union</code></li> <li>using a subclass of a schema or <code>phantom</code> type that was used in the parent</li> </ul> <p>These can be automatically checked most of the time.</p> Source code in <code>src/metador_core/schema/pg.py</code> <pre><code>class PGSchema(pg.PluginGroup[MetadataSchema]):\n\"\"\"Interface to access installed schema plugins.\n\n    All registered schema plugins can be used anywhere in a Metador container to\n    annotate any group or dataset with metadata objects following that schema.\n\n    If you don't want that, do not register the schema as a plugin, but just use\n    the schema class as a normal Python dependency. Schemas that are not\n    registered as plugins still must inherit from MetadataSchema, to ensure that\n    all required methods are available and work as expected by the system.\n\n    Unregistered schemas can be used as \"abstract\" parent schemas that cannot be\n    instantiated in containers because they are too general to be useful, or for\n    schemas that are not intended to be used on their own in the container, but\n    model a meaningful metadata object that can be part of a larger schema.\n\n\n    Guidelines for field definition:\n\n    * Stick to the following types to construct your field annotation:\n      - basic types: (`bool, int, float, str`)\n      - basic hints from `typing`: `Optional, Literal, Union, Set, List, Tuple`\n      - default pydantic types (such as `AnyHttpUrl`)\n      - default classes supported by pydantic (e.g. `enum.Enum`, `datetime`, etc.)\n      - constrained types defined using the `phantom` package\n      - valid schema classes (subclasses of `MetadataSchema`)\n\n    * `Optional` is for values that are semantically *missing*,\n      You must not assume that a `None` value represents anything else than that.\n\n    * Prefer `Set` over `List` when order is irrelevant and duplicates are not needed\n\n    * Avoid using plain `Dict`, always define a schema instead if you know the keys,\n      unless you really need to \"pass through\" whatever is given, which is usually\n      not necessary for schemas that you design from scratch.\n\n    * Prefer types from `phantom` over using pydantic `Field` settings for expressing\n      simple value constraints (e.g. minimal/maximal value or collection length, etc.),\n      because `phantom` types can be subclassed to narrow them down.\n\n    * In general, avoid using `Field` at all, except for defining an `alias` for\n      attributes that are not valid as Python variables (e.g. `@id` or `$schema`).\n\n    * When using `Field`, make sure to annotate it with `typing_extensions.Annotated`,\n      instead of assigning the `Field` object to the field name.\n\n\n    Rules for schema versioning:\n\n    All schemas must be direct or indirect subclass of `MetadataSchema`.\n\n    Semantic versioning (MAJOR, MINOR, PATCH) is to be followed.\n    Bumping a version component means incrementing it and resetting the\n    later ones to 0. When updating a schema, you must bump:\n\n    * PATCH, if you do not modify the set of parsable instances,\n\n    * MINOR, if if your changes strictly increase parsable instances,\n\n    * MAJOR otherwise, i.e. some older metadata might not be valid anymore.\n\n    If you update a nested or inherited schema to a version\n    with higher X (MAJOR, MINOR or PATCH), the version\n    of your schema must be bumped in X as well.\n\n\n    Rules for schema subclassing:\n\n    A child schema that only extends a parent with new fields is safe.\n    To schemas that redefine parent fields additional rules apply:\n\n    EACH instance of a schema MUST also be parsable by the parent schema\n\n    This means that a child schema may only override parent fields\n    with more specific types, i.e., only RESTRICT the set of acceptable\n    values compared to the parent field (safe examples include\n    adding new or narrowing existing bounds and constraints,\n    or excluding some values that are allowed by the parent schema).\n\n    As automatically verifying this in full generality is not feasible, but the\n    ability to \"restrict\" fields is very much needed in practical use, the\n    schema developer MUST create suitable represantative test cases that check\n    whether this property is satisfied.\n\n    Try expressing field value restrictions by:\n\n    * removing alternatives from a `Union`\n    * using a subclass of a schema or `phantom` type that was used in the parent\n\n    These can be automatically checked most of the time.\n    \"\"\"\n\n    class Plugin:\n        name = SCHEMA_GROUP_NAME\n        version = (0, 1, 0)\n        requires = [plugingroups.Plugin.ref()]\n        plugin_class = MetadataSchema\n        plugin_info_class = SchemaPlugin\n\n    def __post_init__(self):\n        self._parent_schema: Dict[\n            Type[MetadataSchema], Optional[Type[MetadataSchema]]\n        ] = {}\n        self._parents: Dict[AnyPluginRef, List[AnyPluginRef]] = {}  # base plugins\n        self._children: Dict[AnyPluginRef, Set[AnyPluginRef]] = {}  # subclass plugins\n\n        # used schemas inside schemas\n        self._field_types: Dict[\n            Type[MetadataSchema], Dict[str, Set[Type[MetadataSchema]]]\n        ] = {}\n        self._subschemas: Dict[MetadataSchema, Set[MetadataSchema]] = {}\n\n        # partial schema classes\n        self._partials: Dict[MetadataSchema, PartialModel] = {}\n        self._forwardrefs: Dict[str, MetadataSchema] = {}\n\n    def plugin_deps(self, plugin) -&gt; Set[AnyPluginRef]:\n        self._parent_schema[plugin] = infer_parent(plugin)\n        if pcls := self._parent_schema[plugin]:\n            # make sure a parent schema plugin is initialized before the child\n            info = pcls.Plugin\n            return {self.PluginRef(name=info.name, version=info.version)}\n        else:\n            return set()\n\n    def check_plugin(self, name: str, plugin: Type[MetadataSchema]):\n        check_types(plugin)  # ensure that (overrides of) fields are valid\n\n    def _compute_parent_path(self, plugin: Type[MetadataSchema]) -&gt; List[AnyPluginRef]:\n        ref = plugin.Plugin.ref()\n        ret = [ref]\n        curr = plugin\n        parent = self._parent_schema[curr]\n        while parent is not None:\n            p_ref = parent.Plugin.ref()\n            ret.append(p_ref)\n            curr = self._get_unsafe(p_ref.name, p_ref.version)\n            parent = self._parent_schema[curr]\n\n        ret.reverse()\n        return ret\n\n    def init_plugin(self, plugin):\n        # pre-compute parent schema path\n        ref = plugin.Plugin.ref()\n        self._parents[ref] = self._compute_parent_path(plugin)\n        if ref not in self._children:\n            self._children[ref] = set()\n\n        # collect children schema set for all parents\n        parents = self._parents[ref][:-1]\n        for parent in parents:\n            self._children[parent].add(ref)\n\n    # ----\n\n    def parent_path(\n        self, schema, version: Optional[SemVerTuple] = None\n    ) -&gt; List[AnyPluginRef]:\n\"\"\"Get sequence of registered parent schema plugins leading to the given schema.\n\n        This sequence can be a subset of the parent sequences in the actual class\n        hierarchy (not every subclass must be registered as a plugin).\n        \"\"\"\n        name, vers = pg.plugin_args(schema, version, require_version=True)\n        ref = self.PluginRef(name=name, version=vers)\n        self._ensure_is_loaded(ref)\n        return list(self._parents[ref])\n\n    def children(\n        self, schema, version: Optional[SemVerTuple] = None\n    ) -&gt; Set[AnyPluginRef]:\n\"\"\"Get set of names of registered (strict) child schemas.\"\"\"\n        name, vers = pg.plugin_args(schema, version, require_version=True)\n        ref = self.PluginRef(name=name, version=vers)\n        self._ensure_is_loaded(ref)\n        return set(self._children[ref])\n</code></pre>"},{"location":"reference/metador_core/schema/pg/#metador_core.schema.pg.PGSchema.parent_path","title":"parent_path","text":"<pre><code>parent_path(\n    schema, version: Optional[SemVerTuple] = None\n) -&gt; List[AnyPluginRef]\n</code></pre> <p>Get sequence of registered parent schema plugins leading to the given schema.</p> <p>This sequence can be a subset of the parent sequences in the actual class hierarchy (not every subclass must be registered as a plugin).</p> Source code in <code>src/metador_core/schema/pg.py</code> <pre><code>def parent_path(\n    self, schema, version: Optional[SemVerTuple] = None\n) -&gt; List[AnyPluginRef]:\n\"\"\"Get sequence of registered parent schema plugins leading to the given schema.\n\n    This sequence can be a subset of the parent sequences in the actual class\n    hierarchy (not every subclass must be registered as a plugin).\n    \"\"\"\n    name, vers = pg.plugin_args(schema, version, require_version=True)\n    ref = self.PluginRef(name=name, version=vers)\n    self._ensure_is_loaded(ref)\n    return list(self._parents[ref])\n</code></pre>"},{"location":"reference/metador_core/schema/pg/#metador_core.schema.pg.PGSchema.children","title":"children","text":"<pre><code>children(\n    schema, version: Optional[SemVerTuple] = None\n) -&gt; Set[AnyPluginRef]\n</code></pre> <p>Get set of names of registered (strict) child schemas.</p> Source code in <code>src/metador_core/schema/pg.py</code> <pre><code>def children(\n    self, schema, version: Optional[SemVerTuple] = None\n) -&gt; Set[AnyPluginRef]:\n\"\"\"Get set of names of registered (strict) child schemas.\"\"\"\n    name, vers = pg.plugin_args(schema, version, require_version=True)\n    ref = self.PluginRef(name=name, version=vers)\n    self._ensure_is_loaded(ref)\n    return set(self._children[ref])\n</code></pre>"},{"location":"reference/metador_core/schema/plugins/","title":"plugins","text":"<p>Schemas needed for the plugin system.</p>"},{"location":"reference/metador_core/schema/plugins/#metador_core.schema.plugins.PkgPlugins","title":"PkgPlugins  <code>module-attribute</code>","text":"<pre><code>PkgPlugins = Dict[str, List[PluginRef]]\n</code></pre> <p>Dict from plugin group name to plugins provided by a package.</p>"},{"location":"reference/metador_core/schema/plugins/#metador_core.schema.plugins.PluginRef","title":"PluginRef","text":"<p>             Bases: <code>MetadataSchema</code></p> <p>Reference to a metador plugin.</p> <p>This class can be subclassed for specific \"marker schemas\".</p> <p>It is not registered as a schema plugin, because it is too general on its own.</p> Source code in <code>src/metador_core/schema/plugins.py</code> <pre><code>@total_ordering\nclass PluginRef(MetadataSchema):\n\"\"\"Reference to a metador plugin.\n\n    This class can be subclassed for specific \"marker schemas\".\n\n    It is not registered as a schema plugin, because it is too general on its own.\n    \"\"\"\n\n    class Config:\n        extra = Extra.forbid\n        allow_mutation = False\n\n    group: NonEmptyStr\n\"\"\"Metador pluggable group name, i.e. name of the entry point group.\"\"\"\n\n    name: NonEmptyStr\n\"\"\"Registered entry point name inside an entry point group.\"\"\"\n\n    version: SemVerTuple\n\"\"\"Version of the Python package.\"\"\"\n\n    def __eq__(self, other):\n        return (\n            self.group == other.group\n            and self.name == other.name\n            and self.version == other.version\n        )\n\n    def __ge__(self, other):\n        if self.group != other.group:\n            return self.group &gt;= other.group\n        if self.name != other.name:\n            return self.name &gt;= other.name\n        if self.version != other.version:\n            return self.version &gt;= other.version\n\n    def __hash__(self):\n        # needed because otherwise would differ in subclass,\n        # causing problems for equality based on __hash__\n        return hash((self.group, self.name, self.version))\n\n    def __str__(self) -&gt; str:\n        return self.json()  # no indent (in contrast to base model)\n\n    def supports(self, other: PluginRef) -&gt; bool:\n\"\"\"Return whether this plugin supports objects marked by given reference.\n\n        True iff the package name, plugin group and plugin name agree,\n        and the package of this reference has equal or larger minor version.\n        \"\"\"\n        if self.group != other.group:\n            return False\n        if self.name != other.name:\n            return False\n        if self.version[0] != other.version[0]:  # major\n            return False\n        if self.version[1] &lt; other.version[1]:  # minor\n            return False\n        return True\n\n    @classmethod\n    def _subclass_for(cls, group: NonEmptyStr):\n\"\"\"Create a subclass of PluginRef with group field pre-set.\"\"\"\n        return create_model(f\"PG{group.capitalize()}.PluginRef\", __base__=cls, group=(Literal[group], group))  # type: ignore\n</code></pre>"},{"location":"reference/metador_core/schema/plugins/#metador_core.schema.plugins.PluginRef.group","title":"group  <code>instance-attribute</code>","text":"<pre><code>group: NonEmptyStr\n</code></pre> <p>Metador pluggable group name, i.e. name of the entry point group.</p>"},{"location":"reference/metador_core/schema/plugins/#metador_core.schema.plugins.PluginRef.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: NonEmptyStr\n</code></pre> <p>Registered entry point name inside an entry point group.</p>"},{"location":"reference/metador_core/schema/plugins/#metador_core.schema.plugins.PluginRef.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: SemVerTuple\n</code></pre> <p>Version of the Python package.</p>"},{"location":"reference/metador_core/schema/plugins/#metador_core.schema.plugins.PluginRef.supports","title":"supports","text":"<pre><code>supports(other: PluginRef) -&gt; bool\n</code></pre> <p>Return whether this plugin supports objects marked by given reference.</p> <p>True iff the package name, plugin group and plugin name agree, and the package of this reference has equal or larger minor version.</p> Source code in <code>src/metador_core/schema/plugins.py</code> <pre><code>def supports(self, other: PluginRef) -&gt; bool:\n\"\"\"Return whether this plugin supports objects marked by given reference.\n\n    True iff the package name, plugin group and plugin name agree,\n    and the package of this reference has equal or larger minor version.\n    \"\"\"\n    if self.group != other.group:\n        return False\n    if self.name != other.name:\n        return False\n    if self.version[0] != other.version[0]:  # major\n        return False\n    if self.version[1] &lt; other.version[1]:  # minor\n        return False\n    return True\n</code></pre>"},{"location":"reference/metador_core/schema/plugins/#metador_core.schema.plugins.PluginBase","title":"PluginBase","text":"<p>             Bases: <code>BaseModelPlus</code></p> <p>All Plugin inner classes must be called <code>Plugin</code> and inherit from this class.</p> Source code in <code>src/metador_core/schema/plugins.py</code> <pre><code>class PluginBase(BaseModelPlus):\n\"\"\"All Plugin inner classes must be called `Plugin` and inherit from this class.\"\"\"\n\n    group: ClassVar[str] = \"\"  # auto-set during plugin group init\n\n    # for type checking (mirrors Fields)\n    name: str\n    version: SemVerTuple\n    requires: List[PluginRef] = []\n\n    def ref(self, *, version: Optional[SemVerTuple] = None):\n        from ..plugins import plugingroups\n\n        assert self.group, \"Must be called from a subclass that has group set!\"\n        return plugingroups[self.group].PluginRef(\n            name=self.name, version=version or self.version\n        )\n\n    def plugin_string(self):\n        return f\"metador.{self.group}.{self.name}.{to_semver_str(self.version)}\"\n\n    def __str__(self) -&gt; str:\n        # pretty-print semver in user-facing representation\n        dct = dict(\n            group=self.group, name=self.name, version=to_semver_str(self.version)\n        )\n        dct.update(\n            self.json_dict(exclude_defaults=True, exclude={\"name\", \"group\", \"version\"})\n        )\n        return json.dumps(dct, indent=2)\n\n    @classmethod\n    def parse_info(cls, info, *, ep_name: str = \"\"):\n        if isinstance(info, cls):\n            return info  # nothing to do, already converted info class to PluginBase (sub)model\n\n        expected_ep_name = f\"{info.name}__{to_semver_str(info.version)}\"\n        ep_name = ep_name or expected_ep_name\n        if ep_name != expected_ep_name:\n            msg = f\"{ep_name}: Based on plugin info, entrypoint must be called '{expected_ep_name}'!\"\n            raise ValueError(msg)\n        try:\n            fields = ChainMap(\n                *(c.__dict__ for c in info.__mro__)\n            )  # this will treat inheritance well\n            # validate\n            public_attrs = {k: v for k, v in fields.items() if is_public_name(k)}\n            return cls(**public_attrs)\n        except ValidationError as e:\n            raise TypeError(f\"{ep_name}: {ep_name}.Plugin validation error: \\n{str(e)}\")\n</code></pre>"},{"location":"reference/metador_core/schema/plugins/#metador_core.schema.plugins.PluginPkgMeta","title":"PluginPkgMeta","text":"<p>             Bases: <code>MetadataSchema</code></p> <p>Metadata of a Python package containing Metador plugins.</p> Source code in <code>src/metador_core/schema/plugins.py</code> <pre><code>class PluginPkgMeta(MetadataSchema):\n\"\"\"Metadata of a Python package containing Metador plugins.\"\"\"\n\n    name: NonEmptyStr\n\"\"\"Name of the Python package.\"\"\"\n\n    version: SemVerTuple\n\"\"\"Version of the Python package.\"\"\"\n\n    repository_url: Optional[AnyHttpUrl] = None\n\"\"\"Python package source location (pip-installable / git-clonable).\"\"\"\n\n    plugins: PkgPlugins = {}\n\n    @classmethod\n    def for_package(cls, package_name: str) -&gt; PluginPkgMeta:\n\"\"\"Extract metadata about a Metador plugin providing Python package.\"\"\"\n        # avoid circular import by importing here\n        from importlib_metadata import distribution\n\n        from ..plugin.entrypoints import DistMeta, distmeta_for\n        from ..plugin.types import EPName, from_ep_name\n\n        dm: DistMeta = distmeta_for(distribution(package_name))\n\n        plugins: PkgPlugins = {}\n        for group, ep_names in dm.plugins.items():\n            plugins[group] = []\n            for ep_name in ep_names:\n                name, version = from_ep_name(EPName(ep_name))\n                ref = PluginRef(group=group, name=name, version=version)\n                plugins[group].append(ref)\n\n        return cls(\n            name=dm.name,\n            version=dm.version,\n            repository_url=dm.repository_url,\n            plugins=plugins,\n        )\n</code></pre>"},{"location":"reference/metador_core/schema/plugins/#metador_core.schema.plugins.PluginPkgMeta.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: NonEmptyStr\n</code></pre> <p>Name of the Python package.</p>"},{"location":"reference/metador_core/schema/plugins/#metador_core.schema.plugins.PluginPkgMeta.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: SemVerTuple\n</code></pre> <p>Version of the Python package.</p>"},{"location":"reference/metador_core/schema/plugins/#metador_core.schema.plugins.PluginPkgMeta.repository_url","title":"repository_url  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>repository_url: Optional[AnyHttpUrl] = None\n</code></pre> <p>Python package source location (pip-installable / git-clonable).</p>"},{"location":"reference/metador_core/schema/plugins/#metador_core.schema.plugins.PluginPkgMeta.for_package","title":"for_package  <code>classmethod</code>","text":"<pre><code>for_package(package_name: str) -&gt; PluginPkgMeta\n</code></pre> <p>Extract metadata about a Metador plugin providing Python package.</p> Source code in <code>src/metador_core/schema/plugins.py</code> <pre><code>@classmethod\ndef for_package(cls, package_name: str) -&gt; PluginPkgMeta:\n\"\"\"Extract metadata about a Metador plugin providing Python package.\"\"\"\n    # avoid circular import by importing here\n    from importlib_metadata import distribution\n\n    from ..plugin.entrypoints import DistMeta, distmeta_for\n    from ..plugin.types import EPName, from_ep_name\n\n    dm: DistMeta = distmeta_for(distribution(package_name))\n\n    plugins: PkgPlugins = {}\n    for group, ep_names in dm.plugins.items():\n        plugins[group] = []\n        for ep_name in ep_names:\n            name, version = from_ep_name(EPName(ep_name))\n            ref = PluginRef(group=group, name=name, version=version)\n            plugins[group].append(ref)\n\n    return cls(\n        name=dm.name,\n        version=dm.version,\n        repository_url=dm.repository_url,\n        plugins=plugins,\n    )\n</code></pre>"},{"location":"reference/metador_core/schema/types/","title":"types","text":"<p>Useful types and validators for use in pydantic models.</p>"},{"location":"reference/metador_core/schema/types/#metador_core.schema.types.SemVerTuple","title":"SemVerTuple  <code>module-attribute</code>","text":"<pre><code>SemVerTuple: TypeAlias = Tuple[\n    NonNegativeInt, NonNegativeInt, NonNegativeInt\n]\n</code></pre> <p>Type to be used for SemVer triples.</p>"},{"location":"reference/metador_core/schema/types/#metador_core.schema.types.NonEmptyStr","title":"NonEmptyStr","text":"<p>             Bases: <code>FullMatch</code></p> <p>Non-empty string (contains non-whitespace characters).</p> Source code in <code>src/metador_core/schema/types.py</code> <pre><code>class NonEmptyStr(FullMatch, pattern=r\"\\s*\\S[\\S\\s]*\"):\n\"\"\"Non-empty string (contains non-whitespace characters).\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/types/#metador_core.schema.types.MimeTypeStr","title":"MimeTypeStr","text":"<p>             Bases: <code>NonEmptyStr</code></p> <p>String that looks like a mime-type.</p> Source code in <code>src/metador_core/schema/types.py</code> <pre><code>class MimeTypeStr(NonEmptyStr, pattern=r\"[^ /;]+/[^ /;]+(;[^ /;]+)*\"):\n\"\"\"String that looks like a mime-type.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/types/#metador_core.schema.types.HashsumStr","title":"HashsumStr","text":"<p>             Bases: <code>NonEmptyStr</code></p> <p>String that looks like a hashsum.</p> Source code in <code>src/metador_core/schema/types.py</code> <pre><code>class HashsumStr(NonEmptyStr, pattern=\"[0-9a-fA-F]+\"):\n\"\"\"String that looks like a hashsum.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/types/#metador_core.schema.types.QualHashsumStr","title":"QualHashsumStr","text":"<p>             Bases: <code>HashsumStr</code></p> <p>Hashsum string, prepended by the used algorithm.</p> Source code in <code>src/metador_core/schema/types.py</code> <pre><code>class QualHashsumStr(HashsumStr, pattern=_hashalg_regex + r\":[0-9a-fA-F]+\"):\n\"\"\"Hashsum string, prepended by the used algorithm.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/types/#metador_core.schema.types.Duration","title":"Duration","text":"<p>             Bases: <code>ParserMixin</code>, <code>Duration</code></p> <p>ISO 8601 Duration.</p> Source code in <code>src/metador_core/schema/types.py</code> <pre><code>@json_encoder(isodate.duration_isoformat)\nclass Duration(ParserMixin, isodate.Duration):\n\"\"\"ISO 8601 Duration.\"\"\"\n\n    class Parser(BaseParser):\n        schema_info = dict(\n            title=\"string in ISO 8601 duration format\",\n            type=\"string\",\n            examples=[\"PT3H4M1S\"],\n        )\n\n        @classmethod\n        def parse(cls, tcls, v):\n            if not isinstance(v, (str, tcls)):\n                raise TypeError(f\"Expected str or Duration, got {type(v)}.\")\n            # we have to force it into a Duration object,\n            # otherwise we get possibly a timedelta back, which we do not want\n            # because we want to serialize to the ISO format in both cases\n            dur = isodate.parse_duration(v) if isinstance(v, str) else v\n            return tcls(seconds=dur.total_seconds())\n</code></pre>"},{"location":"reference/metador_core/schema/types/#metador_core.schema.types.StringParser","title":"StringParser","text":"<p>             Bases: <code>BaseParser</code></p> <p>Parser from string into some target class.</p> Source code in <code>src/metador_core/schema/types.py</code> <pre><code>class StringParser(BaseParser):\n\"\"\"Parser from string into some target class.\"\"\"\n\n    @classmethod\n    def parse(cls, tcls, v):\n        if isinstance(v, tcls):\n            return v\n\n        if not isinstance(v, str):\n            msg = f\"Expected str or {tcls.__name__}, got {type(v)}.\"\n            raise TypeError(msg)\n\n        ret = tcls(v)\n        return ret\n</code></pre>"},{"location":"reference/metador_core/schema/types/#metador_core.schema.types.PintParser","title":"PintParser","text":"<p>             Bases: <code>StringParser</code></p> <p>Shared base for <code>PintUnit</code> and <code>PintQuantity</code>, taking care of exceptions.</p> Source code in <code>src/metador_core/schema/types.py</code> <pre><code>class PintParser(StringParser):\n\"\"\"Shared base for `PintUnit` and `PintQuantity`, taking care of exceptions.\"\"\"\n\n    @classmethod\n    def parse(cls, tcls, v):\n        if not v:\n            msg = f\"Got empty string, expected {tcls.__name__}.\"\n            raise ValueError(msg)\n        try:\n            return super().parse(tcls, v)\n        except UndefinedUnitError as e:\n            raise ValueError(str(e))\n</code></pre>"},{"location":"reference/metador_core/schema/types/#metador_core.schema.types.PintUnit","title":"PintUnit","text":"<p>             Bases: <code>ParserMixin</code>, <code>Unit</code></p> <p>pydantic-compatible pint.Unit.</p> Source code in <code>src/metador_core/schema/types.py</code> <pre><code>@json_encoder(str)\nclass PintUnit(ParserMixin, Unit):\n\"\"\"pydantic-compatible pint.Unit.\"\"\"\n\n    class Parser(PintParser):\n        schema_info = dict(\n            title=\"Physical unit compatible with the Python pint library.\",\n            type=\"string\",\n            examples=[\"meter * candela\", \"kilogram / second ** 2\"],\n        )\n</code></pre>"},{"location":"reference/metador_core/schema/types/#metador_core.schema.types.PintQuantity","title":"PintQuantity","text":"<p>             Bases: <code>ParserMixin</code>, <code>Quantity</code></p> <p>pydantic-compatible pint.Quantity.</p> Source code in <code>src/metador_core/schema/types.py</code> <pre><code>@json_encoder(str)\nclass PintQuantity(ParserMixin, Quantity):\n\"\"\"pydantic-compatible pint.Quantity.\"\"\"\n\n    def __new__(cls, *args, **kwargs):\n        # hack to make parsing work, for some reason it does not work without this\n        # (it does not correctly identify the unit for some reason)\n        if kwargs.get(\"passthrough\"):\n            return super().__new__(cls, *args)\n\n        ret = Quantity(*args)  # ensure that the quantity is correctly parsed\n        # return instance of the subclass:\n        return cls(ret.m, ret.u, passthrough=True)\n\n    class Parser(PintParser):\n        schema_info = dict(\n            title=\"Physical quantity compatible with the Python pint library.\",\n            type=\"string\",\n            examples=[\"5 meter * candela\", \"7.12 kilogram / second ** 2\"],\n        )\n</code></pre>"},{"location":"reference/metador_core/schema/common/","title":"common","text":"<p>Common metadata models based on RO-Crate and Schema.org.</p>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.SIValue","title":"SIValue","text":"<p>             Bases: <code>QuantitativeValue</code></p> <p>QuantitativeValue that holds a numerical value given in SI units.</p> <p>Uses the <code>pint</code> library to parse and normalize the unit.</p> Source code in <code>src/metador_core/schema/common/__init__.py</code> <pre><code>class SIValue(QuantitativeValue):\n\"\"\"QuantitativeValue that holds a numerical value given in SI units.\n\n    Uses the `pint` library to parse and normalize the unit.\n    \"\"\"\n\n    value: Number\n    Parser = SIValueParser\n</code></pre>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.NumValue","title":"NumValue","text":"<p>             Bases: <code>QuantitativeValue</code></p> <p>Quantitative value that can have a unit out of a fixed list.</p> Source code in <code>src/metador_core/schema/common/__init__.py</code> <pre><code>class NumValue(QuantitativeValue):\n\"\"\"Quantitative value that can have a unit out of a fixed list.\"\"\"\n\n    value: Number\n\n    class Parser(BaseParser):\n        schema_info: Dict[str, Any] = {}\n\n        allowed_units: List[str] = []\n        infer_unit: Optional[str] = None\n        require_unit: bool = False\n\n        @classmethod\n        def parse(cls, tcls, v):\n            if isinstance(v, (int, float)):\n                if cls.require_unit:\n                    raise ValueError(f\"Value '{v}' must have a unit!\")\n                return tcls.construct(value=v, unitText=cls.infer_unit)\n\n            if isinstance(v, str):\n                arr = v.strip().split(maxsplit=1)\n\n            # important to parse back serialized data!\n            if isinstance(v, dict):\n                v = tcls.__base__.validate(v)  # -&gt; QuantitativeValue\n\n            if isinstance(v, tcls.__base__):  # unpack QuantitativeValue\n                def_unit = cls.infer_unit or \"\"\n                arr = (v.value, v.unitText or v.unitCode or def_unit)\n\n            # check that value and unit are valid:\n\n            if len(arr) == 1:  # no unit given?\n                if cls.require_unit:\n                    raise ValueError(f\"Value '{v}' must have a unit!\")\n                val = parse_obj_as(Number, arr[0])\n                # return with inferred unit\n                return tcls.construct(value=val, unitText=cls.infer_unit)\n\n            val = parse_obj_as(Tuple[Number, str], arr)\n            if cls.allowed_units and val[1] not in cls.allowed_units:\n                msg = (\n                    f\"Invalid unit '{val[1]}', unit must be one of {cls.allowed_units}\"\n                )\n                raise ValueError(msg)\n            return tcls.construct(value=val[0], unitText=val[1])\n</code></pre>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.Pixels","title":"Pixels","text":"<p>             Bases: <code>NumValue</code></p> <p>Numeric value representing pixels.</p> Source code in <code>src/metador_core/schema/common/__init__.py</code> <pre><code>class Pixels(NumValue):\n\"\"\"Numeric value representing pixels.\"\"\"\n\n    class Parser(NumValue.Parser):\n        allowed_units = [\"px\"]\n        infer_unit = \"px\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.BibMeta","title":"BibMeta","text":"<p>             Bases: <code>DirMeta</code></p> <p>Minimal bibliographic metadata required for a container.</p> Source code in <code>src/metador_core/schema/common/__init__.py</code> <pre><code>@make_mandatory(\"name\", \"abstract\", \"dateCreated\")\nclass BibMeta(DirMeta):\n\"\"\"Minimal bibliographic metadata required for a container.\"\"\"\n\n    class Plugin:\n        name = \"core.bib\"\n        version = (0, 1, 0)\n\n    author: List[Person]\n\"\"\"List of authors (creators of the actual data).\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.BibMeta.author","title":"author  <code>instance-attribute</code>","text":"<pre><code>author: List[Person]\n</code></pre> <p>List of authors (creators of the actual data).</p>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.ImageFileMeta","title":"ImageFileMeta","text":"<p>             Bases: <code>FileMeta</code></p> <p>A rasterized image file with known dimensions.</p> <p>Also serves as marker schema for the imagefile widget.</p> Source code in <code>src/metador_core/schema/common/__init__.py</code> <pre><code>class ImageFileMeta(FileMeta):\n\"\"\"A rasterized image file with known dimensions.\n\n    Also serves as marker schema for the imagefile widget.\n    \"\"\"\n\n    class Plugin:\n        name = \"core.imagefile\"\n        version = (0, 1, 0)\n\n    width: Pixels\n\"\"\"Width of the image in pixels.\"\"\"\n\n    height: Pixels\n\"\"\"Height of the image in pixels.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.ImageFileMeta.width","title":"width  <code>instance-attribute</code>","text":"<pre><code>width: Pixels\n</code></pre> <p>Width of the image in pixels.</p>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.ImageFileMeta.height","title":"height  <code>instance-attribute</code>","text":"<pre><code>height: Pixels\n</code></pre> <p>Height of the image in pixels.</p>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.ColumnHeader","title":"ColumnHeader","text":"<p>             Bases: <code>MetadataSchema</code></p> <p>Table column metadata.</p> Source code in <code>src/metador_core/schema/common/__init__.py</code> <pre><code>class ColumnHeader(MetadataSchema):\n\"\"\"Table column metadata.\"\"\"\n\n    name: Text\n\"\"\"Column title.\"\"\"\n\n    unit: PintUnit\n\"\"\"Physical unit for this column.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.ColumnHeader.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Text\n</code></pre> <p>Column title.</p>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.ColumnHeader.unit","title":"unit  <code>instance-attribute</code>","text":"<pre><code>unit: PintUnit\n</code></pre> <p>Physical unit for this column.</p>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.TableMeta","title":"TableMeta","text":"<p>             Bases: <code>MetadataSchema</code></p> <p>Table metadata.</p> Source code in <code>src/metador_core/schema/common/__init__.py</code> <pre><code>class TableMeta(MetadataSchema):\n\"\"\"Table metadata.\"\"\"\n\n    class Plugin:\n        name = \"core.table\"\n        version = (0, 1, 0)\n\n    name: Text\n\"\"\"Table title.\"\"\"\n\n    columns: List[ColumnHeader]\n\"\"\"List of column descriptions.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.TableMeta.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Text\n</code></pre> <p>Table title.</p>"},{"location":"reference/metador_core/schema/common/#metador_core.schema.common.TableMeta.columns","title":"columns  <code>instance-attribute</code>","text":"<pre><code>columns: List[ColumnHeader]\n</code></pre> <p>List of column descriptions.</p>"},{"location":"reference/metador_core/schema/common/rocrate/","title":"rocrate","text":"<p>RO Crate compatible Metadata.</p> <p>Here we do impose certain constraints (make fields mandatory).</p> <p>See https://www.researchobject.org/ro-crate/1.1/</p>"},{"location":"reference/metador_core/schema/common/rocrate/#metador_core.schema.common.rocrate.FileMeta","title":"FileMeta","text":"<p>             Bases: <code>MediaObject</code></p> Source code in <code>src/metador_core/schema/common/rocrate.py</code> <pre><code>@make_mandatory(\"contentSize\", \"sha256\")\n@rocrate(type=\"File\")\nclass FileMeta(schemaorg.MediaObject):\n    class Plugin:\n        name = \"core.file\"\n        version = (0, 1, 0)\n\n    # NOTE: We do not use `name` here because `name` is used semantically\n    # like a title in schema.org, which could also make sense for a file to have.\n    filename: NonEmptyStr\n\"\"\"Original name of the file in source directory.\"\"\"\n\n    encodingFormat: MimeTypeStr\n\"\"\"MIME type of the file.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/rocrate/#metador_core.schema.common.rocrate.FileMeta.filename","title":"filename  <code>instance-attribute</code>","text":"<pre><code>filename: NonEmptyStr\n</code></pre> <p>Original name of the file in source directory.</p>"},{"location":"reference/metador_core/schema/common/rocrate/#metador_core.schema.common.rocrate.FileMeta.encodingFormat","title":"encodingFormat  <code>instance-attribute</code>","text":"<pre><code>encodingFormat: MimeTypeStr\n</code></pre> <p>MIME type of the file.</p>"},{"location":"reference/metador_core/schema/common/rocrate/#metador_core.schema.common.rocrate.DirMeta","title":"DirMeta","text":"<p>             Bases: <code>Dataset</code></p> Source code in <code>src/metador_core/schema/common/rocrate.py</code> <pre><code>@rocrate(type=\"Dataset\")\nclass DirMeta(schemaorg.Dataset):\n    class Plugin:\n        name = \"core.dir\"\n        version = (0, 1, 0)\n\n    hasPart: Set[LDIdRef] = set()\n\"\"\"References to (a subset of) contained files and subdirectories.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/rocrate/#metador_core.schema.common.rocrate.DirMeta.hasPart","title":"hasPart  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>hasPart: Set[LDIdRef] = set()\n</code></pre> <p>References to (a subset of) contained files and subdirectories.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/","title":"schemaorg","text":"<p>Schema.org-compatible common metadata schemas.</p> <p>Supports a subset of commonly useful fields.</p> <p>Adds almost no constraints beyond the spec, except for fixing a multiplicity for fields.</p> <p>Intended to serve as the basis for more specific schemas.</p> <p>Note that this schemas ARE NOT able to parse arbitrary schema.org-aligned metadata, their purpose is to ensure that successfully parsed input is semantically enriched.</p> <p>See schema.org official documentation for full explanation and list of all fields.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Thing","title":"Thing","text":"<p>             Bases: <code>LDSchema</code></p> <p>See https://schema.org/Thing for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"Thing\")\nclass Thing(LDSchema):\n\"\"\"See https://schema.org/Thing for field documentation.\"\"\"\n\n    name: Optional[Text]\n\"\"\"Name, title or caption of the entity.\"\"\"\n\n    identifier: Optional[Union[URL, Text]]  # can't put PropertyValue here, weird bug\n\"\"\"Arbitrary identifier of the entity.\n\n    Prefer @id if the identifier is web-resolvable, or use more\n    specific fields if available.\"\"\"\n\n    url: Optional[URL]\n\"\"\"URL of the entity.\"\"\"\n\n    description: Optional[Text]\n\"\"\"Description of the entity.\"\"\"\n\n    # ----\n\n    alternateName: Optional[List[Text]]\n\"\"\"Known aliases of the entity.\"\"\"\n\n    sameAs: Optional[List[URL]]\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Thing.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: Optional[Text]\n</code></pre> <p>Name, title or caption of the entity.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Thing.identifier","title":"identifier  <code>instance-attribute</code>","text":"<pre><code>identifier: Optional[Union[URL, Text]]\n</code></pre> <p>Arbitrary identifier of the entity.</p> <p>Prefer @id if the identifier is web-resolvable, or use more specific fields if available.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Thing.url","title":"url  <code>instance-attribute</code>","text":"<pre><code>url: Optional[URL]\n</code></pre> <p>URL of the entity.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Thing.description","title":"description  <code>instance-attribute</code>","text":"<pre><code>description: Optional[Text]\n</code></pre> <p>Description of the entity.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Thing.alternateName","title":"alternateName  <code>instance-attribute</code>","text":"<pre><code>alternateName: Optional[List[Text]]\n</code></pre> <p>Known aliases of the entity.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.ValueCommon","title":"ValueCommon","text":"<p>             Bases: <code>Thing</code></p> <p>Common properties of multiple *Value classes.</p> <p>For some reason these have no common ancestor in schema.org.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>class ValueCommon(Thing):\n\"\"\"Common properties of multiple *Value classes.\n\n    For some reason these have no common ancestor in schema.org.\n    \"\"\"\n\n    value: Optional[Union[Bool, Number, Text, StructuredValue]]\n\n    # valueReference: Optional[]\n\n    minValue: Optional[Number]\n\"\"\"Minimal value of property this value corresponds to.\"\"\"\n\n    maxValue: Optional[Number]\n\"\"\"Maximal value of property this value corresponds to.\"\"\"\n\n    unitCode: Optional[Union[URL, Text]]\n\"\"\"UN/CEFACT Common Code (3 characters) or URL.\n\n    Other codes may be used with a prefix followed by a colon.\"\"\"\n\n    unitText: Optional[Text]\n\"\"\"String indicating the unit of measurement.\n\n    Useful if no standard unitCode can be provided.\n    \"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.ValueCommon.minValue","title":"minValue  <code>instance-attribute</code>","text":"<pre><code>minValue: Optional[Number]\n</code></pre> <p>Minimal value of property this value corresponds to.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.ValueCommon.maxValue","title":"maxValue  <code>instance-attribute</code>","text":"<pre><code>maxValue: Optional[Number]\n</code></pre> <p>Maximal value of property this value corresponds to.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.ValueCommon.unitCode","title":"unitCode  <code>instance-attribute</code>","text":"<pre><code>unitCode: Optional[Union[URL, Text]]\n</code></pre> <p>UN/CEFACT Common Code (3 characters) or URL.</p> <p>Other codes may be used with a prefix followed by a colon.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.ValueCommon.unitText","title":"unitText  <code>instance-attribute</code>","text":"<pre><code>unitText: Optional[Text]\n</code></pre> <p>String indicating the unit of measurement.</p> <p>Useful if no standard unitCode can be provided.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.StructuredValue","title":"StructuredValue","text":"<p>             Bases: <code>ValueCommon</code></p> <p>See https://schema.org/StructuredValue for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"StructuredValue\")\nclass StructuredValue(ValueCommon):\n\"\"\"See https://schema.org/StructuredValue for field documentation.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.QuantitativeValue","title":"QuantitativeValue","text":"<p>             Bases: <code>StructuredValue</code></p> <p>See https://schema.org/QuantitativeValue for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"QuantitativeValue\")\nclass QuantitativeValue(StructuredValue):\n\"\"\"See https://schema.org/QuantitativeValue for field documentation.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.PropertyValue","title":"PropertyValue","text":"<p>             Bases: <code>StructuredValue</code></p> <p>Use 'name' for the property name and 'description' for alternative human-readable value.</p> <p>See https://schema.org/PropertyValue for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"PropertyValue\")\nclass PropertyValue(StructuredValue):\n\"\"\"Use 'name' for the property name and 'description' for alternative human-readable value.\n\n    See https://schema.org/PropertyValue for field documentation.\n    \"\"\"\n\n    propertyID: Optional[Union[URL, Text]]\n\"\"\"A commonly used identifier for the characteristic represented by the property,\n    e.g. a manufacturer or a standard code for a property.\"\"\"\n\n    measurementTechnique: Optional[Union[URL, Text]]\n\"\"\"A technique or technology used in a Dataset (or DataDownload, DataCatalog),\n    corresponding to the method used for measuring the corresponding variable(s)\n    (described using variableMeasured).\n\n    This is oriented towards scientific and scholarly dataset publication but\n    may have broader applicability; it is not intended as a full representation\n    of measurement, but rather as a high level summary for dataset discovery.\n    \"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.PropertyValue.propertyID","title":"propertyID  <code>instance-attribute</code>","text":"<pre><code>propertyID: Optional[Union[URL, Text]]\n</code></pre> <p>A commonly used identifier for the characteristic represented by the property, e.g. a manufacturer or a standard code for a property.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.PropertyValue.measurementTechnique","title":"measurementTechnique  <code>instance-attribute</code>","text":"<pre><code>measurementTechnique: Optional[Union[URL, Text]]\n</code></pre> <p>A technique or technology used in a Dataset (or DataDownload, DataCatalog), corresponding to the method used for measuring the corresponding variable(s) (described using variableMeasured).</p> <p>This is oriented towards scientific and scholarly dataset publication but may have broader applicability; it is not intended as a full representation of measurement, but rather as a high level summary for dataset discovery.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Organization","title":"Organization","text":"<p>             Bases: <code>Thing</code></p> <p>See https://schema.org/Organization for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"Organization\")\nclass Organization(Thing):\n\"\"\"See https://schema.org/Organization for field documentation.\"\"\"\n\n    address: Optional[Text]\n\"\"\"Address of the organization.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Organization.address","title":"address  <code>instance-attribute</code>","text":"<pre><code>address: Optional[Text]\n</code></pre> <p>Address of the organization.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Person","title":"Person","text":"<p>             Bases: <code>Thing</code></p> <p>See https://schema.org/Person for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"Person\")\nclass Person(Thing):\n\"\"\"See https://schema.org/Person for field documentation.\"\"\"\n\n    givenName: Optional[Text]\n\"\"\"Given name, typically the first name of a Person.\"\"\"\n\n    familyName: Optional[Text]\n\"\"\"Family name of a Person.\"\"\"\n\n    additionalName: Optional[Text]\n\"\"\"Additional name for a Person, e.g. for a middle name.\"\"\"\n\n    email: Optional[Text]\n\"\"\"E-mail address.\"\"\"\n\n    affiliation: Optional[LDOrRef[Organization]]\n\"\"\"An organization this person is affiliated with.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Person.givenName","title":"givenName  <code>instance-attribute</code>","text":"<pre><code>givenName: Optional[Text]\n</code></pre> <p>Given name, typically the first name of a Person.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Person.familyName","title":"familyName  <code>instance-attribute</code>","text":"<pre><code>familyName: Optional[Text]\n</code></pre> <p>Family name of a Person.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Person.additionalName","title":"additionalName  <code>instance-attribute</code>","text":"<pre><code>additionalName: Optional[Text]\n</code></pre> <p>Additional name for a Person, e.g. for a middle name.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Person.email","title":"email  <code>instance-attribute</code>","text":"<pre><code>email: Optional[Text]\n</code></pre> <p>E-mail address.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Person.affiliation","title":"affiliation  <code>instance-attribute</code>","text":"<pre><code>affiliation: Optional[LDOrRef[Organization]]\n</code></pre> <p>An organization this person is affiliated with.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.CreativeWork","title":"CreativeWork","text":"<p>             Bases: <code>Thing</code></p> <p>See https://schema.org/CreativeWork for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"CreativeWork\")\nclass CreativeWork(Thing):\n\"\"\"See https://schema.org/CreativeWork for field documentation.\"\"\"\n\n    version: Optional[Union[NonNegativeInt, Text]]\n\"\"\"Version of this work.\n\n    Either an integer, or a version string, e.g. \"1.0.5\".\n\n    When using version strings, follow https://semver.org\n    whenever applicable.\n    \"\"\"\n\n    citation: Optional[Set[Union[LDOrRef[CreativeWork], Text]]]\n\"\"\"Citation or reference to another creative work, e.g.\n    another publication, scholarly article, etc.\"\"\"\n\n    # search\n\n    abstract: Optional[Text]\n\"\"\"A short description that summarizes the creative work.\"\"\"\n\n    keywords: Optional[Set[Text]]\n\"\"\"Keywords or tags to describe this creative work.\"\"\"\n\n    # people\n\n    author: Optional[List[LDOrRef[OrgOrPerson]]]\n\"\"\"People responsible for the work, e.g. in research,\n    the people who would be authors on the relevant paper.\"\"\"\n\n    contributor: Optional[List[LDOrRef[OrgOrPerson]]]\n\"\"\"Additional people who contributed to the work, e.g.\n    in research, the people who would be in the acknowledgements\n    section of the relevant paper.\"\"\"\n\n    maintainer: Optional[List[LDOrRef[OrgOrPerson]]]\n    producer: Optional[List[LDOrRef[OrgOrPerson]]]\n    provider: Optional[List[LDOrRef[OrgOrPerson]]]\n    publisher: Optional[List[LDOrRef[OrgOrPerson]]]\n    sponsor: Optional[List[LDOrRef[OrgOrPerson]]]\n    editor: Optional[List[LDOrRef[Person]]]\n\n    # date\n\n    dateCreated: Optional[DateOrDatetime]\n    dateModified: Optional[DateOrDatetime]\n    datePublished: Optional[DateOrDatetime]\n\n    # legal\n\n    copyrightHolder: Optional[LDOrRef[OrgOrPerson]]\n    copyrightYear: Optional[Int]\n    copyrightNotice: Optional[Text]\n    license: Optional[Union[URL, LDOrRef[CreativeWork]]]\n\n    # provenance\n\n    about: Optional[Set[LDOrRef[Thing]]]\n    subjectOf: Optional[Set[LDOrRef[CreativeWork]]]\n    hasPart: Optional[Set[LDOrRef[CreativeWork]]]\n    isPartOf: Optional[Set[Union[URL, LDOrRef[CreativeWork]]]]\n    isBasedOn: Optional[Set[Union[URL, LDOrRef[CreativeWork]]]]\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.CreativeWork.version","title":"version  <code>instance-attribute</code>","text":"<pre><code>version: Optional[Union[NonNegativeInt, Text]]\n</code></pre> <p>Version of this work.</p> <p>Either an integer, or a version string, e.g. \"1.0.5\".</p> <p>When using version strings, follow https://semver.org whenever applicable.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.CreativeWork.citation","title":"citation  <code>instance-attribute</code>","text":"<pre><code>citation: Optional[Set[Union[LDOrRef[CreativeWork], Text]]]\n</code></pre> <p>Citation or reference to another creative work, e.g. another publication, scholarly article, etc.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.CreativeWork.abstract","title":"abstract  <code>instance-attribute</code>","text":"<pre><code>abstract: Optional[Text]\n</code></pre> <p>A short description that summarizes the creative work.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.CreativeWork.keywords","title":"keywords  <code>instance-attribute</code>","text":"<pre><code>keywords: Optional[Set[Text]]\n</code></pre> <p>Keywords or tags to describe this creative work.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.CreativeWork.author","title":"author  <code>instance-attribute</code>","text":"<pre><code>author: Optional[List[LDOrRef[OrgOrPerson]]]\n</code></pre> <p>People responsible for the work, e.g. in research, the people who would be authors on the relevant paper.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.CreativeWork.contributor","title":"contributor  <code>instance-attribute</code>","text":"<pre><code>contributor: Optional[List[LDOrRef[OrgOrPerson]]]\n</code></pre> <p>Additional people who contributed to the work, e.g. in research, the people who would be in the acknowledgements section of the relevant paper.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.DefinedTermSet","title":"DefinedTermSet","text":"<p>             Bases: <code>CreativeWork</code></p> <p>See https://schema.org/DefinedTermSet for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"DefinedTermSet\")\nclass DefinedTermSet(CreativeWork):\n\"\"\"See https://schema.org/DefinedTermSet for field documentation.\"\"\"\n\n    hasDefinedTerm: List[LDOrRef[DefinedTerm]]\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.DefinedTerm","title":"DefinedTerm","text":"<p>             Bases: <code>Thing</code></p> <p>See https://schema.org/DefinedTerm for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"DefinedTerm\")\nclass DefinedTerm(Thing):\n\"\"\"See https://schema.org/DefinedTerm for field documentation.\"\"\"\n\n    # NOTE: also use name and description\n\n    termCode: Text\n\"\"\"A code that identifies this DefinedTerm within a DefinedTermSet.\"\"\"\n\n    inDefinedTermSet: Optional[Union[URL, LDIdRef]]  # ref to a DefinedTermSet\n\"\"\"A DefinedTermSet that contains this term.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.DefinedTerm.termCode","title":"termCode  <code>instance-attribute</code>","text":"<pre><code>termCode: Text\n</code></pre> <p>A code that identifies this DefinedTerm within a DefinedTermSet.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.DefinedTerm.inDefinedTermSet","title":"inDefinedTermSet  <code>instance-attribute</code>","text":"<pre><code>inDefinedTermSet: Optional[Union[URL, LDIdRef]]\n</code></pre> <p>A DefinedTermSet that contains this term.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.CategoryCodeSet","title":"CategoryCodeSet","text":"<p>             Bases: <code>DefinedTermSet</code></p> <p>See https://schema.org/CategoryCodeSet for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"CategoryCodeSet\")\nclass CategoryCodeSet(DefinedTermSet):\n\"\"\"See https://schema.org/CategoryCodeSet for field documentation.\"\"\"\n\n    hasCategoryCode: List[LDOrRef[CategoryCode]]\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.CategoryCode","title":"CategoryCode","text":"<p>             Bases: <code>DefinedTerm</code></p> <p>See https://schema.org/CategoryCode for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"CategoryCode\")\nclass CategoryCode(DefinedTerm):\n\"\"\"See https://schema.org/CategoryCode for field documentation.\"\"\"\n\n    codeValue: Text\n\"\"\"A short textual code that uniquely identifies the value.\"\"\"\n\n    inCodeSet: Optional[Union[URL, LDIdRef]]  # ref to a CategoryCodeSet\n\"\"\"A CategoryCodeSet that contains this category code.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.CategoryCode.codeValue","title":"codeValue  <code>instance-attribute</code>","text":"<pre><code>codeValue: Text\n</code></pre> <p>A short textual code that uniquely identifies the value.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.CategoryCode.inCodeSet","title":"inCodeSet  <code>instance-attribute</code>","text":"<pre><code>inCodeSet: Optional[Union[URL, LDIdRef]]\n</code></pre> <p>A CategoryCodeSet that contains this category code.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.MediaObject","title":"MediaObject","text":"<p>             Bases: <code>CreativeWork</code></p> <p>See https://schema.org/MediaObject for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"MediaObject\")\nclass MediaObject(CreativeWork):\n\"\"\"See https://schema.org/MediaObject for field documentation.\"\"\"\n\n    contentSize: Optional[Int]\n\"\"\"Size of the object in bytes.\"\"\"\n\n    sha256: Optional[Text]\n\"\"\"Sha256 hashsum string of the object.\"\"\"\n\n    encodingFormat: Optional[Union[URL, Text]]\n\"\"\"MIME type, or if the format is too niche or no standard MIME type is\n    defined, an URL pointing to a description of the format.\"\"\"\n\n    width: Optional[QuantitativeValue]\n\"\"\"Width of the entity.\"\"\"\n\n    height: Optional[QuantitativeValue]\n\"\"\"Height of the entity.\"\"\"\n\n    bitrate: Optional[Text]\n\"\"\"Bitrate of the entity (e.g. for audio or video).\"\"\"\n\n    duration: Optional[Duration]\n\"\"\"Duration of the entity (e.g. for audio or video).\"\"\"\n\n    startTime: Optional[TimeOrDatetime]\n\"\"\"Physical starting time, e.g. of a recording or measurement.\"\"\"\n\n    endTime: Optional[TimeOrDatetime]\n\"\"\"Physical ending time, e.g. of a recording or measurement.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.MediaObject.contentSize","title":"contentSize  <code>instance-attribute</code>","text":"<pre><code>contentSize: Optional[Int]\n</code></pre> <p>Size of the object in bytes.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.MediaObject.sha256","title":"sha256  <code>instance-attribute</code>","text":"<pre><code>sha256: Optional[Text]\n</code></pre> <p>Sha256 hashsum string of the object.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.MediaObject.encodingFormat","title":"encodingFormat  <code>instance-attribute</code>","text":"<pre><code>encodingFormat: Optional[Union[URL, Text]]\n</code></pre> <p>MIME type, or if the format is too niche or no standard MIME type is defined, an URL pointing to a description of the format.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.MediaObject.width","title":"width  <code>instance-attribute</code>","text":"<pre><code>width: Optional[QuantitativeValue]\n</code></pre> <p>Width of the entity.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.MediaObject.height","title":"height  <code>instance-attribute</code>","text":"<pre><code>height: Optional[QuantitativeValue]\n</code></pre> <p>Height of the entity.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.MediaObject.bitrate","title":"bitrate  <code>instance-attribute</code>","text":"<pre><code>bitrate: Optional[Text]\n</code></pre> <p>Bitrate of the entity (e.g. for audio or video).</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.MediaObject.duration","title":"duration  <code>instance-attribute</code>","text":"<pre><code>duration: Optional[Duration]\n</code></pre> <p>Duration of the entity (e.g. for audio or video).</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.MediaObject.startTime","title":"startTime  <code>instance-attribute</code>","text":"<pre><code>startTime: Optional[TimeOrDatetime]\n</code></pre> <p>Physical starting time, e.g. of a recording or measurement.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.MediaObject.endTime","title":"endTime  <code>instance-attribute</code>","text":"<pre><code>endTime: Optional[TimeOrDatetime]\n</code></pre> <p>Physical ending time, e.g. of a recording or measurement.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Dataset","title":"Dataset","text":"<p>             Bases: <code>CreativeWork</code></p> <p>See https://schema.org/Dataset for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"Dataset\")\nclass Dataset(CreativeWork):\n\"\"\"See https://schema.org/Dataset for field documentation.\"\"\"\n\n    distribution: Optional[URL]  # NOTE: for top level description could link to repo\n\"\"\"Downloadable form of this dataset, at a specific location, in a specific format.\"\"\"\n\n    variableMeasured: Optional[List[Union[Text, PropertyValue]]]\n\"\"\"Variables that are measured in the dataset.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Dataset.distribution","title":"distribution  <code>instance-attribute</code>","text":"<pre><code>distribution: Optional[URL]\n</code></pre> <p>Downloadable form of this dataset, at a specific location, in a specific format.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Dataset.variableMeasured","title":"variableMeasured  <code>instance-attribute</code>","text":"<pre><code>variableMeasured: Optional[List[Union[Text, PropertyValue]]]\n</code></pre> <p>Variables that are measured in the dataset.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Product","title":"Product","text":"<p>             Bases: <code>Thing</code></p> <p>See https://schema.org/Product for field documentation.</p> Source code in <code>src/metador_core/schema/common/schemaorg.py</code> <pre><code>@schemaorg(type=\"Product\")\nclass Product(Thing):\n\"\"\"See https://schema.org/Product for field documentation.\"\"\"\n\n    productID: Optional[Text]\n\"\"\"The product identifier, such as ISBN.\"\"\"\n\n    # properties\n\n    category: Optional[Union[Text, URL, CategoryCode, Thing]]\n\"\"\"A category for the item.\n\n    Greater signs or slashes can be used to informally indicate a category hierarchy.\n    \"\"\"\n\n    material: Optional[Union[URL, Text, Product]]\n\"\"\"A material that something is made from, e.g. leather, wool, cotton, paper.\"\"\"\n\n    pattern: Optional[Union[DefinedTerm, Text]]\n\"\"\"A pattern that something has, for example 'polka dot', 'striped', 'Canadian flag'.\n\n    Values are typically expressed as text, although links to controlled value schemes are also supported.\n    \"\"\"\n\n    width: Optional[QuantitativeValue]\n    height: Optional[QuantitativeValue]\n    depth: Optional[QuantitativeValue]\n\n    weight: Optional[QuantitativeValue]\n    color: Optional[Text]\n\n    additionalProperty: Optional[List[PropertyValue]]\n\"\"\"A property-value pair representing an additional characteristic of the entity, e.g. a product feature or another characteristic for which there is no matching property in schema.org.\"\"\"\n\n    # meta\n\n    productionDate: Optional[DateOrDatetime]\n    releaseDate: Optional[DateOrDatetime]\n\n    isRelatedTo: Optional[LDOrRef[Product]]\n    isSimilarTo: Optional[LDOrRef[Product]]\n</code></pre>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Product.productID","title":"productID  <code>instance-attribute</code>","text":"<pre><code>productID: Optional[Text]\n</code></pre> <p>The product identifier, such as ISBN.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Product.category","title":"category  <code>instance-attribute</code>","text":"<pre><code>category: Optional[Union[Text, URL, CategoryCode, Thing]]\n</code></pre> <p>A category for the item.</p> <p>Greater signs or slashes can be used to informally indicate a category hierarchy.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Product.material","title":"material  <code>instance-attribute</code>","text":"<pre><code>material: Optional[Union[URL, Text, Product]]\n</code></pre> <p>A material that something is made from, e.g. leather, wool, cotton, paper.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Product.pattern","title":"pattern  <code>instance-attribute</code>","text":"<pre><code>pattern: Optional[Union[DefinedTerm, Text]]\n</code></pre> <p>A pattern that something has, for example 'polka dot', 'striped', 'Canadian flag'.</p> <p>Values are typically expressed as text, although links to controlled value schemes are also supported.</p>"},{"location":"reference/metador_core/schema/common/schemaorg/#metador_core.schema.common.schemaorg.Product.additionalProperty","title":"additionalProperty  <code>instance-attribute</code>","text":"<pre><code>additionalProperty: Optional[List[PropertyValue]]\n</code></pre> <p>A property-value pair representing an additional characteristic of the entity, e.g. a product feature or another characteristic for which there is no matching property in schema.org.</p>"},{"location":"reference/metador_core/schema/examples/","title":"examples","text":"<p>Example schemas used for tutorials and other didactic materials.</p>"},{"location":"reference/metador_core/schema/examples/matsci/","title":"matsci","text":"<p>Example schema for Metador in Materials Science tutorial.</p> <p>The metadata modelling here is intentionally left simple and minimal.</p>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.Material","title":"Material","text":"<p>             Bases: <code>MetadataSchema</code></p> <p>A material that was used in an experiment or simulation.</p> Source code in <code>src/metador_core/schema/examples/matsci.py</code> <pre><code>class Material(MetadataSchema):\n\"\"\"A material that was used in an experiment or simulation.\"\"\"\n\n    class Plugin:\n        name = \"example.matsci.material\"\n        version = (0, 1, 0)\n\n    materialName: schemaorg.Text\n\"\"\"The name of material.\"\"\"\n\n    chemicalComposition: Optional[schemaorg.Text]\n\"\"\"Chemical formula reflecting distribution of elements in the material.\"\"\"\n\n    density: Optional[PositiveFloat]\n\"\"\"The density of the material (in g/cm3).\"\"\"\n\n    crystalGrainType: Optional[Literal[\"single_crystal\", \"bi_crystal\", \"poly_crystal\"]]\n\"\"\"Type of a grain of a crystalline material.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.Material.materialName","title":"materialName  <code>instance-attribute</code>","text":"<pre><code>materialName: schemaorg.Text\n</code></pre> <p>The name of material.</p>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.Material.chemicalComposition","title":"chemicalComposition  <code>instance-attribute</code>","text":"<pre><code>chemicalComposition: Optional[schemaorg.Text]\n</code></pre> <p>Chemical formula reflecting distribution of elements in the material.</p>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.Material.density","title":"density  <code>instance-attribute</code>","text":"<pre><code>density: Optional[PositiveFloat]\n</code></pre> <p>The density of the material (in g/cm3).</p>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.Material.crystalGrainType","title":"crystalGrainType  <code>instance-attribute</code>","text":"<pre><code>crystalGrainType: Optional[\n    Literal[\"single_crystal\", \"bi_crystal\", \"poly_crystal\"]\n]\n</code></pre> <p>Type of a grain of a crystalline material.</p>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.Instrument","title":"Instrument","text":"<p>             Bases: <code>MetadataSchema</code></p> <p>Metadata of the instrument used to generate some data.</p> Source code in <code>src/metador_core/schema/examples/matsci.py</code> <pre><code>class Instrument(MetadataSchema):\n\"\"\"Metadata of the instrument used to generate some data.\"\"\"\n\n    class Plugin:\n        name = \"example.matsci.instrument\"\n        version = (0, 1, 0)\n\n    instrumentName: schemaorg.Text\n    instrumentModel: schemaorg.Text\n    instrumentManufacturer: Optional[rocrate.Organization]\n</code></pre>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.Specimen","title":"Specimen","text":"<p>             Bases: <code>MetadataSchema</code></p> <p>Metadata of the specimen tested in an experiment.</p> Source code in <code>src/metador_core/schema/examples/matsci.py</code> <pre><code>class Specimen(MetadataSchema):\n\"\"\"Metadata of the specimen tested in an experiment.\"\"\"\n\n    class Plugin:\n        name = \"example.matsci.specimen\"\n        version = (0, 1, 0)\n\n    diameter: PositiveFloat\n\"\"\"The diameter of the specimen (in mm).\"\"\"\n\n    gaugeLength: PositiveFloat\n\"\"\"The gauge length of the specimen (in mm).\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.Specimen.diameter","title":"diameter  <code>instance-attribute</code>","text":"<pre><code>diameter: PositiveFloat\n</code></pre> <p>The diameter of the specimen (in mm).</p>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.Specimen.gaugeLength","title":"gaugeLength  <code>instance-attribute</code>","text":"<pre><code>gaugeLength: PositiveFloat\n</code></pre> <p>The gauge length of the specimen (in mm).</p>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.Method","title":"Method","text":"<p>             Bases: <code>MetadataSchema</code></p> <p>A method used to conduct a materials science experiment or simulation.</p> Source code in <code>src/metador_core/schema/examples/matsci.py</code> <pre><code>class Method(MetadataSchema):\n\"\"\"A method used to conduct a materials science experiment or simulation.\"\"\"\n\n    class Plugin:\n        name = \"example.matsci.method\"\n        version = (0, 1, 0)\n\n    methodType: Optional[Literal[\"tensile_test\", \"other\"]]\n\"\"\"Type of method used to obtain the resulting data.\"\"\"\n\n    instrument: Instrument\n    specimen: Specimen\n</code></pre>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.Method.methodType","title":"methodType  <code>instance-attribute</code>","text":"<pre><code>methodType: Optional[Literal['tensile_test', 'other']]\n</code></pre> <p>Type of method used to obtain the resulting data.</p>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.MatsciFileInfo","title":"MatsciFileInfo","text":"<p>             Bases: <code>CreativeWork</code></p> <p>Metadata for a file with data obtained from research in Materials Science.</p> <p>Note: To state authors or contributors, use the <code>core.person</code> schema.</p> Source code in <code>src/metador_core/schema/examples/matsci.py</code> <pre><code>@make_mandatory(\"abstract\", \"dateCreated\", \"author\")\nclass MatsciFileInfo(schemaorg.CreativeWork):\n\"\"\"Metadata for a file with data obtained from research in Materials Science.\n\n    **Note:** To state authors or contributors, use the `core.person` schema.\n    \"\"\"\n\n    class Plugin:\n        name = \"example.matsci.info\"\n        version = (0, 1, 0)\n\n    material: Annotated[List[Material], Field(default_factory=lambda: [], min_items=1)]\n\"\"\"Physical material associated with the file.\"\"\"\n\n    method: Annotated[List[Method], Field(default_factory=lambda: [])]\n\"\"\"Materials Science method associated with the file.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.MatsciFileInfo.material","title":"material  <code>instance-attribute</code>","text":"<pre><code>material: Annotated[\n    List[Material],\n    Field(default_factory=lambda: [], min_items=1),\n]\n</code></pre> <p>Physical material associated with the file.</p>"},{"location":"reference/metador_core/schema/examples/matsci/#metador_core.schema.examples.matsci.MatsciFileInfo.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method: Annotated[\n    List[Method], Field(default_factory=lambda: [])\n]\n</code></pre> <p>Materials Science method associated with the file.</p>"},{"location":"reference/metador_core/util/","title":"util","text":""},{"location":"reference/metador_core/util/#metador_core.util.eprint","title":"eprint","text":"<pre><code>eprint(*args, **kwargs)\n</code></pre> <p>Print to error stream.</p> Source code in <code>src/metador_core/util/__init__.py</code> <pre><code>def eprint(*args, **kwargs):\n\"\"\"Print to error stream.\"\"\"\n    print(*args, file=sys.stderr, **kwargs)\n</code></pre>"},{"location":"reference/metador_core/util/#metador_core.util.drop","title":"drop","text":"<pre><code>drop(n: int, it: Iterable)\n</code></pre> <p>Drop fixed number of elements from iterator.</p> Source code in <code>src/metador_core/util/__init__.py</code> <pre><code>def drop(n: int, it: Iterable):\n\"\"\"Drop fixed number of elements from iterator.\"\"\"\n    return (x for i, x in enumerate(it) if i &gt;= n)\n</code></pre>"},{"location":"reference/metador_core/util/#metador_core.util.is_public_name","title":"is_public_name","text":"<pre><code>is_public_name(n: str)\n</code></pre> <p>Return whether a name is public (does not start with _).</p> Source code in <code>src/metador_core/util/__init__.py</code> <pre><code>def is_public_name(n: str):\n\"\"\"Return whether a name is public (does not start with _).\"\"\"\n    return n[0] != \"_\"\n</code></pre>"},{"location":"reference/metador_core/util/#metador_core.util.pythonize_name","title":"pythonize_name","text":"<pre><code>pythonize_name(name: str)\n</code></pre> <p>Sanitize a string to be a valid Python variable name.</p> Source code in <code>src/metador_core/util/__init__.py</code> <pre><code>def pythonize_name(name: str):\n\"\"\"Sanitize a string to be a valid Python variable name.\"\"\"\n    return re.sub(r\"\\W|^(?=\\d)\", \"_\", \" \".join(name.split()))\n</code></pre>"},{"location":"reference/metador_core/util/diff/","title":"diff","text":""},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DiffNode","title":"DiffNode","text":"<p>             Bases: <code>BaseModel</code></p> <p>Node representing a file, symlink or directory in a diff.</p> Source code in <code>src/metador_core/util/diff.py</code> <pre><code>class DiffNode(BaseModel):\n\"\"\"Node representing a file, symlink or directory in a diff.\"\"\"\n\n    class Status(str, Enum):\n\"\"\"Change that happened to a DiffNode.\"\"\"\n\n        removed = \"-\"\n        modified = \"~\"\n        added = \"+\"\n        unchanged = \"0\"\n\n    class ObjType(str, Enum):\n\"\"\"Entities represented in a DiffNode.\"\"\"\n\n        directory = \"d\"\n        file = \"f\"\n        symlink = \"s\"\n\n    path: Path\n\"\"\"Location represented by this node.\"\"\"\n\n    prev: Union[None, str, Dict[str, Any]]\n\"\"\"Previous entity at this location.\"\"\"\n\n    curr: Union[None, str, Dict[str, Any]]\n\"\"\"Current entity at this location.\"\"\"\n\n    removed: Dict[Path, DiffNode] = {}\n\"\"\"Deleted files and subdirectories.\"\"\"\n\n    modified: Dict[Path, DiffNode] = {}\n\"\"\"Modified or replaced files and subdirectories.\"\"\"\n\n    added: Dict[Path, DiffNode] = {}\n\"\"\"New files and subdirectories.\"\"\"\n\n    def _type(self, entity) -&gt; Optional[DiffNode.ObjType]:\n        if isinstance(entity, dict):\n            return DiffNode.ObjType.directory\n        elif entity:\n            if entity.find(\"symlink:\") == 0:\n                return DiffNode.ObjType.symlink\n            else:\n                return DiffNode.ObjType.file\n        return None\n\n    @property\n    def prev_type(self) -&gt; Optional[DiffNode.ObjType]:\n        return self._type(self.prev)\n\n    @property\n    def curr_type(self) -&gt; Optional[DiffNode.ObjType]:\n        return self._type(self.curr)\n\n    def children(self) -&gt; Iterable[DiffNode]:\n\"\"\"Return immediate children in no specific order.\"\"\"\n        return itertools.chain(\n            *(x.values() for x in [self.removed, self.modified, self.added])\n        )\n\n    def nodes(self) -&gt; List[DiffNode]:\n\"\"\"Return list of all nodes in this diff.\n\n        The order will be recursively:\n        removed children, modified children, itself, then added children.\n        Within the same category the children are sorted in alphabetical order.\n        \"\"\"\n        ret = []\n        buckets = [self.removed, self.modified, None, self.added]\n        for b in buckets:\n            if b is None:\n                ret.append(self)\n            else:\n                for v in sorted(b.values(), key=lambda x: x.path):\n                    ret += v.nodes()\n        return ret\n\n    def status(self) -&gt; DiffNode.Status:\n\"\"\"Check the given path (which is assumed to be relative to this diff node).\n\n        Returns whether the path was added, removed or modified, respectively.\n        \"\"\"\n        if self.prev is None:\n            return DiffNode.Status.added\n        elif self.curr is None:\n            return DiffNode.Status.removed\n        else:  # we can assume self.prev != self.curr\n            return DiffNode.Status.modified\n\n    @classmethod\n    def compare(\n        cls,\n        prev: Optional[DirHashsums],\n        curr: Optional[DirHashsums],\n        path: Path,\n    ) -&gt; Optional[DiffNode]:\n\"\"\"Compare two nested file and directory hashsum dicts.\n\n        Returns None if no difference is found, otherwise a DiffNode tree\n        containing only the additions, removals and changes.\n        \"\"\"\n        ret = cls(path=path, prev=prev, curr=curr)\n        if not isinstance(prev, dict) and not isinstance(curr, dict):\n            if prev == curr:\n                return None  # same (non-)file -&gt; no diff\n            else:\n                return ret  # indicates that a change happened\n\n        if (prev is None or isinstance(prev, str)) and isinstance(curr, dict):\n            # file -&gt; dir: everything inside \"added\"\n            for k, v in curr.items():\n                kpath = ret.path / k\n                d = cls.compare(None, v, kpath)\n                assert d is not None\n                ret.added[kpath] = d\n            return ret\n\n        if isinstance(prev, dict) and (curr is None or isinstance(curr, str)):\n            # dir -&gt; file: everything inside \"removed\"\n            for k, v in prev.items():\n                kpath = ret.path / k\n                d = cls.compare(v, None, kpath)\n                assert d is not None\n                ret.removed[kpath] = d\n            return ret\n\n        assert isinstance(prev, dict) and isinstance(curr, dict)\n        # two directories -&gt; compare\n        prev_keys = set(prev.keys())\n        curr_keys = set(curr.keys())\n\n        added = curr_keys - prev_keys\n        removed = prev_keys - curr_keys\n        intersection = (prev_keys | curr_keys) - added - removed\n\n        for k in added:  # added in curr\n            kpath = ret.path / k\n            d = cls.compare(None, curr[k], kpath)\n            assert d is not None\n            ret.added[kpath] = d\n        for k in removed:  # removed in curr\n            kpath = ret.path / k\n            d = cls.compare(prev[k], None, kpath)\n            assert d is not None\n            ret.removed[kpath] = d\n        for k in intersection:  # changed in curr\n            kpath = ret.path / k\n            diff = cls.compare(prev[k], curr[k], kpath)\n            if diff is not None:  # add child if there is a difference\n                ret.modified[kpath] = diff\n\n        # all children same -&gt; directories same\n        same_dir = not ret.added and not ret.removed and not ret.modified\n        if same_dir:\n            return None\n\n        # return dir node with the changes\n        return ret\n</code></pre>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DiffNode.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path: Path\n</code></pre> <p>Location represented by this node.</p>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DiffNode.prev","title":"prev  <code>instance-attribute</code>","text":"<pre><code>prev: Union[None, str, Dict[str, Any]]\n</code></pre> <p>Previous entity at this location.</p>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DiffNode.curr","title":"curr  <code>instance-attribute</code>","text":"<pre><code>curr: Union[None, str, Dict[str, Any]]\n</code></pre> <p>Current entity at this location.</p>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DiffNode.removed","title":"removed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>removed: Dict[Path, DiffNode] = {}\n</code></pre> <p>Deleted files and subdirectories.</p>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DiffNode.modified","title":"modified  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>modified: Dict[Path, DiffNode] = {}\n</code></pre> <p>Modified or replaced files and subdirectories.</p>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DiffNode.added","title":"added  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>added: Dict[Path, DiffNode] = {}\n</code></pre> <p>New files and subdirectories.</p>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DiffNode.Status","title":"Status","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Change that happened to a DiffNode.</p> Source code in <code>src/metador_core/util/diff.py</code> <pre><code>class Status(str, Enum):\n\"\"\"Change that happened to a DiffNode.\"\"\"\n\n    removed = \"-\"\n    modified = \"~\"\n    added = \"+\"\n    unchanged = \"0\"\n</code></pre>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DiffNode.ObjType","title":"ObjType","text":"<p>             Bases: <code>str</code>, <code>Enum</code></p> <p>Entities represented in a DiffNode.</p> Source code in <code>src/metador_core/util/diff.py</code> <pre><code>class ObjType(str, Enum):\n\"\"\"Entities represented in a DiffNode.\"\"\"\n\n    directory = \"d\"\n    file = \"f\"\n    symlink = \"s\"\n</code></pre>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DiffNode.children","title":"children","text":"<pre><code>children() -&gt; Iterable[DiffNode]\n</code></pre> <p>Return immediate children in no specific order.</p> Source code in <code>src/metador_core/util/diff.py</code> <pre><code>def children(self) -&gt; Iterable[DiffNode]:\n\"\"\"Return immediate children in no specific order.\"\"\"\n    return itertools.chain(\n        *(x.values() for x in [self.removed, self.modified, self.added])\n    )\n</code></pre>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DiffNode.nodes","title":"nodes","text":"<pre><code>nodes() -&gt; List[DiffNode]\n</code></pre> <p>Return list of all nodes in this diff.</p> <p>The order will be recursively: removed children, modified children, itself, then added children. Within the same category the children are sorted in alphabetical order.</p> Source code in <code>src/metador_core/util/diff.py</code> <pre><code>def nodes(self) -&gt; List[DiffNode]:\n\"\"\"Return list of all nodes in this diff.\n\n    The order will be recursively:\n    removed children, modified children, itself, then added children.\n    Within the same category the children are sorted in alphabetical order.\n    \"\"\"\n    ret = []\n    buckets = [self.removed, self.modified, None, self.added]\n    for b in buckets:\n        if b is None:\n            ret.append(self)\n        else:\n            for v in sorted(b.values(), key=lambda x: x.path):\n                ret += v.nodes()\n    return ret\n</code></pre>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DiffNode.status","title":"status","text":"<pre><code>status() -&gt; DiffNode.Status\n</code></pre> <p>Check the given path (which is assumed to be relative to this diff node).</p> <p>Returns whether the path was added, removed or modified, respectively.</p> Source code in <code>src/metador_core/util/diff.py</code> <pre><code>def status(self) -&gt; DiffNode.Status:\n\"\"\"Check the given path (which is assumed to be relative to this diff node).\n\n    Returns whether the path was added, removed or modified, respectively.\n    \"\"\"\n    if self.prev is None:\n        return DiffNode.Status.added\n    elif self.curr is None:\n        return DiffNode.Status.removed\n    else:  # we can assume self.prev != self.curr\n        return DiffNode.Status.modified\n</code></pre>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DiffNode.compare","title":"compare  <code>classmethod</code>","text":"<pre><code>compare(\n    prev: Optional[DirHashsums],\n    curr: Optional[DirHashsums],\n    path: Path,\n) -&gt; Optional[DiffNode]\n</code></pre> <p>Compare two nested file and directory hashsum dicts.</p> <p>Returns None if no difference is found, otherwise a DiffNode tree containing only the additions, removals and changes.</p> Source code in <code>src/metador_core/util/diff.py</code> <pre><code>@classmethod\ndef compare(\n    cls,\n    prev: Optional[DirHashsums],\n    curr: Optional[DirHashsums],\n    path: Path,\n) -&gt; Optional[DiffNode]:\n\"\"\"Compare two nested file and directory hashsum dicts.\n\n    Returns None if no difference is found, otherwise a DiffNode tree\n    containing only the additions, removals and changes.\n    \"\"\"\n    ret = cls(path=path, prev=prev, curr=curr)\n    if not isinstance(prev, dict) and not isinstance(curr, dict):\n        if prev == curr:\n            return None  # same (non-)file -&gt; no diff\n        else:\n            return ret  # indicates that a change happened\n\n    if (prev is None or isinstance(prev, str)) and isinstance(curr, dict):\n        # file -&gt; dir: everything inside \"added\"\n        for k, v in curr.items():\n            kpath = ret.path / k\n            d = cls.compare(None, v, kpath)\n            assert d is not None\n            ret.added[kpath] = d\n        return ret\n\n    if isinstance(prev, dict) and (curr is None or isinstance(curr, str)):\n        # dir -&gt; file: everything inside \"removed\"\n        for k, v in prev.items():\n            kpath = ret.path / k\n            d = cls.compare(v, None, kpath)\n            assert d is not None\n            ret.removed[kpath] = d\n        return ret\n\n    assert isinstance(prev, dict) and isinstance(curr, dict)\n    # two directories -&gt; compare\n    prev_keys = set(prev.keys())\n    curr_keys = set(curr.keys())\n\n    added = curr_keys - prev_keys\n    removed = prev_keys - curr_keys\n    intersection = (prev_keys | curr_keys) - added - removed\n\n    for k in added:  # added in curr\n        kpath = ret.path / k\n        d = cls.compare(None, curr[k], kpath)\n        assert d is not None\n        ret.added[kpath] = d\n    for k in removed:  # removed in curr\n        kpath = ret.path / k\n        d = cls.compare(prev[k], None, kpath)\n        assert d is not None\n        ret.removed[kpath] = d\n    for k in intersection:  # changed in curr\n        kpath = ret.path / k\n        diff = cls.compare(prev[k], curr[k], kpath)\n        if diff is not None:  # add child if there is a difference\n            ret.modified[kpath] = diff\n\n    # all children same -&gt; directories same\n    same_dir = not ret.added and not ret.removed and not ret.modified\n    if same_dir:\n        return None\n\n    # return dir node with the changes\n    return ret\n</code></pre>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DirDiff","title":"DirDiff","text":"<p>Interface to directory diffs based on comparing <code>DirHashsums</code>.</p> <p>An instance represents a change at the path.</p> <p>Granular change inspection is accessible through the provided methods.</p> <p>Typically, you will want to capture <code>dir_hashsums</code> of the same <code>dir</code> at two points in time and will be interested in the dict <code>DirDiff.compare(prev, curr).annotate(dir)</code>.</p> Source code in <code>src/metador_core/util/diff.py</code> <pre><code>class DirDiff:\n\"\"\"Interface to directory diffs based on comparing `DirHashsums`.\n\n    An instance represents a change at the path.\n\n    Granular change inspection is accessible through the provided methods.\n\n    Typically, you will want to capture `dir_hashsums` of the same `dir`\n    at two points in time and will be interested in the dict\n    `DirDiff.compare(prev, curr).annotate(dir)`.\n    \"\"\"\n\n    _diff_root: Optional[DiffNode]\n\n    @property\n    def is_empty(self):\n        return self._diff_root is None\n\n    @classmethod\n    def compare(cls, prev: DirHashsums, curr: DirHashsums) -&gt; DirDiff:\n\"\"\"Compute a DirDiff based on two DirHashsum trees.\n\n        To be meaningful, `prev` and `curr` should be trees obtained from the\n        same directory at two points in time.\n        \"\"\"\n        ret = cls.__new__(cls)\n        ret._diff_root = DiffNode.compare(prev, curr, Path(\"\"))\n        return ret\n\n    def get(self, path: Path) -&gt; Optional[DiffNode]:\n        path = Path(path)  # if it was a str\n        assert not path.is_absolute()\n        if self._diff_root is None:\n            return None\n        curr: DiffNode = self._diff_root\n        prefixes = [path] + list(path.parents)\n        prefixes.pop()  # drop '.'\n        while prefixes:\n            path = prefixes.pop()\n            node: Optional[DiffNode] = next(\n                (x for x in curr.children() if x.path == path), None\n            )\n            if node is None:\n                return None\n            curr = node\n        return curr\n\n    def status(self, node: Optional[DiffNode]) -&gt; DiffNode.Status:\n\"\"\"Return the type of change that happened to a diff node.\n\n        Wraps `node.status()` additionally covering the case that `node` is `None`.\n        Useful when processing possible path nodes returned by `annotate`.\n        \"\"\"\n        if node is None:\n            return DiffNode.Status.unchanged\n        return node.status()\n\n    def annotate(self, base_dir: Path) -&gt; Dict[Path, Optional[DiffNode]]:\n\"\"\"Return a dict of path -&gt; status mappings based on passed directory.\n\n        The keys are all paths that exist in this diff object as well as\n        all paths that currently exist in the passed `base_dir`.\n        The keys will all have `base_dir` as prefix.\n\n        The values will be the corresponding DiffNodes, if there is a change.\n        If there was no change at that path, the value will be None.\n\n        The iteration order will be such that for each entity, first the removed\n        children, then changed subentities, then the entity itself, and finally\n        added subentities are listed. This is useful for updating operations\n        of directories and ensures that children can be deleted before their parents\n        and parents can be created before their children.\n        \"\"\"\n        if self._diff_root is None:\n            return {}\n\n        nodes = self._diff_root.nodes()  # nodes of all paths in the diff\n\n        path_nodes = {node.path: node for node in nodes}\n        # paths in base_dir, but not in the diff\n        missing = sorted(set(dir_paths(base_dir)) - set(path_nodes.keys()))\n\n        # construct result, keeping correct order and prepending base_dir\n        ret: Dict[Path, Optional[DiffNode]] = {\n            base_dir / str(k): v for k, v in path_nodes.items()\n        }\n        for path in missing:\n            ret[base_dir / str(path)] = None\n        return ret\n</code></pre>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DirDiff.compare","title":"compare  <code>classmethod</code>","text":"<pre><code>compare(prev: DirHashsums, curr: DirHashsums) -&gt; DirDiff\n</code></pre> <p>Compute a DirDiff based on two DirHashsum trees.</p> <p>To be meaningful, <code>prev</code> and <code>curr</code> should be trees obtained from the same directory at two points in time.</p> Source code in <code>src/metador_core/util/diff.py</code> <pre><code>@classmethod\ndef compare(cls, prev: DirHashsums, curr: DirHashsums) -&gt; DirDiff:\n\"\"\"Compute a DirDiff based on two DirHashsum trees.\n\n    To be meaningful, `prev` and `curr` should be trees obtained from the\n    same directory at two points in time.\n    \"\"\"\n    ret = cls.__new__(cls)\n    ret._diff_root = DiffNode.compare(prev, curr, Path(\"\"))\n    return ret\n</code></pre>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DirDiff.status","title":"status","text":"<pre><code>status(node: Optional[DiffNode]) -&gt; DiffNode.Status\n</code></pre> <p>Return the type of change that happened to a diff node.</p> <p>Wraps <code>node.status()</code> additionally covering the case that <code>node</code> is <code>None</code>. Useful when processing possible path nodes returned by <code>annotate</code>.</p> Source code in <code>src/metador_core/util/diff.py</code> <pre><code>def status(self, node: Optional[DiffNode]) -&gt; DiffNode.Status:\n\"\"\"Return the type of change that happened to a diff node.\n\n    Wraps `node.status()` additionally covering the case that `node` is `None`.\n    Useful when processing possible path nodes returned by `annotate`.\n    \"\"\"\n    if node is None:\n        return DiffNode.Status.unchanged\n    return node.status()\n</code></pre>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.DirDiff.annotate","title":"annotate","text":"<pre><code>annotate(base_dir: Path) -&gt; Dict[Path, Optional[DiffNode]]\n</code></pre> <p>Return a dict of path -&gt; status mappings based on passed directory.</p> <p>The keys are all paths that exist in this diff object as well as all paths that currently exist in the passed <code>base_dir</code>. The keys will all have <code>base_dir</code> as prefix.</p> <p>The values will be the corresponding DiffNodes, if there is a change. If there was no change at that path, the value will be None.</p> <p>The iteration order will be such that for each entity, first the removed children, then changed subentities, then the entity itself, and finally added subentities are listed. This is useful for updating operations of directories and ensures that children can be deleted before their parents and parents can be created before their children.</p> Source code in <code>src/metador_core/util/diff.py</code> <pre><code>def annotate(self, base_dir: Path) -&gt; Dict[Path, Optional[DiffNode]]:\n\"\"\"Return a dict of path -&gt; status mappings based on passed directory.\n\n    The keys are all paths that exist in this diff object as well as\n    all paths that currently exist in the passed `base_dir`.\n    The keys will all have `base_dir` as prefix.\n\n    The values will be the corresponding DiffNodes, if there is a change.\n    If there was no change at that path, the value will be None.\n\n    The iteration order will be such that for each entity, first the removed\n    children, then changed subentities, then the entity itself, and finally\n    added subentities are listed. This is useful for updating operations\n    of directories and ensures that children can be deleted before their parents\n    and parents can be created before their children.\n    \"\"\"\n    if self._diff_root is None:\n        return {}\n\n    nodes = self._diff_root.nodes()  # nodes of all paths in the diff\n\n    path_nodes = {node.path: node for node in nodes}\n    # paths in base_dir, but not in the diff\n    missing = sorted(set(dir_paths(base_dir)) - set(path_nodes.keys()))\n\n    # construct result, keeping correct order and prepending base_dir\n    ret: Dict[Path, Optional[DiffNode]] = {\n        base_dir / str(k): v for k, v in path_nodes.items()\n    }\n    for path in missing:\n        ret[base_dir / str(path)] = None\n    return ret\n</code></pre>"},{"location":"reference/metador_core/util/diff/#metador_core.util.diff.dir_paths","title":"dir_paths","text":"<pre><code>dir_paths(base_dir: Path)\n</code></pre> <p>Recursively list all paths in given directory, relative to itself.</p> Source code in <code>src/metador_core/util/diff.py</code> <pre><code>def dir_paths(base_dir: Path):\n\"\"\"Recursively list all paths in given directory, relative to itself.\"\"\"\n    return map(lambda p: p.relative_to(base_dir), sorted(base_dir.rglob(\"*\")))\n</code></pre>"},{"location":"reference/metador_core/util/enum/","title":"enum","text":""},{"location":"reference/metador_core/util/enum/#metador_core.util.enum.NestedEnumMeta","title":"NestedEnumMeta","text":"<p>             Bases: <code>EnumMeta</code></p> <p>Enum subclass for Enums generated from a semantic taxonomy.</p> <p>Approach is to collect values and treat them as sets.</p> Source code in <code>src/metador_core/util/enum.py</code> <pre><code>class NestedEnumMeta(EnumMeta):\n\"\"\"Enum subclass for Enums generated from a semantic taxonomy.\n\n    Approach is to collect values and treat them as sets.\n    \"\"\"\n\n    __self__: Optional[Union[int, str]] = None\n    __values__: Set[Any] = set()\n    __nested__: Dict[str, NestedEnumMeta] = {}\n\n    name: str\n    value: Union[int, str]\n\n    def __call__(metacls, clsname, dct, *args, **kwargs):\n        self = kwargs.pop(\"self\", None)\n        mod_dct = {}\n        nested = {}\n        for k, v in dct.items():\n            if isinstance(v, NestedEnumMeta):\n                if v.__self__ is None:\n                    raise ValueError(f\"Nested enum {v} must have a self value!\")\n                nested[k] = v\n            else:\n                mod_dct[k] = v\n\n        ret = super().__call__(clsname, mod_dct, *args, **kwargs)\n\n        ret.__self__ = self\n        nested_values = set.union(\n            set(), *map(lambda x: {x.__self__}.union(x.__values__), nested.values())\n        )\n        ret.__values__ = set(map(lambda x: x.value, iter(ret))).union(nested_values)\n        ret.__nested__ = nested\n        for name, nst in nested.items():\n            setattr(ret, name, nst)\n\n        return ret\n\n    def __contains__(self, other):\n        print(other, \"in\", self, \"?\")\n        if isinstance(other, NestedEnum):\n            return self.__contains__(other.value)\n        if isinstance(other, type) and issubclass(other.__class__, NestedEnumMeta):\n            return self.__contains__(other.__self__)\n        # lookup plain value\n        return other in self.__values__\n\n    def __iter__(self):\n        return itertools.chain(\n            super().__iter__(),\n            *(itertools.chain(iter((x,)), iter(x)) for x in self.__nested__.values()),\n        )\n\n    def __dir__(self):\n        return itertools.chain(super().__dir__(), self.__nested__.keys())\n\n    def __repr__(self):\n        if self.__self__ is not None:\n            return f\"&lt;enum {self.__name__}: {self.__self__}&gt;\"\n        else:\n            return super().__repr__()\n</code></pre>"},{"location":"reference/metador_core/util/enum/#metador_core.util.enum.NestedEnum","title":"NestedEnum","text":"<p>             Bases: <code>Enum</code></p> <p>Base class for hierarchical enumerations.</p> Source code in <code>src/metador_core/util/enum.py</code> <pre><code>class NestedEnum(Enum, metaclass=NestedEnumMeta):\n\"\"\"Base class for hierarchical enumerations.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/util/hashsums/","title":"hashsums","text":""},{"location":"reference/metador_core/util/hashsums/#metador_core.util.hashsums.DEF_HASH_ALG","title":"DEF_HASH_ALG  <code>module-attribute</code>","text":"<pre><code>DEF_HASH_ALG = 'sha256'\n</code></pre> <p>Algorithm to use and string to prepend to a resulting hashsum.</p>"},{"location":"reference/metador_core/util/hashsums/#metador_core.util.hashsums.DirHashsums","title":"DirHashsums  <code>module-attribute</code>","text":"<pre><code>DirHashsums = Dict[str, Any]\n</code></pre> <p>Nested dict representing a directory.</p> <p>str values represent files (checksum) or symlinks (target path), dict values represent sub-directories.</p>"},{"location":"reference/metador_core/util/hashsums/#metador_core.util.hashsums.hashsum","title":"hashsum","text":"<pre><code>hashsum(data: Union[bytes, BinaryIO], alg: str)\n</code></pre> <p>Compute hashsum from given binary file stream using selected algorithm.</p> Source code in <code>src/metador_core/util/hashsums.py</code> <pre><code>def hashsum(data: Union[bytes, BinaryIO], alg: str):\n\"\"\"Compute hashsum from given binary file stream using selected algorithm.\"\"\"\n    if isinstance(data, bytes):\n        data = BytesIO(data)\n    try:\n        h = _hash_alg[alg]()\n    except KeyError:\n        raise ValueError(f\"Unsupported hashsum: {alg}\")\n\n    while True:\n        chunk = data.read(h.block_size)\n        if not chunk:\n            break\n        h.update(chunk)\n\n    return h.hexdigest()\n</code></pre>"},{"location":"reference/metador_core/util/hashsums/#metador_core.util.hashsums.qualified_hashsum","title":"qualified_hashsum","text":"<pre><code>qualified_hashsum(\n    data: Union[bytes, BinaryIO], alg: str = DEF_HASH_ALG\n)\n</code></pre> <p>Like hashsum, but prepends the algorithm to the string.</p> Source code in <code>src/metador_core/util/hashsums.py</code> <pre><code>def qualified_hashsum(data: Union[bytes, BinaryIO], alg: str = DEF_HASH_ALG):\n\"\"\"Like hashsum, but prepends the algorithm to the string.\"\"\"\n    return f\"{alg}:{hashsum(data, alg)}\"\n</code></pre>"},{"location":"reference/metador_core/util/hashsums/#metador_core.util.hashsums.rel_symlink","title":"rel_symlink","text":"<pre><code>rel_symlink(base: Path, dir: Path) -&gt; Optional[Path]\n</code></pre> <p>From base path and a symlink path, normalize it to be relative to base.</p> <p>Mainly used to eliminate .. in paths.</p> <p>If path points outside base, returns None.</p> Source code in <code>src/metador_core/util/hashsums.py</code> <pre><code>def rel_symlink(base: Path, dir: Path) -&gt; Optional[Path]:\n\"\"\"From base path and a symlink path, normalize it to be relative to base.\n\n    Mainly used to eliminate .. in paths.\n\n    If path points outside base, returns None.\n    \"\"\"\n    path = dir.parent / os.readlink(str(dir))\n    try:\n        return path.resolve().relative_to(base.resolve())\n    except ValueError:\n        return None  # link points outside of base directory\n</code></pre>"},{"location":"reference/metador_core/util/hashsums/#metador_core.util.hashsums.dir_hashsums","title":"dir_hashsums","text":"<pre><code>dir_hashsums(\n    dir: Path, alg: str = DEF_HASH_ALG\n) -&gt; DirHashsums\n</code></pre> <p>Return hashsums of all files.</p> <p>Resulting paths are relative to the provided <code>dir</code>.</p> <p>In-directory symlinks are treated like files and the target is stored instead of computing a checksum.</p> <p>Out-of-directory symlinks are not allowed.</p> Source code in <code>src/metador_core/util/hashsums.py</code> <pre><code>def dir_hashsums(dir: Path, alg: str = DEF_HASH_ALG) -&gt; DirHashsums:\n\"\"\"Return hashsums of all files.\n\n    Resulting paths are relative to the provided `dir`.\n\n    In-directory symlinks are treated like files and the target is stored\n    instead of computing a checksum.\n\n    Out-of-directory symlinks are not allowed.\n    \"\"\"\n    ret: Dict[str, Any] = {}\n    for path in dir.rglob(\"*\"):\n        is_file, is_sym = path.is_file(), path.is_symlink()\n        relpath = path.relative_to(dir)\n\n        fname = None\n        val = \"\"\n\n        if is_file or is_sym:\n            fname = relpath.name\n            relpath = relpath.parent  # directory dicts to create = up to parent\n\n        if is_file:\n            val = file_hashsum(path, alg)  # value = hashsum\n        elif is_sym:\n            sym_trg = rel_symlink(dir, path)\n            if sym_trg is None:\n                raise ValueError(f\"Symlink inside '{dir}' points to the outside!\")\n            val = \"symlink:\" + str(sym_trg)  # value = symlink target\n\n        # create nested dicts, if not existing yet\n        curr = ret\n        for seg in str(relpath).split(\"/\"):\n            if seg == \".\":\n                continue\n            if seg not in curr:\n                curr[seg] = dict()\n            curr = curr[seg]\n        # store file hashsum or symlink target\n        if is_file or is_sym:\n            assert fname is not None\n            curr[fname] = val\n\n    return ret\n</code></pre>"},{"location":"reference/metador_core/util/models/","title":"models","text":""},{"location":"reference/metador_core/util/models/#metador_core.util.models.field_origins","title":"field_origins","text":"<pre><code>field_origins(\n    m: Type[BaseModel], name: str\n) -&gt; Iterator[Type[BaseModel]]\n</code></pre> <p>Return sequence of bases where the field type hint was defined / overridden.</p> Source code in <code>src/metador_core/util/models.py</code> <pre><code>def field_origins(m: Type[BaseModel], name: str) -&gt; Iterator[Type[BaseModel]]:\n\"\"\"Return sequence of bases where the field type hint was defined / overridden.\"\"\"\n    return (\n        b for b in m.__mro__ if issubclass(b, BaseModel) and name in get_annotations(b)\n    )\n</code></pre>"},{"location":"reference/metador_core/util/models/#metador_core.util.models.updated_fields","title":"updated_fields","text":"<pre><code>updated_fields(m: Type[BaseModel]) -&gt; Set[str]\n</code></pre> <p>Return subset of fields that are added or overridden by a new type hint.</p> Source code in <code>src/metador_core/util/models.py</code> <pre><code>def updated_fields(m: Type[BaseModel]) -&gt; Set[str]:\n\"\"\"Return subset of fields that are added or overridden by a new type hint.\"\"\"\n    return {n for n in m.__fields__.keys() if next(field_origins(m, n)) is m}\n</code></pre>"},{"location":"reference/metador_core/util/models/#metador_core.util.models.field_atomic_types","title":"field_atomic_types","text":"<pre><code>field_atomic_types(\n    mf: ModelField, *, bound: ModelField = object\n) -&gt; Iterator[Type]\n</code></pre> <p>Return sequence of nested atomic types in the hint of given field.</p> Source code in <code>src/metador_core/util/models.py</code> <pre><code>def field_atomic_types(mf: ModelField, *, bound=object) -&gt; Iterator[Type]:\n\"\"\"Return sequence of nested atomic types in the hint of given field.\"\"\"\n    return filter(is_subclass_of(bound), traverse_typehint(mf.type_))\n</code></pre>"},{"location":"reference/metador_core/util/models/#metador_core.util.models.atomic_types","title":"atomic_types","text":"<pre><code>atomic_types(\n    m: BaseModel, *, bound: BaseModel = object\n) -&gt; Dict[str, Set[Type]]\n</code></pre> <p>Return dict from field name to model classes referenced in the field definition.</p> <p>Parameters:</p> Name Type Description Default <code>m</code> <code>BaseModel</code> <p>Pydantic model</p> required <code>bound</code> <code>object</code> <p>If provided, will be used to filter results to contain only subclasses of the bound.</p> <code>object</code> Source code in <code>src/metador_core/util/models.py</code> <pre><code>def atomic_types(m: BaseModel, *, bound=object) -&gt; Dict[str, Set[Type]]:\n\"\"\"Return dict from field name to model classes referenced in the field definition.\n\n    Args:\n        m: Pydantic model\n        bound (object): If provided, will be used to filter results to\n            contain only subclasses of the bound.\n    \"\"\"\n    return {k: set(field_atomic_types(v, bound=bound)) for k, v in m.__fields__.items()}\n</code></pre>"},{"location":"reference/metador_core/util/models/#metador_core.util.models.field_parent_type","title":"field_parent_type","text":"<pre><code>field_parent_type(\n    m: Type[BaseModel], name: str\n) -&gt; Type[BaseModel]\n</code></pre> <p>Return type of field assigned in the next parent that provides a type hint.</p> Source code in <code>src/metador_core/util/models.py</code> <pre><code>def field_parent_type(m: Type[BaseModel], name: str) -&gt; Type[BaseModel]:\n\"\"\"Return type of field assigned in the next parent that provides a type hint.\"\"\"\n    b = next(filter(lambda x: x is not m, field_origins(m, name)), None)\n    if not b:\n        raise ValueError(f\"No base class of {m} defines a field called '{name}'!\")\n    return get_type_hints(b).get(name)\n</code></pre>"},{"location":"reference/metador_core/util/pytest/","title":"pytest","text":""},{"location":"reference/metador_core/util/pytest/#metador_core.util.pytest.random_hex","title":"random_hex","text":"<pre><code>random_hex(length: int) -&gt; str\n</code></pre> <p>Return random hex string of given length.</p> Source code in <code>src/metador_core/util/pytest.py</code> <pre><code>def random_hex(length: int) -&gt; str:\n\"\"\"Return random hex string of given length.\"\"\"\n    return secrets.token_hex(length // 2 + 1)[:length]\n</code></pre>"},{"location":"reference/metador_core/util/pytest/#metador_core.util.pytest.parameters","title":"parameters","text":"<pre><code>parameters(d, keys = None)\n</code></pre> <p>Expand parameter combinations from a nested dict into a list of tuples.</p> Source code in <code>src/metador_core/util/pytest.py</code> <pre><code>def parameters(d, keys=None):\n\"\"\"Expand parameter combinations from a nested dict into a list of tuples.\"\"\"\n    keys = keys or []\n    if isinstance(d, list):\n        return sum((parameters(y, keys) for y in d), [])\n    elif isinstance(d, dict):\n        return sum((parameters(v, keys + [k]) for k, v in d.items()), [])\n    else:\n        return [tuple(keys + [d])]\n</code></pre>"},{"location":"reference/metador_core/util/typing/","title":"typing","text":""},{"location":"reference/metador_core/util/typing/#metador_core.util.typing.TypeHint","title":"TypeHint  <code>module-attribute</code>","text":"<pre><code>TypeHint: Any\n</code></pre> <p>For documentation purposes - to mark type hint arguments.</p>"},{"location":"reference/metador_core/util/typing/#metador_core.util.typing.to38hint","title":"to38hint  <code>module-attribute</code>","text":"<pre><code>to38hint: Dict[Type, Any] = {\n    list: List,\n    set: Set,\n    dict: Dict,\n    type: Type,\n}\n</code></pre> <p>Type hint map for consistent behavior between python versions.</p>"},{"location":"reference/metador_core/util/typing/#metador_core.util.typing.traverse_typehint","title":"traverse_typehint  <code>module-attribute</code>","text":"<pre><code>traverse_typehint = make_tree_traversal(get_args)\n</code></pre> <p>Perform depth-first pre-order traversal of a type annotation.</p> <p>Parameters:</p> Name Type Description Default <code>th</code> <code>object</code> <p>type hint object to be traversed</p> required"},{"location":"reference/metador_core/util/typing/#metador_core.util.typing.get_type_hints","title":"get_type_hints","text":"<pre><code>get_type_hints(cls) -&gt; Mapping[str, Any]\n</code></pre> <p>Return type hints of this class.</p> Source code in <code>src/metador_core/util/typing.py</code> <pre><code>def get_type_hints(cls) -&gt; Mapping[str, Any]:\n\"\"\"Return type hints of this class.\"\"\"\n    return te.get_type_hints(cls, include_extras=True)\n</code></pre>"},{"location":"reference/metador_core/util/typing/#metador_core.util.typing.get_annotations","title":"get_annotations","text":"<pre><code>get_annotations(\n    cls, *, all: bool = False\n) -&gt; Mapping[str, Any]\n</code></pre> <p>Return (non-inherited) annotations (unparsed) of given class.</p> Source code in <code>src/metador_core/util/typing.py</code> <pre><code>def get_annotations(cls, *, all: bool = False) -&gt; Mapping[str, Any]:\n\"\"\"Return (non-inherited) annotations (unparsed) of given class.\"\"\"\n    if not all:\n        return cls.__dict__.get(\"__annotations__\", {})\n    return ChainMap(*(c.__dict__.get(\"__annotations__\", {}) for c in cls.__mro__))\n</code></pre>"},{"location":"reference/metador_core/util/typing/#metador_core.util.typing.make_literal","title":"make_literal","text":"<pre><code>make_literal(val)\n</code></pre> <p>Given a JSON object, return type that parses exactly that object.</p> <p>Note that dicts can have extra fields that will be ignored and that coercion between bool and int might happen.</p> <p>Sets and floats are not supported.</p> Source code in <code>src/metador_core/util/typing.py</code> <pre><code>def make_literal(val):\n\"\"\"Given a JSON object, return type that parses exactly that object.\n\n    Note that dicts can have extra fields that will be ignored\n    and that coercion between bool and int might happen.\n\n    Sets and floats are not supported.\n    \"\"\"\n    if val is None:\n        return type(None)\n    elif isinstance(val, (bool, int, str)):\n        return make_typehint(LIT, val)\n    elif issubclass(val.__class__, enum.Enum):\n        return make_typehint(LIT, val.value)\n    elif isinstance(val, (tuple, list)):\n        args = tuple(map(make_literal, val))\n        return make_typehint(TUP, *args)\n    elif isinstance(val, dict):\n        d = {k: make_literal(v) for k, v in val.items()}\n        # NOTE: the TypedDict must be from typing_extensions for 3.8!\n        return TypedDict(\"AnonConstDict\", d)  # type: ignore\n    raise ValueError(f\"Unsupported value: {val}\")\n</code></pre>"},{"location":"reference/metador_core/util/typing/#metador_core.util.typing.make_tree_traversal","title":"make_tree_traversal","text":"<pre><code>make_tree_traversal(succ_func: Callable[[Any], Iterable])\n</code></pre> <p>Return generator to traverse nodes of a tree-shaped object.</p> <p>Returned function has a boolean keyword argument <code>post_order</code>. If True, will emit the parent node after children instead of before.</p> <p>Parameters:</p> Name Type Description Default <code>succ_func</code> <code>Callable[[Any], Iterable]</code> <p>Function to be called on each node returning Iterable of children</p> required Source code in <code>src/metador_core/util/typing.py</code> <pre><code>def make_tree_traversal(succ_func: Callable[[Any], Iterable]):\n\"\"\"Return generator to traverse nodes of a tree-shaped object.\n\n    Returned function has a boolean keyword argument `post_order`.\n    If True, will emit the parent node after children instead of before.\n\n    Args:\n        succ_func: Function to be called on each node returning Iterable of children\n    \"\"\"\n\n    def traverse(obj, *, post_order: bool = False):\n        if not post_order:\n            yield obj\n        for t in succ_func(obj):\n            yield from traverse(t, post_order=post_order)\n        if post_order:\n            yield obj\n\n    return traverse\n</code></pre>"},{"location":"reference/metador_core/util/typing/#metador_core.util.typing.unoptional","title":"unoptional","text":"<pre><code>unoptional(th)\n</code></pre> <p>Return type hint that is not optional (if it was optional).</p> Source code in <code>src/metador_core/util/typing.py</code> <pre><code>def unoptional(th):\n\"\"\"Return type hint that is not optional (if it was optional).\"\"\"\n    if is_annotated(th):\n        # remove inner optional, preserve annotation\n        return make_typehint(th, unoptional(get_args(th)[0]))\n\n    if not is_union(th):\n        # all optionals are actually unions -&gt; nothing to do\n        return th\n\n    # filter out NoneType from the Union arguments\n    args = tuple(filter(lambda h: not is_nonetype(h), get_args(th)))\n    if len(args) == 1:\n        # not a union anymore -&gt; remove type\n        return args[0]\n    # remove union without NoneType (i.e. not optional)\n    return make_typehint(UNION_PROXY, *args)\n</code></pre>"},{"location":"reference/metador_core/util/typing/#metador_core.util.typing.is_subtype_of","title":"is_subtype_of","text":"<pre><code>is_subtype_of(t: Any) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a predicate to check issubtype for a given type.</p> Source code in <code>src/metador_core/util/typing.py</code> <pre><code>def is_subtype_of(t: Any) -&gt; Callable[[Any], bool]:\n\"\"\"Return a predicate to check issubtype for a given type.\"\"\"\n    return lambda obj: is_subtype(obj, t)\n</code></pre>"},{"location":"reference/metador_core/util/typing/#metador_core.util.typing.is_instance_of","title":"is_instance_of","text":"<pre><code>is_instance_of(t: Any) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a predicate to check isinstance for a given type.</p> Source code in <code>src/metador_core/util/typing.py</code> <pre><code>def is_instance_of(t: Any) -&gt; Callable[[Any], bool]:\n\"\"\"Return a predicate to check isinstance for a given type.\"\"\"\n    return lambda obj: isinstance(obj, t)\n</code></pre>"},{"location":"reference/metador_core/util/typing/#metador_core.util.typing.is_subclass_of","title":"is_subclass_of","text":"<pre><code>is_subclass_of(t: Any) -&gt; Callable[[Any], bool]\n</code></pre> <p>Return a predicate to check issubclass for a given type.</p> Source code in <code>src/metador_core/util/typing.py</code> <pre><code>def is_subclass_of(t: Any) -&gt; Callable[[Any], bool]:\n\"\"\"Return a predicate to check issubclass for a given type.\"\"\"\n    return lambda obj: isinstance(obj, type) and issubclass(obj, t)\n</code></pre>"},{"location":"reference/metador_core/widget/","title":"widget","text":"<p>Interface of Metador widget plugins.</p>"},{"location":"reference/metador_core/widget/#metador_core.widget.Widget","title":"Widget","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for metador widgets.</p> Source code in <code>src/metador_core/widget/__init__.py</code> <pre><code>class Widget(ABC):\n\"\"\"Base class for metador widgets.\"\"\"\n\n    _args: Dict[str, Any]\n\"\"\"Additional passed arguments (e.g. from the dashboard).\"\"\"\n\n    _node: MetadorNode\n\"\"\"Container node passed to the widget.\"\"\"\n\n    _meta: MetadataSchema\n\"\"\"Metadata object to be used as the starting point (e.g. widget config).\"\"\"\n\n    _server: WidgetServer\n\"\"\"Widget-backing server object to use (e.g. to access container files from frontend).\"\"\"\n\n    Plugin: ClassVar[WidgetPlugin]\n\n    def __init__(\n        self,\n        node: MetadorNode,\n        schema_name: str = \"\",\n        schema_version: Optional[SemVerTuple] = None,\n        *,\n        server: Optional[WidgetServer] = None,\n        container_id: Optional[str] = None,\n        metadata: Optional[MetadataSchema] = None,\n        max_width: Optional[int] = None,\n        max_height: Optional[int] = None,\n        keep_ratio: bool = False,\n    ):\n\"\"\"Instantiate a widget for a node.\n\n        If no schema name is provided, the widget will try to pick the first metadata object\n        from the node that is an instance of a supported schema, in the listed order.\n\n        If no server is provided, a stand-alone server is started (e.g. for use in a notebook).\n\n        If a metadata object is passed explicitly, it will be used instead of trying to\n        retrieve one from the node.\n        \"\"\"\n        # NOTE: we restrict the node so that widgets don't try to escape their scope\n        self._node = node.restrict(read_only=True, local_only=True)\n        # NOTE: if no container_id is passed, we assume its jupyter mode (based on the metador UUIDs)\n        self._container_id = container_id or str(node.metador.container_uuid)\n\n        # if no server passed, we're in Jupyter mode - use standalone\n        srv: WidgetServer\n        if server is not None:\n            srv = server\n        else:\n            from .jupyter.standalone import running, widget_server\n\n            if not running():\n                raise ValueError(\n                    \"No widget server passed and standalone server not running!\"\n                )\n            srv = widget_server()\n        self._server = srv\n\n        # setup correct metadata\n        if metadata is not None:\n            if not self.supports_meta(metadata):\n                msg = \"Passed metadata is not instance of a supported schema!\"\n                raise ValueError(msg)\n            self._meta = metadata\n        else:\n            if not schema_name:\n                for schemaref in self.Plugin.supports:\n                    if node.meta.get(schemaref.name):\n                        schema_name = schemaref.name\n                        break\n            if not schema_name:\n                raise ValueError(\"The node does not contain any suitable metadata!\")\n\n            if metadata := node.meta.get(schema_name, schema_version):\n                self._meta = metadata\n            else:\n                raise ValueError(\"The node does not contain '{schema_name}' metadata!\")\n\n        # maximum width and height that can be used (if None, unlimited)\n        self._w: Optional[int] = max_width\n        self._h: Optional[int] = max_height\n\n        # recalibrate maximum width and height of widgets to preserve ration, if possible + desired\n        can_scale = self._meta.width is not None and self._meta.height is not None\n        if keep_ratio and can_scale:\n            scale_factor = min(\n                self._h / self._meta.height.value, self._w / self._meta.width.value\n            )\n            self._w = int(self._meta.width.value * scale_factor)\n            self._h = int(self._meta.height.value * scale_factor)\n\n        # widget-specific setup hook\n        self.setup()\n\n    def file_data(self, node: Optional[MetadorDataset] = None) -&gt; bytes:\n\"\"\"Return data at passed dataset node as bytes.\n\n        If no node passed, will use the widget root node (if it is a dataset).\n        \"\"\"\n        node = node or self._node\n        if not isinstance(node, MetadorDataset):\n            raise ValueError(\n                f\"Passed node {node.name} does not look like a dataset node!\"\n            )\n        return node[()].tolist()\n\n    def file_url(self, node: Optional[MetadorNode] = None) -&gt; str:\n\"\"\"Return URL resolving to the data at given node.\n\n        If no node passed, will use the widget root node (if it is a dataset).\n        \"\"\"\n        node = node or self._node\n        if not isinstance(node, MetadorDataset):\n            raise ValueError(\n                f\"Passed node {node.name} does not look like a dataset node!\"\n            )\n        return self._server.file_url_for(self._container_id, node)\n\n    @classmethod\n    def supports(cls, *schemas: PluginRef) -&gt; bool:\n\"\"\"Return whether any (exact) schema is supported (version-compatible) by widget.\"\"\"\n        return any(\n            any(map(lambda sref: sref.supports(schema), cls.Plugin.supports))\n            for schema in schemas\n        )\n\n    @classmethod\n    def supports_meta(cls, obj: MetadataSchema) -&gt; bool:\n\"\"\"Return whether widget supports the specific metadata object.\n\n        The passed object is assumed to be of one of the supported schema types.\n\n        Default implementation will just check that the object is of a supported schema.\n\n        Override to constrain further (e.g. check field values).\n\n        This method affects the dashboard widget selection process and is used\n        to check a metadata object if directly passed to `__init__`.\n        \"\"\"\n        return cls.supports(type(obj).Plugin.ref())\n\n    def setup(self):  # noqa: B027  # implementing it is not mandatory\n\"\"\"Check that passed node is valid and do preparations.\n\n        If multiple supported schemas are listed, case splitting based on the\n        schema type should be done here to minimize logic in the rendering.\n\n        Everything that instances can reuse, especially if it is computationally\n        expensive, should also be done here.\n\n        In case the widget is not able to work with the given node and metadata,\n        it will raise a `ValueError`.\n        \"\"\"\n\n    @abstractmethod\n    def show(self) -&gt; Viewable:\n\"\"\"Return a fresh Panel widget representing the node data and/or metadata.\n\n        If width and height were provided during initialization, the widget is supposed\n        to fit within these dimensions, not exceed them and if possible, usefully\n        fill up the space.\n\n        This method assumes that the widget is fully initialized and setup is completed.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/metador_core/widget/#metador_core.widget.Widget.__init__","title":"__init__","text":"<pre><code>__init__(\n    node: MetadorNode,\n    schema_name: str = \"\",\n    schema_version: Optional[SemVerTuple] = None,\n    *,\n    server: Optional[WidgetServer] = None,\n    container_id: Optional[str] = None,\n    metadata: Optional[MetadataSchema] = None,\n    max_width: Optional[int] = None,\n    max_height: Optional[int] = None,\n    keep_ratio: bool = False\n)\n</code></pre> <p>Instantiate a widget for a node.</p> <p>If no schema name is provided, the widget will try to pick the first metadata object from the node that is an instance of a supported schema, in the listed order.</p> <p>If no server is provided, a stand-alone server is started (e.g. for use in a notebook).</p> <p>If a metadata object is passed explicitly, it will be used instead of trying to retrieve one from the node.</p> Source code in <code>src/metador_core/widget/__init__.py</code> <pre><code>def __init__(\n    self,\n    node: MetadorNode,\n    schema_name: str = \"\",\n    schema_version: Optional[SemVerTuple] = None,\n    *,\n    server: Optional[WidgetServer] = None,\n    container_id: Optional[str] = None,\n    metadata: Optional[MetadataSchema] = None,\n    max_width: Optional[int] = None,\n    max_height: Optional[int] = None,\n    keep_ratio: bool = False,\n):\n\"\"\"Instantiate a widget for a node.\n\n    If no schema name is provided, the widget will try to pick the first metadata object\n    from the node that is an instance of a supported schema, in the listed order.\n\n    If no server is provided, a stand-alone server is started (e.g. for use in a notebook).\n\n    If a metadata object is passed explicitly, it will be used instead of trying to\n    retrieve one from the node.\n    \"\"\"\n    # NOTE: we restrict the node so that widgets don't try to escape their scope\n    self._node = node.restrict(read_only=True, local_only=True)\n    # NOTE: if no container_id is passed, we assume its jupyter mode (based on the metador UUIDs)\n    self._container_id = container_id or str(node.metador.container_uuid)\n\n    # if no server passed, we're in Jupyter mode - use standalone\n    srv: WidgetServer\n    if server is not None:\n        srv = server\n    else:\n        from .jupyter.standalone import running, widget_server\n\n        if not running():\n            raise ValueError(\n                \"No widget server passed and standalone server not running!\"\n            )\n        srv = widget_server()\n    self._server = srv\n\n    # setup correct metadata\n    if metadata is not None:\n        if not self.supports_meta(metadata):\n            msg = \"Passed metadata is not instance of a supported schema!\"\n            raise ValueError(msg)\n        self._meta = metadata\n    else:\n        if not schema_name:\n            for schemaref in self.Plugin.supports:\n                if node.meta.get(schemaref.name):\n                    schema_name = schemaref.name\n                    break\n        if not schema_name:\n            raise ValueError(\"The node does not contain any suitable metadata!\")\n\n        if metadata := node.meta.get(schema_name, schema_version):\n            self._meta = metadata\n        else:\n            raise ValueError(\"The node does not contain '{schema_name}' metadata!\")\n\n    # maximum width and height that can be used (if None, unlimited)\n    self._w: Optional[int] = max_width\n    self._h: Optional[int] = max_height\n\n    # recalibrate maximum width and height of widgets to preserve ration, if possible + desired\n    can_scale = self._meta.width is not None and self._meta.height is not None\n    if keep_ratio and can_scale:\n        scale_factor = min(\n            self._h / self._meta.height.value, self._w / self._meta.width.value\n        )\n        self._w = int(self._meta.width.value * scale_factor)\n        self._h = int(self._meta.height.value * scale_factor)\n\n    # widget-specific setup hook\n    self.setup()\n</code></pre>"},{"location":"reference/metador_core/widget/#metador_core.widget.Widget.file_data","title":"file_data","text":"<pre><code>file_data(node: Optional[MetadorDataset] = None) -&gt; bytes\n</code></pre> <p>Return data at passed dataset node as bytes.</p> <p>If no node passed, will use the widget root node (if it is a dataset).</p> Source code in <code>src/metador_core/widget/__init__.py</code> <pre><code>def file_data(self, node: Optional[MetadorDataset] = None) -&gt; bytes:\n\"\"\"Return data at passed dataset node as bytes.\n\n    If no node passed, will use the widget root node (if it is a dataset).\n    \"\"\"\n    node = node or self._node\n    if not isinstance(node, MetadorDataset):\n        raise ValueError(\n            f\"Passed node {node.name} does not look like a dataset node!\"\n        )\n    return node[()].tolist()\n</code></pre>"},{"location":"reference/metador_core/widget/#metador_core.widget.Widget.file_url","title":"file_url","text":"<pre><code>file_url(node: Optional[MetadorNode] = None) -&gt; str\n</code></pre> <p>Return URL resolving to the data at given node.</p> <p>If no node passed, will use the widget root node (if it is a dataset).</p> Source code in <code>src/metador_core/widget/__init__.py</code> <pre><code>def file_url(self, node: Optional[MetadorNode] = None) -&gt; str:\n\"\"\"Return URL resolving to the data at given node.\n\n    If no node passed, will use the widget root node (if it is a dataset).\n    \"\"\"\n    node = node or self._node\n    if not isinstance(node, MetadorDataset):\n        raise ValueError(\n            f\"Passed node {node.name} does not look like a dataset node!\"\n        )\n    return self._server.file_url_for(self._container_id, node)\n</code></pre>"},{"location":"reference/metador_core/widget/#metador_core.widget.Widget.supports","title":"supports  <code>classmethod</code>","text":"<pre><code>supports(*schemas: PluginRef) -&gt; bool\n</code></pre> <p>Return whether any (exact) schema is supported (version-compatible) by widget.</p> Source code in <code>src/metador_core/widget/__init__.py</code> <pre><code>@classmethod\ndef supports(cls, *schemas: PluginRef) -&gt; bool:\n\"\"\"Return whether any (exact) schema is supported (version-compatible) by widget.\"\"\"\n    return any(\n        any(map(lambda sref: sref.supports(schema), cls.Plugin.supports))\n        for schema in schemas\n    )\n</code></pre>"},{"location":"reference/metador_core/widget/#metador_core.widget.Widget.supports_meta","title":"supports_meta  <code>classmethod</code>","text":"<pre><code>supports_meta(obj: MetadataSchema) -&gt; bool\n</code></pre> <p>Return whether widget supports the specific metadata object.</p> <p>The passed object is assumed to be of one of the supported schema types.</p> <p>Default implementation will just check that the object is of a supported schema.</p> <p>Override to constrain further (e.g. check field values).</p> <p>This method affects the dashboard widget selection process and is used to check a metadata object if directly passed to <code>__init__</code>.</p> Source code in <code>src/metador_core/widget/__init__.py</code> <pre><code>@classmethod\ndef supports_meta(cls, obj: MetadataSchema) -&gt; bool:\n\"\"\"Return whether widget supports the specific metadata object.\n\n    The passed object is assumed to be of one of the supported schema types.\n\n    Default implementation will just check that the object is of a supported schema.\n\n    Override to constrain further (e.g. check field values).\n\n    This method affects the dashboard widget selection process and is used\n    to check a metadata object if directly passed to `__init__`.\n    \"\"\"\n    return cls.supports(type(obj).Plugin.ref())\n</code></pre>"},{"location":"reference/metador_core/widget/#metador_core.widget.Widget.setup","title":"setup","text":"<pre><code>setup()\n</code></pre> <p>Check that passed node is valid and do preparations.</p> <p>If multiple supported schemas are listed, case splitting based on the schema type should be done here to minimize logic in the rendering.</p> <p>Everything that instances can reuse, especially if it is computationally expensive, should also be done here.</p> <p>In case the widget is not able to work with the given node and metadata, it will raise a <code>ValueError</code>.</p> Source code in <code>src/metador_core/widget/__init__.py</code> <pre><code>def setup(self):  # noqa: B027  # implementing it is not mandatory\n\"\"\"Check that passed node is valid and do preparations.\n\n    If multiple supported schemas are listed, case splitting based on the\n    schema type should be done here to minimize logic in the rendering.\n\n    Everything that instances can reuse, especially if it is computationally\n    expensive, should also be done here.\n\n    In case the widget is not able to work with the given node and metadata,\n    it will raise a `ValueError`.\n    \"\"\"\n</code></pre>"},{"location":"reference/metador_core/widget/#metador_core.widget.Widget.show","title":"show  <code>abstractmethod</code>","text":"<pre><code>show() -&gt; Viewable\n</code></pre> <p>Return a fresh Panel widget representing the node data and/or metadata.</p> <p>If width and height were provided during initialization, the widget is supposed to fit within these dimensions, not exceed them and if possible, usefully fill up the space.</p> <p>This method assumes that the widget is fully initialized and setup is completed.</p> Source code in <code>src/metador_core/widget/__init__.py</code> <pre><code>@abstractmethod\ndef show(self) -&gt; Viewable:\n\"\"\"Return a fresh Panel widget representing the node data and/or metadata.\n\n    If width and height were provided during initialization, the widget is supposed\n    to fit within these dimensions, not exceed them and if possible, usefully\n    fill up the space.\n\n    This method assumes that the widget is fully initialized and setup is completed.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/metador_core/widget/#metador_core.widget.WidgetPlugin","title":"WidgetPlugin","text":"<p>             Bases: <code>PluginBase</code></p> Source code in <code>src/metador_core/widget/__init__.py</code> <pre><code>class WidgetPlugin(pg.PluginBase):\n    supports: Annotated[List[SchemaPluginRef], Field(min_items=1)]  # type: ignore\n\"\"\"Return list of schemas supported by this widget.\"\"\"\n\n    primary: bool = True\n\"\"\"Return whether the widget is a primary choice.\n\n    If False, will not be used automatically by dashboard.\n    \"\"\"\n</code></pre>"},{"location":"reference/metador_core/widget/#metador_core.widget.WidgetPlugin.supports","title":"supports  <code>instance-attribute</code>","text":"<pre><code>supports: Annotated[\n    List[SchemaPluginRef], Field(min_items=1)\n]\n</code></pre> <p>Return list of schemas supported by this widget.</p>"},{"location":"reference/metador_core/widget/#metador_core.widget.WidgetPlugin.primary","title":"primary  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>primary: bool = True\n</code></pre> <p>Return whether the widget is a primary choice.</p> <p>If False, will not be used automatically by dashboard.</p>"},{"location":"reference/metador_core/widget/#metador_core.widget.PGWidget","title":"PGWidget","text":"<p>             Bases: <code>PluginGroup[Widget]</code></p> <p>Widget plugin group interface.</p> Source code in <code>src/metador_core/widget/__init__.py</code> <pre><code>class PGWidget(pg.PluginGroup[Widget]):\n\"\"\"Widget plugin group interface.\"\"\"\n\n    class Plugin:\n        name = WIDGET_GROUP_NAME\n        version = (0, 1, 0)\n\n        requires = [PluginRef(group=\"plugingroup\", name=\"schema\", version=(0, 1, 0))]\n\n        plugin_class = Widget\n        plugin_info_class = WidgetPlugin\n\n    @overrides\n    def check_plugin(self, ep_name: str, plugin: Type[Widget]):\n        pg.util.check_implements_method(ep_name, plugin, Widget.show)\n\n    def plugin_deps(self, plugin) -&gt; Set[PluginRef]:\n        return set(plugin.Plugin.supports)\n\n    def widgets_for(self, schema: PluginRef) -&gt; Iterator[PluginRef]:\n\"\"\"Return widgets that support (a parent of) the given schema.\"\"\"\n        ws = set()\n        p_path = schemas.parent_path(schema.name, schema.version)\n        for s_ref in reversed(p_path):  # in decreasing specifity\n            for w_cls in self.values():\n                if w_cls.supports(s_ref) and s_ref not in ws:\n                    w_ref = w_cls.Plugin.ref()\n                    ws.add(w_ref)\n                    yield w_ref\n</code></pre>"},{"location":"reference/metador_core/widget/#metador_core.widget.PGWidget.widgets_for","title":"widgets_for","text":"<pre><code>widgets_for(schema: PluginRef) -&gt; Iterator[PluginRef]\n</code></pre> <p>Return widgets that support (a parent of) the given schema.</p> Source code in <code>src/metador_core/widget/__init__.py</code> <pre><code>def widgets_for(self, schema: PluginRef) -&gt; Iterator[PluginRef]:\n\"\"\"Return widgets that support (a parent of) the given schema.\"\"\"\n    ws = set()\n    p_path = schemas.parent_path(schema.name, schema.version)\n    for s_ref in reversed(p_path):  # in decreasing specifity\n        for w_cls in self.values():\n            if w_cls.supports(s_ref) and s_ref not in ws:\n                w_ref = w_cls.Plugin.ref()\n                ws.add(w_ref)\n                yield w_ref\n</code></pre>"},{"location":"reference/metador_core/widget/common/","title":"common","text":"<p>Common generic widgets.</p> <p>These basically integrate many default widgets provided by panel/bokeh into Metador.</p>"},{"location":"reference/metador_core/widget/common/#metador_core.widget.common.FileWidget","title":"FileWidget","text":"<p>             Bases: <code>Widget</code></p> <p>Simple widget based on (a subschema of) 'core.file'.</p> <p>Allows to state supported MIME types with less boilerplate.</p> Source code in <code>src/metador_core/widget/common.py</code> <pre><code>class FileWidget(Widget):\n\"\"\"Simple widget based on (a subschema of) 'core.file'.\n\n    Allows to state supported MIME types with less boilerplate.\n    \"\"\"\n\n    class Plugin:\n        # name and version must be overridden in subclasses\n        supports = [FileMeta.Plugin.ref()]\n\n    MIME_TYPES: Set[str] = set()\n\"\"\"If non-empty, metadata objects must have a MIME type from this set.\"\"\"\n\n    FILE_EXTS: Set[str] = set()\n\"\"\"If non-empty, filename must have an extension from this set.\"\"\"\n\n    @property\n    def title(self) -&gt; str:\n        return self._meta.name or self._meta.filename or self._node.name\n\n    @classmethod\n    @overrides\n    def supports_meta(cls, obj: MetadataSchema) -&gt; bool:\n        supported_mime = True\n        if cls.MIME_TYPES:\n            supported_mime = obj.encodingFormat in cls.MIME_TYPES\n        supported_ext = False\n        if cls.FILE_EXTS:\n            supported_ext = obj.filename.endswith(tuple(cls.FILE_EXTS))\n        # either supported mime or supported ext is enough\n        return supported_mime or supported_ext\n</code></pre>"},{"location":"reference/metador_core/widget/common/#metador_core.widget.common.FileWidget.MIME_TYPES","title":"MIME_TYPES  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MIME_TYPES: Set[str] = set()\n</code></pre> <p>If non-empty, metadata objects must have a MIME type from this set.</p>"},{"location":"reference/metador_core/widget/common/#metador_core.widget.common.FileWidget.FILE_EXTS","title":"FILE_EXTS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FILE_EXTS: Set[str] = set()\n</code></pre> <p>If non-empty, filename must have an extension from this set.</p>"},{"location":"reference/metador_core/widget/common/#metador_core.widget.common.DispatcherWidget","title":"DispatcherWidget","text":"<p>             Bases: <code>Widget</code></p> <p>Meta-widget to dispatch a node+metadata object to a more specific widget.</p> <p>Make sure that the dispatcher widget is probed before the widgets it can dispatch to.</p> <p>This works if and only if for each widget in listed in <code>WIDGETS</code>: * the plugin name of the dispatcher is a prefix of the widget name, or * the widget has <code>primary = False</code> and thus is not considered by the dashboard.</p> Source code in <code>src/metador_core/widget/common.py</code> <pre><code>class DispatcherWidget(Widget):\n\"\"\"Meta-widget to dispatch a node+metadata object to a more specific widget.\n\n    Make sure that the dispatcher widget is probed before the widgets it can\n    dispatch to.\n\n    This works if and only if for each widget in listed in `WIDGETS`:\n    * the plugin name of the dispatcher is a prefix of the widget name, or\n    * the widget has `primary = False` and thus is not considered by the dashboard.\n    \"\"\"\n\n    WIDGETS: List[Type[Widget]]\n\"\"\"Widgets in the order they should be tested.\"\"\"\n\n    def dispatch(self, w_cls: Type[Widget]) -&gt; Widget:\n\"\"\"Dispatch to another widget (used by meta-widgets).\"\"\"\n        return w_cls(\n            self._node,\n            \"\",\n            server=self._server,\n            metadata=self._meta,\n            max_width=self._w,\n            max_height=self._h,\n        )\n\n    @classmethod\n    @overrides\n    def supports_meta(cls, obj: MetadataSchema) -&gt; bool:\n        return any(map(lambda w: w.supports_meta(obj), cls.WIDGETS))\n\n    @overrides\n    def setup(self):\n        for w_cls in self.WIDGETS:\n            if w_cls.supports_meta(self._meta):\n                self._widget = self.dispatch(w_cls)\n                break\n\n    @overrides\n    def show(self) -&gt; Viewable:\n        return self._widget.show()\n</code></pre>"},{"location":"reference/metador_core/widget/common/#metador_core.widget.common.DispatcherWidget.WIDGETS","title":"WIDGETS  <code>instance-attribute</code>","text":"<pre><code>WIDGETS: List[Type[Widget]]\n</code></pre> <p>Widgets in the order they should be tested.</p>"},{"location":"reference/metador_core/widget/common/#metador_core.widget.common.DispatcherWidget.dispatch","title":"dispatch","text":"<pre><code>dispatch(w_cls: Type[Widget]) -&gt; Widget\n</code></pre> <p>Dispatch to another widget (used by meta-widgets).</p> Source code in <code>src/metador_core/widget/common.py</code> <pre><code>def dispatch(self, w_cls: Type[Widget]) -&gt; Widget:\n\"\"\"Dispatch to another widget (used by meta-widgets).\"\"\"\n    return w_cls(\n        self._node,\n        \"\",\n        server=self._server,\n        metadata=self._meta,\n        max_width=self._w,\n        max_height=self._h,\n    )\n</code></pre>"},{"location":"reference/metador_core/widget/dashboard/","title":"dashboard","text":"<p>Generic container dashboard.</p> <p>To configure a container dashboard: attach <code>DashboardConf</code> metadata to <code>MetadorContainer</code> nodes.</p> <p>To show a container dashboard: create a <code>Dashboard</code> instance.</p>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.NodeWidgetPair","title":"NodeWidgetPair  <code>module-attribute</code>","text":"<pre><code>NodeWidgetPair = Tuple[MetadorNode, WidgetConf]\n</code></pre> <p>A container node paired up with a widget configuration.</p>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.NodeWidgetRow","title":"NodeWidgetRow  <code>module-attribute</code>","text":"<pre><code>NodeWidgetRow = List[NodeWidgetPair]\n</code></pre> <p>Sorted list of NodeWidgetPairs.</p> <p>Ordered first by descending priority, then by ascending node path.</p>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.DashboardPriority","title":"DashboardPriority","text":"<p>             Bases: <code>int</code>, <code>Inclusive</code></p> <p>Dashboard priority of a widget.</p> Source code in <code>src/metador_core/widget/dashboard.py</code> <pre><code>class DashboardPriority(int, Inclusive, low=1, high=10):\n\"\"\"Dashboard priority of a widget.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.DashboardGroup","title":"DashboardGroup","text":"<p>             Bases: <code>int</code>, <code>Inclusive</code></p> <p>Dashboard group of a widget.</p> Source code in <code>src/metador_core/widget/dashboard.py</code> <pre><code>class DashboardGroup(int, Inclusive, low=1):\n\"\"\"Dashboard group of a widget.\"\"\"\n</code></pre>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.WidgetConf","title":"WidgetConf","text":"<p>             Bases: <code>MetadataSchema</code></p> <p>Configuration of a widget in the dashboard.</p> Source code in <code>src/metador_core/widget/dashboard.py</code> <pre><code>class WidgetConf(MetadataSchema):\n\"\"\"Configuration of a widget in the dashboard.\"\"\"\n\n    priority: Optional[DashboardPriority] = DashboardPriority(1)\n\"\"\"Priority of the widget (1-10), higher priority nodes are shown first.\"\"\"\n\n    group: Optional[DashboardGroup]\n\"\"\"Dashboard group of the widget.\n\n    Groups are presented in ascending order.\n    Widgets are ordered by priority within a group.\n    All widgets in a group are shown in a single row.\n\n    Widgets without an assigned group come last.\n    \"\"\"\n\n    # ----\n\n    schema_name: Optional[NonEmptyStr]\n\"\"\"Name of schema of an metadata object at the current node that is to be visualized.\n\n    If not given, any suitable will be selected if possible.\n    \"\"\"\n\n    schema_version: Optional[SemVerTuple]\n\"\"\"Version of schema to be used.\n\n    If not given, any suitable will be selected if possible.\n    \"\"\"\n\n    widget_name: Optional[str]\n\"\"\"Name of widget to be used.\n\n    If not given, any suitable will be selected if possible.\n    \"\"\"\n\n    widget_version: Optional[SemVerTuple]\n\"\"\"Version of widget to be used.\n\n    If not given, any suitable will be selected if possible.\n    \"\"\"\n</code></pre>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.WidgetConf.priority","title":"priority  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>priority: Optional[DashboardPriority] = DashboardPriority(1)\n</code></pre> <p>Priority of the widget (1-10), higher priority nodes are shown first.</p>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.WidgetConf.group","title":"group  <code>instance-attribute</code>","text":"<pre><code>group: Optional[DashboardGroup]\n</code></pre> <p>Dashboard group of the widget.</p> <p>Groups are presented in ascending order. Widgets are ordered by priority within a group. All widgets in a group are shown in a single row.</p> <p>Widgets without an assigned group come last.</p>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.WidgetConf.schema_name","title":"schema_name  <code>instance-attribute</code>","text":"<pre><code>schema_name: Optional[NonEmptyStr]\n</code></pre> <p>Name of schema of an metadata object at the current node that is to be visualized.</p> <p>If not given, any suitable will be selected if possible.</p>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.WidgetConf.schema_version","title":"schema_version  <code>instance-attribute</code>","text":"<pre><code>schema_version: Optional[SemVerTuple]\n</code></pre> <p>Version of schema to be used.</p> <p>If not given, any suitable will be selected if possible.</p>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.WidgetConf.widget_name","title":"widget_name  <code>instance-attribute</code>","text":"<pre><code>widget_name: Optional[str]\n</code></pre> <p>Name of widget to be used.</p> <p>If not given, any suitable will be selected if possible.</p>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.WidgetConf.widget_version","title":"widget_version  <code>instance-attribute</code>","text":"<pre><code>widget_version: Optional[SemVerTuple]\n</code></pre> <p>Version of widget to be used.</p> <p>If not given, any suitable will be selected if possible.</p>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.DashboardConf","title":"DashboardConf","text":"<p>             Bases: <code>MetadataSchema</code></p> <p>Schema describing dashboard configuration for a node in a container.</p> <p>Instantiating without passing a list of widget configurations will return an instance that will show an arbitrary suitable widget, i.e. is equivalent to <code>DashboardConf.show()</code></p> Source code in <code>src/metador_core/widget/dashboard.py</code> <pre><code>class DashboardConf(MetadataSchema):\n\"\"\"Schema describing dashboard configuration for a node in a container.\n\n    Instantiating without passing a list of widget configurations will\n    return an instance that will show an arbitrary suitable widget, i.e.\n    is equivalent to `DashboardConf.show()`\n    \"\"\"\n\n    class Plugin:\n        name = \"core.dashboard\"\n        version = (0, 1, 0)\n\n    widgets: List[WidgetConf] = [WidgetConf()]\n\"\"\"Widgets to present for this node in the dashboard.\n\n    If left empty, will try present any widget usable for this node.\n    \"\"\"\n\n    @staticmethod\n    def widget(**kwargs) -&gt; WidgetConf:\n\"\"\"Construct a dashboard widget configuration (see `WidgetConf`).\"\"\"\n        # for convenience\n        return WidgetConf(**kwargs)\n\n    @classmethod\n    def show(cls, _arg: List[WidgetConf] = None, **kwargs):\n\"\"\"Construct a dashboard configuration for the widget(s) of one container node.\n\n        For one widget, pass the widget config (if any) as keyword arguments,\n        e.g.  `DashboardConf.show(group=1)`.\n\n        For multiple widgets, create widget configurations with `widget(...)`,\n        and pass them to `show`, e.g.:\n        `DashboardConf.show([DashboardConf.widget(), DashboardConf.widget(group=2)])`.\n        \"\"\"\n        if _arg and kwargs:\n            msg = \"Pass widget config arguments or list of widget configs - not both!\"\n            raise ValueError(msg)\n\n        if _arg is None:\n            # kwargs have config for a singleton widget\n            widgets = [cls.widget(**kwargs)]\n        else:\n            # multiple widgets, preconfigured\n            widgets = list(_arg)\n        return cls(widgets=widgets)\n</code></pre>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.DashboardConf.widgets","title":"widgets  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>widgets: List[WidgetConf] = [WidgetConf()]\n</code></pre> <p>Widgets to present for this node in the dashboard.</p> <p>If left empty, will try present any widget usable for this node.</p>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.DashboardConf.widget","title":"widget  <code>staticmethod</code>","text":"<pre><code>widget(**kwargs) -&gt; WidgetConf\n</code></pre> <p>Construct a dashboard widget configuration (see <code>WidgetConf</code>).</p> Source code in <code>src/metador_core/widget/dashboard.py</code> <pre><code>@staticmethod\ndef widget(**kwargs) -&gt; WidgetConf:\n\"\"\"Construct a dashboard widget configuration (see `WidgetConf`).\"\"\"\n    # for convenience\n    return WidgetConf(**kwargs)\n</code></pre>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.DashboardConf.show","title":"show  <code>classmethod</code>","text":"<pre><code>show(\n    _arg: List[WidgetConf] = None,\n    **kwargs: List[WidgetConf]\n)\n</code></pre> <p>Construct a dashboard configuration for the widget(s) of one container node.</p> <p>For one widget, pass the widget config (if any) as keyword arguments, e.g.  <code>DashboardConf.show(group=1)</code>.</p> <p>For multiple widgets, create widget configurations with <code>widget(...)</code>, and pass them to <code>show</code>, e.g.: <code>DashboardConf.show([DashboardConf.widget(), DashboardConf.widget(group=2)])</code>.</p> Source code in <code>src/metador_core/widget/dashboard.py</code> <pre><code>@classmethod\ndef show(cls, _arg: List[WidgetConf] = None, **kwargs):\n\"\"\"Construct a dashboard configuration for the widget(s) of one container node.\n\n    For one widget, pass the widget config (if any) as keyword arguments,\n    e.g.  `DashboardConf.show(group=1)`.\n\n    For multiple widgets, create widget configurations with `widget(...)`,\n    and pass them to `show`, e.g.:\n    `DashboardConf.show([DashboardConf.widget(), DashboardConf.widget(group=2)])`.\n    \"\"\"\n    if _arg and kwargs:\n        msg = \"Pass widget config arguments or list of widget configs - not both!\"\n        raise ValueError(msg)\n\n    if _arg is None:\n        # kwargs have config for a singleton widget\n        widgets = [cls.widget(**kwargs)]\n    else:\n        # multiple widgets, preconfigured\n        widgets = list(_arg)\n    return cls(widgets=widgets)\n</code></pre>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.Dashboard","title":"Dashboard","text":"<p>The dashboard presents a view of all marked nodes in a container.</p> <p>To be included in the dashboard, a node must be marked by a <code>DashboardConf</code> object configuring at least one widget for that node.</p> <p>Note that the <code>Dashboard</code> needs * either a widget server to be passed (embedding in a website), * or the container is wrapped by <code>metador_core.widget.jupyter.Previewable</code> (notebook mode)</p> Source code in <code>src/metador_core/widget/dashboard.py</code> <pre><code>class Dashboard:\n\"\"\"The dashboard presents a view of all marked nodes in a container.\n\n    To be included in the dashboard, a node must be marked by a `DashboardConf`\n    object configuring at least one widget for that node.\n\n\n    Note that the `Dashboard` needs\n    * either a widget server to be passed (embedding in a website),\n    * or the container is wrapped by `metador_core.widget.jupyter.Previewable` (notebook mode)\n    \"\"\"\n\n    def __init__(\n        self,\n        container: MetadorContainer,\n        *,\n        server: WidgetServer = None,\n        container_id: Optional[str] = None,\n    ):\n\"\"\"Return instance of a dashboard.\n\n        Args:\n            container: Actual Metador container that is open and readable\n            server: `WidgetServer` to use for the widgets (default: standalone server / Jupyter mode)\n            container_id: Container id usable with the server to get this container (default: container UUID)\n        \"\"\"\n        self._container: MetadorContainer = container\n        self._server = server\n        self._container_id: str = container_id\n\n        # figure out what schemas to show and what widgets to use and collect\n        ws: List[NodeWidgetPair] = []\n        for node in self._container.metador.query(DashboardConf):\n            dbmeta = node.meta.get(DashboardConf)\n            restr_node = node.restrict(read_only=True, local_only=True)\n            for wmeta in dbmeta.widgets:\n                ws.append((restr_node, self._resolve_node(node, wmeta)))\n\n        grps, ungrp = sorted_widgets(ws)\n        self._groups = grps\n        self._ungrouped = ungrp\n\n    def _resolve_node(self, node: MetadorNode, wmeta: WidgetConf) -&gt; WidgetConf:\n\"\"\"Check and resolve widget dashboard metadata for a node.\"\"\"\n        wmeta = wmeta.copy()  # use copy, abandon original\n\n        s_ref: PluginRef = _resolve_schema(node, wmeta)\n        wmeta.schema_name = s_ref.name\n        wmeta.schema_version = s_ref.version\n\n        w_ref: PluginRef = _resolve_widget(\n            node, s_ref, wmeta.widget_name, wmeta.widget_version\n        )\n        wmeta.widget_name = w_ref.name\n        wmeta.widget_version = w_ref.version\n\n        return wmeta\n\n    def show(self) -&gt; Viewable:\n\"\"\"Instantiate widgets for container and return resulting dashboard.\"\"\"\n        # Outermost element: The Dashboard is a column of widget groups\n        db = pn.FlexBox(\n            flex_direction=\"column\",\n            justify_content=\"space-evenly\",\n            align_content=\"space-evenly\",\n            align_items=\"center\",\n            sizing_mode=\"scale_both\",\n        )\n\n        # add each widget group within individual, flexibly-wrapping rows\n        for idx, widget_group in enumerate(self._groups.values()):\n            db.append(\n                get_grp_row(\n                    idx=idx,\n                    widget_group=widget_group,\n                    divider=False,  # does not work offline with panel &gt;= 1.0?\n                    server=self._server,\n                    container_id=self._container_id,\n                )\n            )\n\n        # dump remaining ungrouped widgets into a separate flexibly-wrapping row\n        ungrp_exist = len(self._ungrouped) != 0\n        if ungrp_exist:\n            db.append(\n                get_grp_row(\n                    widget_group=self._ungrouped,\n                    server=self._server,\n                    container_id=self._container_id,\n                )\n            )\n        return db\n</code></pre>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.Dashboard.__init__","title":"__init__","text":"<pre><code>__init__(\n    container: MetadorContainer,\n    *,\n    server: WidgetServer = None,\n    container_id: Optional[str] = None\n)\n</code></pre> <p>Return instance of a dashboard.</p> <p>Parameters:</p> Name Type Description Default <code>container</code> <code>MetadorContainer</code> <p>Actual Metador container that is open and readable</p> required <code>server</code> <code>WidgetServer</code> <p><code>WidgetServer</code> to use for the widgets (default: standalone server / Jupyter mode)</p> <code>None</code> <code>container_id</code> <code>Optional[str]</code> <p>Container id usable with the server to get this container (default: container UUID)</p> <code>None</code> Source code in <code>src/metador_core/widget/dashboard.py</code> <pre><code>def __init__(\n    self,\n    container: MetadorContainer,\n    *,\n    server: WidgetServer = None,\n    container_id: Optional[str] = None,\n):\n\"\"\"Return instance of a dashboard.\n\n    Args:\n        container: Actual Metador container that is open and readable\n        server: `WidgetServer` to use for the widgets (default: standalone server / Jupyter mode)\n        container_id: Container id usable with the server to get this container (default: container UUID)\n    \"\"\"\n    self._container: MetadorContainer = container\n    self._server = server\n    self._container_id: str = container_id\n\n    # figure out what schemas to show and what widgets to use and collect\n    ws: List[NodeWidgetPair] = []\n    for node in self._container.metador.query(DashboardConf):\n        dbmeta = node.meta.get(DashboardConf)\n        restr_node = node.restrict(read_only=True, local_only=True)\n        for wmeta in dbmeta.widgets:\n            ws.append((restr_node, self._resolve_node(node, wmeta)))\n\n    grps, ungrp = sorted_widgets(ws)\n    self._groups = grps\n    self._ungrouped = ungrp\n</code></pre>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.Dashboard.show","title":"show","text":"<pre><code>show() -&gt; Viewable\n</code></pre> <p>Instantiate widgets for container and return resulting dashboard.</p> Source code in <code>src/metador_core/widget/dashboard.py</code> <pre><code>def show(self) -&gt; Viewable:\n\"\"\"Instantiate widgets for container and return resulting dashboard.\"\"\"\n    # Outermost element: The Dashboard is a column of widget groups\n    db = pn.FlexBox(\n        flex_direction=\"column\",\n        justify_content=\"space-evenly\",\n        align_content=\"space-evenly\",\n        align_items=\"center\",\n        sizing_mode=\"scale_both\",\n    )\n\n    # add each widget group within individual, flexibly-wrapping rows\n    for idx, widget_group in enumerate(self._groups.values()):\n        db.append(\n            get_grp_row(\n                idx=idx,\n                widget_group=widget_group,\n                divider=False,  # does not work offline with panel &gt;= 1.0?\n                server=self._server,\n                container_id=self._container_id,\n            )\n        )\n\n    # dump remaining ungrouped widgets into a separate flexibly-wrapping row\n    ungrp_exist = len(self._ungrouped) != 0\n    if ungrp_exist:\n        db.append(\n            get_grp_row(\n                widget_group=self._ungrouped,\n                server=self._server,\n                container_id=self._container_id,\n            )\n        )\n    return db\n</code></pre>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.sorted_widgets","title":"sorted_widgets","text":"<pre><code>sorted_widgets(\n    widgets: Iterable[NodeWidgetPair],\n) -&gt; Tuple[Dict[int, NodeWidgetRow], NodeWidgetRow]\n</code></pre> <p>Return widgets in groups, ordered by priority and node path.</p> <p>Returns tuple with dict of groups and a remainder of ungrouped widgets.</p> Source code in <code>src/metador_core/widget/dashboard.py</code> <pre><code>def sorted_widgets(\n    widgets: Iterable[NodeWidgetPair],\n) -&gt; Tuple[Dict[int, NodeWidgetRow], NodeWidgetRow]:\n\"\"\"Return widgets in groups, ordered by priority and node path.\n\n    Returns tuple with dict of groups and a remainder of ungrouped widgets.\n    \"\"\"\n\n    def nwp_group(tup: NodeWidgetPair) -&gt; int:\n        return tup[1].group or 0\n\n    def nwp_prio(tup: NodeWidgetPair) -&gt; int:\n        return -tup[1].priority or 0  # in descending order of priority\n\n    def sorted_group(ws: Iterable[NodeWidgetPair]) -&gt; NodeWidgetRow:\n\"\"\"Sort first on priority, and for same priority on container node.\"\"\"\n        return list(sorted(sorted(ws, key=lambda x: x[0].name), key=nwp_prio))\n\n    # dict, sorted in ascending group order (but ungrouped are 0)\n    ret = dict(\n        sorted(\n            {\n                k: sorted_group(v)\n                for k, v in groupby(sorted(widgets, key=nwp_group), key=nwp_group)\n            }.items()\n        )\n    )\n    ungrp = ret.pop(0, [])  # separate out the ungrouped (were mapped to 0)\n    return ret, ungrp\n</code></pre>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.get_grp_label","title":"get_grp_label","text":"<pre><code>get_grp_label(idx)\n</code></pre> <p>Create and return a styled group label.</p> Source code in <code>src/metador_core/widget/dashboard.py</code> <pre><code>def get_grp_label(idx):\n\"\"\"Create and return a styled group label.\"\"\"\n    return pn.pane.Str(\n        f\"Group {idx+1}\" if idx is not None else \"Ungrouped resources\",\n        style={\n            \"font-size\": \"15px\",\n            \"font-weight\": \"bold\",\n            \"text-decoration\": \"underline\",\n        },\n    )\n</code></pre>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.add_widgets","title":"add_widgets","text":"<pre><code>add_widgets(\n    w_grp,\n    ui_grp,\n    *,\n    server=None,\n    container_id: Optional[str] = None\n)\n</code></pre> <p>Instantiate and add widget to the flexibly wrapping row that handles the entire group.</p> Source code in <code>src/metador_core/widget/dashboard.py</code> <pre><code>def add_widgets(w_grp, ui_grp, *, server=None, container_id: Optional[str] = None):\n\"\"\"Instantiate and add widget to the flexibly wrapping row that handles the entire group.\"\"\"\n    w_width, w_height = 500, 500  # max size of a widget tile, arbitrarily set\n    for node, wmeta in w_grp:\n        w_cls = widgets.get(wmeta.widget_name, wmeta.widget_version)\n        label = pn.pane.Str(f\"{node.name}:\")\n\n        # instantiating the appropriate widget\n        w_obj = w_cls(\n            node,\n            wmeta.schema_name,\n            wmeta.schema_version,\n            server=server,\n            container_id=container_id,\n            # reset max widget of a widget tile,  only if it is for a pdf, text or video file\n            max_width=700\n            if \"pdf\" in wmeta.widget_name\n            or \"text\" in wmeta.widget_name\n            or \"video\" in wmeta.widget_name\n            else w_width,\n            # reset max height of a widget tile, only if it is for a text file\n            max_height=700 if \"text\" in wmeta.widget_name else w_height,\n        )\n\n        # adding the new widget to the given row\n        ui_grp.append(\n            pn.Column(\n                label,\n                w_obj.show(),\n                sizing_mode=\"scale_both\",\n                scroll=False\n                if \"image\" in wmeta.widget_name or \"pdf\" in wmeta.widget_name\n                else True,\n            )\n        )\n    return ui_grp\n</code></pre>"},{"location":"reference/metador_core/widget/dashboard/#metador_core.widget.dashboard.get_grp_row","title":"get_grp_row","text":"<pre><code>get_grp_row(\n    *,\n    idx=None,\n    widget_group=None,\n    divider=False,\n    server=None,\n    container_id: Optional[str] = None\n)\n</code></pre> <p>Create a flexible and wrapping row for all widgets within a single group.</p> Source code in <code>src/metador_core/widget/dashboard.py</code> <pre><code>def get_grp_row(\n    *,\n    idx=None,\n    widget_group=None,\n    divider=False,\n    server=None,\n    container_id: Optional[str] = None,\n):\n\"\"\"Create a flexible and wrapping row for all widgets within a single group.\"\"\"\n    return pn.FlexBox(\n        get_grp_label(idx=idx),\n        add_widgets(\n            widget_group,\n            pn.FlexBox(\n                flex_direction=\"row\",\n                justify_content=\"space-evenly\",\n                align_content=\"space-evenly\",\n                align_items=\"center\",\n                sizing_mode=\"scale_both\",\n            ),\n            server=server,\n            container_id=container_id,\n        ),\n        pn.layout.Divider(margin=(100, 0, 20, 0)) if divider else None,\n        flex_direction=\"column\",\n        justify_content=\"space-evenly\",\n        align_content=\"space-evenly\",\n        align_items=\"center\",\n        sizing_mode=\"scale_both\",\n    )\n</code></pre>"},{"location":"reference/metador_core/widget/jupyter/","title":"jupyter","text":"<p>Functionality to make widgets and dashboard work stand-alone in Jupyter.</p>"},{"location":"reference/metador_core/widget/jupyter/#metador_core.widget.jupyter.Previewable","title":"Previewable","text":"<p>             Bases: <code>ObjectProxy</code></p> <p>Wrapper to be used around MetadorContainer inside Jupyter.</p> <p>Will ensure that widgets can work in the notebook.</p> Source code in <code>src/metador_core/widget/jupyter/__init__.py</code> <pre><code>class Previewable(wrapt.ObjectProxy):\n\"\"\"Wrapper to be used around MetadorContainer inside Jupyter.\n\n    Will ensure that widgets can work in the notebook.\n    \"\"\"\n\n    def __init__(self, container: MetadorContainer):\n        super().__init__(container)\n\n        if not standalone.running():\n            standalone.run()\n\n        if provider := standalone.container_provider():\n            provider[str(self.metador.container_uuid)] = self\n\n    def close(self, *args, **kwargs):\n        if provider := standalone.container_provider():\n            del provider[str(self.metador.container_uuid)]\n\n        self.__wrapped__.close(*args, **kwargs)\n</code></pre>"},{"location":"reference/metador_core/widget/jupyter/standalone/","title":"standalone","text":"<p>Ad-hoc standalone dashboard/widget server for use within Jupyter notebooks.</p> <p>It runs everything needed to see a dashboard or widget in threads.</p> <p>This is mostly intended for convenient local use (e.g. by a researcher), or could be adapted for a containerized (in the Docker-sense) environment, e.g. where the user has metador libraries available and can inspect containers.</p> <p>Do not use this to deploy a widget server backing the widgets on a website.</p>"},{"location":"reference/metador_core/widget/jupyter/standalone/#metador_core.widget.jupyter.standalone.silence_flask","title":"silence_flask","text":"<pre><code>silence_flask()\n</code></pre> <p>Disable HTTP request log (for use inside jupyter).</p> Source code in <code>src/metador_core/widget/jupyter/standalone.py</code> <pre><code>def silence_flask():\n\"\"\"Disable HTTP request log (for use inside jupyter).\"\"\"\n    log = logging.getLogger(\"werkzeug\")\n    log.setLevel(logging.ERROR)\n</code></pre>"},{"location":"reference/metador_core/widget/jupyter/standalone/#metador_core.widget.jupyter.standalone.run","title":"run","text":"<pre><code>run(\n    *,\n    debug: bool = False,\n    pn_exts: Optional[List[str]] = None\n)\n</code></pre> <p>Run ad-hoc standalone server to use widgets and dashboards in a Jupyter notebook.</p> Source code in <code>src/metador_core/widget/jupyter/standalone.py</code> <pre><code>def run(*, debug: bool = False, pn_exts: Optional[List[str]] = None):\n\"\"\"Run ad-hoc standalone server to use widgets and dashboards in a Jupyter notebook.\"\"\"\n    global _widget_server, _known_containers, port\n\n    if not debug:\n        silence_flask()\n    pn.extension(\n        *(pn_exts or DEFAULT_PANEL_EXTS), inline=True\n    )  # required for panel within jupyter\n\n    port = get_free_port()\n    flask_base = f\"http://{host}:{port}\"\n\n    # prepare bokeh server\n    bokeh_port = get_free_port()\n    bokeh_base = f\"http://{host}:{bokeh_port}\"\n\n    def run_bokeh():\n        _widget_server.run(\n            host=host, port=bokeh_port, allowed_websocket_origin=[f\"{host}:{port}\"]\n        )\n\n    # prepare flask server\n    flask_app = Flask(__name__)\n    _widget_server.bokeh_endpoint = bokeh_base\n    _widget_server.flask_endpoint = flask_base\n    flask_bokeh = _widget_server.get_flask_blueprint(\"widget-api\", __name__)\n    flask_app.register_blueprint(flask_bokeh)\n\n    def run_flask():\n        flask_app.run(host=host, port=port)\n\n    # launch\n    t_flask = Thread(target=run_flask, daemon=True)\n    t_bokeh = Thread(target=run_bokeh, daemon=True)\n    t_flask.start()\n    t_bokeh.start()\n</code></pre>"},{"location":"reference/metador_core/widget/server/","title":"server","text":"<p>The Metador widget server.</p>"},{"location":"reference/metador_core/widget/server/#metador_core.widget.server.WidgetServer","title":"WidgetServer","text":"<p>Server backing the instances of Metador widgets (and dashboard).</p> <p>Metador widgets depend on a <code>WidgetServer</code> to: * get data from Metador containers (via special flask API, provided as a mountable blueprint) * wire up the information flow with a bokeh server instance (requirement for interactive bokeh widgets)</p> <p>For information on running a bokeh server see: https://docs.bokeh.org/en/latest/docs/user_guide/server.html#embedding-bokeh-server-as-a-library</p> Source code in <code>src/metador_core/widget/server/__init__.py</code> <pre><code>class WidgetServer:\n\"\"\"Server backing the instances of Metador widgets (and dashboard).\n\n    Metador widgets depend on a `WidgetServer` to:\n    * get data from Metador containers (via special flask API, provided as a mountable blueprint)\n    * wire up the information flow with a bokeh server instance (requirement for interactive bokeh widgets)\n\n    For information on running a bokeh server see:\n    https://docs.bokeh.org/en/latest/docs/user_guide/server.html#embedding-bokeh-server-as-a-library\n    \"\"\"\n\n    @classmethod\n    def _get_widget_arg(cls, args: Dict[str, List[bytes]], name: str) -&gt; Optional[str]:\n\"\"\"Extract argument from bokeh server request argument dict.\"\"\"\n        return args[name][0].decode(\"utf-8\") if name in args and args[name] else None\n\n    @classmethod\n    def _get_widget_args(cls, doc: Document):\n\"\"\"Extract arguments from bokeh server request parameters.\"\"\"\n        args = doc.session_context.request.arguments\n        return dict(\n            container_id=cls._get_widget_arg(args, \"id\"),\n            container_path=cls._get_widget_arg(args, \"path\"),\n        )\n\n    @classmethod\n    def _make_widget_args(\n        cls, container_id: str, container_path: Optional[str]\n    ) -&gt; Dict[str, str]:\n\"\"\"Construct dict to be passed through bokeh request into widget.\"\"\"\n        req_args = {\"id\": container_id}\n        if container_path:\n            req_args[\"path\"] = container_path\n        return req_args\n\n    def _get_bokeh_widget_name(\n        self,\n        viewable_type: Literal[\"widget\", \"dashboard\"],\n        name: str,\n    ) -&gt; str:\n\"\"\"Return mapped name of a registered widget or dashboard (bokeh server endpoint).\n\n        Raises NotFound exception if widget has not been found.\n        \"\"\"\n        if viewable_type not in {\"widget\", \"dashboard\"}:\n            msg = f\"Invalid type: {viewable_type}. Must be widget or dashboard!\"\n            raise NotFound(msg)\n        known = self._reg_widgets if viewable_type == \"widget\" else self._reg_dashboards\n        if name not in known:\n            raise NotFound(f\"Bokeh {viewable_type} not found: '{name}'\")\n        return known[name]\n\n    def _get_container_node(\n        self, container_id: str, container_path: Optional[str] = None\n    ) -&gt; Optional[Union[MetadorContainer, MetadorNode]]:\n\"\"\"Retrieve desired container (and target path, if provided).\n\n        If `path` is provided in the query parameters,\n        will return the container node, otherwise returns the full container.\n\n        Raises NotFound exception if container or path in container do not exist.\n        \"\"\"\n        try:\n            container = self._containers.get(container_id)\n        except KeyError as e:\n            raise NotFound(f\"Container not found: '{container_id}'\") from e\n\n        if container_path is None:\n            return container\n        if node := container.get(container_path):\n            return node.restrict(read_only=True, local_only=True)\n\n        raise NotFound(f\"Path not found in container: {container_path}\")\n\n    # ----\n\n    def __init__(\n        self,\n        containers: ContainerProxy[str],\n        *,\n        bokeh_endpoint: Optional[str] = None,\n        flask_endpoint: Optional[str] = None,\n        populate: bool = True,\n    ):\n\"\"\"Widget server to serve widget- and dashboard-like bokeh entities.\n\n        Args:\n            containers: `ContainerProxy` to retrieve containers by some container id string.\n            bokeh_endpoint: Endpoint where the bokeh server will run (`WidgetServer.run()`)\n            flask_endpoint: Endpoint where Widget API is mounted (`WidgetServer.get_flask_blueprint()`)\n            populate: If true (default), load and serve all installed widgets and generic dashboard\n        \"\"\"\n        self._containers = containers\n        self._bokeh_apps: Dict[str, Application] = {}\n        self._reg_widgets: Dict[str, str] = {}\n        self._reg_dashboards: Dict[str, str] = {}\n\n        # these can be set after launching the server threads\n        # (e.g. in case of dynamic port selection)\n        self._flask_endpoint = flask_endpoint or \"\"\n        self._bokeh_endpoint = bokeh_endpoint or \"\"\n\n        if populate:\n            self.register_installed()\n\n    def register_installed(self) -&gt; None:\n\"\"\"Register installed widgets and the generic dashboard.\"\"\"\n        # NOTE: do imports here, otherwise circular imports.\n        from metador_core.plugins import widgets\n\n        from ..dashboard import Dashboard\n\n        self.register_dashboard(\"generic\", self.make_bokeh_app(Dashboard))\n        for wclass in widgets.values():\n            self.register_widget(\n                wclass.Plugin.plugin_string(), self.make_bokeh_app(wclass)\n            )\n\n    def register_widget(self, name: str, bokeh_app: Application) -&gt; None:\n\"\"\"Register a new widget application.\"\"\"\n        mapped_name = f\"w-{name}\"\n        self._bokeh_apps[f\"/{mapped_name}\"] = bokeh_app\n        self._reg_widgets[name] = mapped_name\n\n    def register_dashboard(self, name: str, bokeh_app: Application) -&gt; None:\n\"\"\"Register a new dashboard application.\"\"\"\n        mapped_name = f\"d-{name}\"\n        self._bokeh_apps[f\"/{mapped_name}\"] = bokeh_app\n        self._reg_dashboards[name] = mapped_name\n\n    def make_bokeh_app(self, viewable_class: Viewable) -&gt; Application:\n        def handler(doc: Document) -&gt; None:\n\"\"\"Return bokeh app for Metador widget.\n\n            In this context, a suitable class must satisfy the interface\n            of being initialized with a metador node or container,\n            and having a `show()` method returning a panel `Viewable`.\n\n            The app will understand take `id` and optionally a `path` as query params.\n            These are parsed and used to look up the correct container (node).\n            \"\"\"\n            w_args = self._get_widget_args(doc)\n            if c_obj := self._get_container_node(**w_args):\n                # if we retrieved container / node, instantiate a widget and show it\n                widget = viewable_class(\n                    c_obj, server=self, container_id=w_args[\"container_id\"]\n                ).show()\n                doc.add_root(widget.get_root(doc))\n\n        return Application(FunctionHandler(handler, trap_exceptions=True))\n\n    @property\n    def flask_endpoint(self) -&gt; str:\n\"\"\"Get configured endpoint where WidgetServer API is mounted.\"\"\"\n        return self._flask_endpoint\n\n    @flask_endpoint.setter\n    def flask_endpoint(self, uri: str):\n\"\"\"Set URI where the blueprint from `get_flask_blueprint` is mounted.\"\"\"\n        self._flask_endpoint = uri.rstrip(\"/\")\n\n    @property\n    def bokeh_endpoint(self) -&gt; str:\n\"\"\"Get URI where the bokeh server is running.\"\"\"\n        return self._bokeh_endpoint\n\n    @bokeh_endpoint.setter\n    def bokeh_endpoint(self, uri: str):\n\"\"\"Set URI where the bokeh server is running.\"\"\"\n        self._bokeh_endpoint = uri.rstrip(\"/\")\n\n    def run(self, **kwargs):\n\"\"\"Run bokeh server with the registered apps (will block the current process).\"\"\"\n        # kwargs[\"io_loop\"] = kwargs.get(\"io_loop\") or IOLoop()\n        # server = pn.io.server.get_server(self._bokeh_apps, **kwargs)\n\n        # NOTE: this loads unused extensions (e.g. ace) that are not even listed?!\n        # pn.extension(inline=True)\n        # This seems to work ok:\n        pn.config.inline = True\n\n        kwargs[\"loop\"] = kwargs.get(\"io_loop\") or IOLoop()\n        server = Server(self._bokeh_apps, **kwargs)\n\n        server.start()\n        server.io_loop.start()\n\n    # ----\n    # Helper functions exposed to widgets\n\n    def file_url_for(self, container_id: str, node: MetadorNode) -&gt; str:\n\"\"\"Return URL for given container ID and file at Metador Container node.\n\n        To be used by widgets that need direct access to files in the container.\n        \"\"\"\n        if not self._flask_endpoint:\n            raise RuntimeError(\"missing flask endpoint!\")\n        return f\"{self._flask_endpoint}/file/{container_id}{node.name}\"\n\n    # ----\n    # Functions making up the WidgetServer API\n\n    def index(self):\n\"\"\"Return information about current Metador environment.\n\n        Response includes an overview of metador-related Python packages,\n        Metador plugins, and the known widgets (nodes) and dashboards (containers).\n        \"\"\"\n        from metador_core.plugin.types import to_ep_name\n        from metador_core.plugins import plugingroups\n\n        # build dict with all available metador plugins\n        pgs = {to_ep_name(x.name, x.version): x.dict() for x in plugingroups.keys()}\n        groups = {plugingroups.Plugin.name: pgs}\n        for pg in plugingroups.values():\n            groups[pg.Plugin.name] = {\n                to_ep_name(x.name, x.version): x.dict() for x in pg.keys()\n            }\n\n        return {\n            \"widgets\": list(self._reg_widgets),\n            \"dashboards\": list(self._reg_dashboards),\n            \"plugins\": groups,\n        }\n\n    def download(self, container_id: str, container_path: str):\n\"\"\"Return file download stream of a file embedded in the container.\"\"\"\n        node = self._get_container_node(container_id, container_path)\n        # get data out of container\n        obj = node[()]\n        bs = obj.tolist() if isinstance(obj, np.void) else obj\n        if not isinstance(bs, bytes):\n            raise BadRequest(f\"Path not a bytes object: /{container_path}\")\n\n        # construct a default file name based on path in container\n        def_name = f\"{container_id}_{container_path.replace('/', '__')}\"\n        # if object has attached file metadata, use it to serve data:\n        filemeta = node.meta.get(\"core.file\")\n        name = filemeta.id_ if filemeta else def_name\n        mime = filemeta.encodingFormat if filemeta else None\n\n        # requested as explicit file download?\n        dl = bool(request.args.get(\"download\", False))\n        # return file download stream with download metadata\n        return send_file(\n            io.BytesIO(bs), download_name=name, mimetype=mime, as_attachment=dl\n        )\n\n    def get_script(\n        self,\n        viewable_type: Literal[\"widget\", \"dashboard\"],\n        name: str,\n        container_id: str,\n        container_path: Optional[str] = None,\n    ) -&gt; str:\n\"\"\"Return a script tag that will auto-load the desired widget for selected container.\"\"\"\n        if not self._bokeh_endpoint:\n            raise RuntimeError(\"missing bokeh endpoint!\")\n        if viewable_type == \"dashboard\" and container_path:\n            raise BadRequest(\"Dashboards do not accept a container path!\")\n\n        return server_document(\n            f\"{self._bokeh_endpoint}/{self._get_bokeh_widget_name(viewable_type, name)}\",\n            arguments=self._make_widget_args(container_id, container_path),\n        )\n\n    def get_flask_blueprint(self, *args):\n\"\"\"Return a Flask blueprint with the Metador container and widget API.\"\"\"\n        api = Blueprint(*args)\n\n        api.route(\"/\")(self.index)\n        api.route(\"/file/&lt;container_id&gt;/&lt;path:container_path&gt;\")(self.download)\n        api.route(\"/&lt;viewable_type&gt;/&lt;name&gt;/&lt;container_id&gt;/\")(\n            api.route(\"/&lt;viewable_type&gt;/&lt;name&gt;/&lt;container_id&gt;/&lt;path:container_path&gt;\")(\n                self.get_script\n            )\n        )\n\n        return api\n</code></pre>"},{"location":"reference/metador_core/widget/server/#metador_core.widget.server.WidgetServer.flask_endpoint","title":"flask_endpoint  <code>property</code> <code>writable</code>","text":"<pre><code>flask_endpoint: str\n</code></pre> <p>Get configured endpoint where WidgetServer API is mounted.</p>"},{"location":"reference/metador_core/widget/server/#metador_core.widget.server.WidgetServer.bokeh_endpoint","title":"bokeh_endpoint  <code>property</code> <code>writable</code>","text":"<pre><code>bokeh_endpoint: str\n</code></pre> <p>Get URI where the bokeh server is running.</p>"},{"location":"reference/metador_core/widget/server/#metador_core.widget.server.WidgetServer.__init__","title":"__init__","text":"<pre><code>__init__(\n    containers: ContainerProxy[str],\n    *,\n    bokeh_endpoint: Optional[str] = None,\n    flask_endpoint: Optional[str] = None,\n    populate: bool = True\n)\n</code></pre> <p>Widget server to serve widget- and dashboard-like bokeh entities.</p> <p>Parameters:</p> Name Type Description Default <code>containers</code> <code>ContainerProxy[str]</code> <p><code>ContainerProxy</code> to retrieve containers by some container id string.</p> required <code>bokeh_endpoint</code> <code>Optional[str]</code> <p>Endpoint where the bokeh server will run (<code>WidgetServer.run()</code>)</p> <code>None</code> <code>flask_endpoint</code> <code>Optional[str]</code> <p>Endpoint where Widget API is mounted (<code>WidgetServer.get_flask_blueprint()</code>)</p> <code>None</code> <code>populate</code> <code>bool</code> <p>If true (default), load and serve all installed widgets and generic dashboard</p> <code>True</code> Source code in <code>src/metador_core/widget/server/__init__.py</code> <pre><code>def __init__(\n    self,\n    containers: ContainerProxy[str],\n    *,\n    bokeh_endpoint: Optional[str] = None,\n    flask_endpoint: Optional[str] = None,\n    populate: bool = True,\n):\n\"\"\"Widget server to serve widget- and dashboard-like bokeh entities.\n\n    Args:\n        containers: `ContainerProxy` to retrieve containers by some container id string.\n        bokeh_endpoint: Endpoint where the bokeh server will run (`WidgetServer.run()`)\n        flask_endpoint: Endpoint where Widget API is mounted (`WidgetServer.get_flask_blueprint()`)\n        populate: If true (default), load and serve all installed widgets and generic dashboard\n    \"\"\"\n    self._containers = containers\n    self._bokeh_apps: Dict[str, Application] = {}\n    self._reg_widgets: Dict[str, str] = {}\n    self._reg_dashboards: Dict[str, str] = {}\n\n    # these can be set after launching the server threads\n    # (e.g. in case of dynamic port selection)\n    self._flask_endpoint = flask_endpoint or \"\"\n    self._bokeh_endpoint = bokeh_endpoint or \"\"\n\n    if populate:\n        self.register_installed()\n</code></pre>"},{"location":"reference/metador_core/widget/server/#metador_core.widget.server.WidgetServer.register_installed","title":"register_installed","text":"<pre><code>register_installed() -&gt; None\n</code></pre> <p>Register installed widgets and the generic dashboard.</p> Source code in <code>src/metador_core/widget/server/__init__.py</code> <pre><code>def register_installed(self) -&gt; None:\n\"\"\"Register installed widgets and the generic dashboard.\"\"\"\n    # NOTE: do imports here, otherwise circular imports.\n    from metador_core.plugins import widgets\n\n    from ..dashboard import Dashboard\n\n    self.register_dashboard(\"generic\", self.make_bokeh_app(Dashboard))\n    for wclass in widgets.values():\n        self.register_widget(\n            wclass.Plugin.plugin_string(), self.make_bokeh_app(wclass)\n        )\n</code></pre>"},{"location":"reference/metador_core/widget/server/#metador_core.widget.server.WidgetServer.register_widget","title":"register_widget","text":"<pre><code>register_widget(name: str, bokeh_app: Application) -&gt; None\n</code></pre> <p>Register a new widget application.</p> Source code in <code>src/metador_core/widget/server/__init__.py</code> <pre><code>def register_widget(self, name: str, bokeh_app: Application) -&gt; None:\n\"\"\"Register a new widget application.\"\"\"\n    mapped_name = f\"w-{name}\"\n    self._bokeh_apps[f\"/{mapped_name}\"] = bokeh_app\n    self._reg_widgets[name] = mapped_name\n</code></pre>"},{"location":"reference/metador_core/widget/server/#metador_core.widget.server.WidgetServer.register_dashboard","title":"register_dashboard","text":"<pre><code>register_dashboard(\n    name: str, bokeh_app: Application\n) -&gt; None\n</code></pre> <p>Register a new dashboard application.</p> Source code in <code>src/metador_core/widget/server/__init__.py</code> <pre><code>def register_dashboard(self, name: str, bokeh_app: Application) -&gt; None:\n\"\"\"Register a new dashboard application.\"\"\"\n    mapped_name = f\"d-{name}\"\n    self._bokeh_apps[f\"/{mapped_name}\"] = bokeh_app\n    self._reg_dashboards[name] = mapped_name\n</code></pre>"},{"location":"reference/metador_core/widget/server/#metador_core.widget.server.WidgetServer.run","title":"run","text":"<pre><code>run(**kwargs)\n</code></pre> <p>Run bokeh server with the registered apps (will block the current process).</p> Source code in <code>src/metador_core/widget/server/__init__.py</code> <pre><code>def run(self, **kwargs):\n\"\"\"Run bokeh server with the registered apps (will block the current process).\"\"\"\n    # kwargs[\"io_loop\"] = kwargs.get(\"io_loop\") or IOLoop()\n    # server = pn.io.server.get_server(self._bokeh_apps, **kwargs)\n\n    # NOTE: this loads unused extensions (e.g. ace) that are not even listed?!\n    # pn.extension(inline=True)\n    # This seems to work ok:\n    pn.config.inline = True\n\n    kwargs[\"loop\"] = kwargs.get(\"io_loop\") or IOLoop()\n    server = Server(self._bokeh_apps, **kwargs)\n\n    server.start()\n    server.io_loop.start()\n</code></pre>"},{"location":"reference/metador_core/widget/server/#metador_core.widget.server.WidgetServer.file_url_for","title":"file_url_for","text":"<pre><code>file_url_for(container_id: str, node: MetadorNode) -&gt; str\n</code></pre> <p>Return URL for given container ID and file at Metador Container node.</p> <p>To be used by widgets that need direct access to files in the container.</p> Source code in <code>src/metador_core/widget/server/__init__.py</code> <pre><code>def file_url_for(self, container_id: str, node: MetadorNode) -&gt; str:\n\"\"\"Return URL for given container ID and file at Metador Container node.\n\n    To be used by widgets that need direct access to files in the container.\n    \"\"\"\n    if not self._flask_endpoint:\n        raise RuntimeError(\"missing flask endpoint!\")\n    return f\"{self._flask_endpoint}/file/{container_id}{node.name}\"\n</code></pre>"},{"location":"reference/metador_core/widget/server/#metador_core.widget.server.WidgetServer.index","title":"index","text":"<pre><code>index()\n</code></pre> <p>Return information about current Metador environment.</p> <p>Response includes an overview of metador-related Python packages, Metador plugins, and the known widgets (nodes) and dashboards (containers).</p> Source code in <code>src/metador_core/widget/server/__init__.py</code> <pre><code>def index(self):\n\"\"\"Return information about current Metador environment.\n\n    Response includes an overview of metador-related Python packages,\n    Metador plugins, and the known widgets (nodes) and dashboards (containers).\n    \"\"\"\n    from metador_core.plugin.types import to_ep_name\n    from metador_core.plugins import plugingroups\n\n    # build dict with all available metador plugins\n    pgs = {to_ep_name(x.name, x.version): x.dict() for x in plugingroups.keys()}\n    groups = {plugingroups.Plugin.name: pgs}\n    for pg in plugingroups.values():\n        groups[pg.Plugin.name] = {\n            to_ep_name(x.name, x.version): x.dict() for x in pg.keys()\n        }\n\n    return {\n        \"widgets\": list(self._reg_widgets),\n        \"dashboards\": list(self._reg_dashboards),\n        \"plugins\": groups,\n    }\n</code></pre>"},{"location":"reference/metador_core/widget/server/#metador_core.widget.server.WidgetServer.download","title":"download","text":"<pre><code>download(container_id: str, container_path: str)\n</code></pre> <p>Return file download stream of a file embedded in the container.</p> Source code in <code>src/metador_core/widget/server/__init__.py</code> <pre><code>def download(self, container_id: str, container_path: str):\n\"\"\"Return file download stream of a file embedded in the container.\"\"\"\n    node = self._get_container_node(container_id, container_path)\n    # get data out of container\n    obj = node[()]\n    bs = obj.tolist() if isinstance(obj, np.void) else obj\n    if not isinstance(bs, bytes):\n        raise BadRequest(f\"Path not a bytes object: /{container_path}\")\n\n    # construct a default file name based on path in container\n    def_name = f\"{container_id}_{container_path.replace('/', '__')}\"\n    # if object has attached file metadata, use it to serve data:\n    filemeta = node.meta.get(\"core.file\")\n    name = filemeta.id_ if filemeta else def_name\n    mime = filemeta.encodingFormat if filemeta else None\n\n    # requested as explicit file download?\n    dl = bool(request.args.get(\"download\", False))\n    # return file download stream with download metadata\n    return send_file(\n        io.BytesIO(bs), download_name=name, mimetype=mime, as_attachment=dl\n    )\n</code></pre>"},{"location":"reference/metador_core/widget/server/#metador_core.widget.server.WidgetServer.get_script","title":"get_script","text":"<pre><code>get_script(\n    viewable_type: Literal[\"widget\", \"dashboard\"],\n    name: str,\n    container_id: str,\n    container_path: Optional[str] = None,\n) -&gt; str\n</code></pre> <p>Return a script tag that will auto-load the desired widget for selected container.</p> Source code in <code>src/metador_core/widget/server/__init__.py</code> <pre><code>def get_script(\n    self,\n    viewable_type: Literal[\"widget\", \"dashboard\"],\n    name: str,\n    container_id: str,\n    container_path: Optional[str] = None,\n) -&gt; str:\n\"\"\"Return a script tag that will auto-load the desired widget for selected container.\"\"\"\n    if not self._bokeh_endpoint:\n        raise RuntimeError(\"missing bokeh endpoint!\")\n    if viewable_type == \"dashboard\" and container_path:\n        raise BadRequest(\"Dashboards do not accept a container path!\")\n\n    return server_document(\n        f\"{self._bokeh_endpoint}/{self._get_bokeh_widget_name(viewable_type, name)}\",\n        arguments=self._make_widget_args(container_id, container_path),\n    )\n</code></pre>"},{"location":"reference/metador_core/widget/server/#metador_core.widget.server.WidgetServer.get_flask_blueprint","title":"get_flask_blueprint","text":"<pre><code>get_flask_blueprint(*args)\n</code></pre> <p>Return a Flask blueprint with the Metador container and widget API.</p> Source code in <code>src/metador_core/widget/server/__init__.py</code> <pre><code>def get_flask_blueprint(self, *args):\n\"\"\"Return a Flask blueprint with the Metador container and widget API.\"\"\"\n    api = Blueprint(*args)\n\n    api.route(\"/\")(self.index)\n    api.route(\"/file/&lt;container_id&gt;/&lt;path:container_path&gt;\")(self.download)\n    api.route(\"/&lt;viewable_type&gt;/&lt;name&gt;/&lt;container_id&gt;/\")(\n        api.route(\"/&lt;viewable_type&gt;/&lt;name&gt;/&lt;container_id&gt;/&lt;path:container_path&gt;\")(\n            self.get_script\n        )\n    )\n\n    return api\n</code></pre>"}]}